{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing Annotations Supervisely\n",
    "\n",
    "**Objective:** Process annotated rooftop segmentation data for quality control and standardization.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load dataset and annotation metadata\n",
    "2. Clip images using building polygons\n",
    "3. Handle tile overlaps and remove duplicates\n",
    "4. Standardize dimensions to 1280x1280\n",
    "5. Validate data integrity and export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import tempfile\n",
    "import traceback\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, mapping\n",
    "\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.windows import from_bounds, Window\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "DATASET_ANNOTATED_BRUT_PATH = \"datasets/supervisely/341575_free_space_rooftop_geneva_20250511_binary_mask\"\n",
    "DATASET_TILES_INFORMATION_CSV_PATH = \"data/notebook_06/dataset_20250405-193125/PNG_dataset_roboflow_20250405-193143/sampled_tiles.csv\"\n",
    "CORRECT_CRS = CRS.from_epsg(2056)\n",
    "EPSG_SUISSE = \"EPSG:2056\"\n",
    "\n",
    "CAD_BATIMENT_HORSOL_TOIT_MERGE_PARQUET_PATH = \"data/notebook_04/parquet/04_02_merged_rooftops_poly.parquet\"\n",
    "\n",
    "# Parquet files\n",
    "VERIFICATION_OUTPUT_PARQUET_PATH = \"data/notebook_06/parquet/06_01_verification.parquet\"\n",
    "DATASET_OUTPUT_PARQUET_PATH = \"data/notebook_06/parquet/06_02_dataset_processed.parquet\"\n",
    "DATASET_FINAL_OUTPUT_PARQUET_PATH = \"data/notebook_06/parquet/06_03_dataset_final.parquet\"\n",
    "\n",
    "# dataset processed\n",
    "DATASET_PROCESSED_NAME = \"dataset_processed_\" + str(todays_date)\n",
    "DATASET_PROCESSED_PATH = \"datasets/supervisely/\" + DATASET_PROCESSED_NAME\n",
    "DATASET_OUTPUT_IMG_PATH = DATASET_PROCESSED_PATH + \"/images\"\n",
    "DATASET_OUTPUT_MASKS_PATH = DATASET_PROCESSED_PATH + \"/masks\"\n",
    "DATASET_OUTPUT_CHECKS_PATH = DATASET_PROCESSED_PATH + \"/check_dataset\"\n",
    "\n",
    "# buffer pour les chevauchements\n",
    "BUFFER_DISTANCE = 0 # en mètre\n",
    "OVERLAP_POSITIONS=['top', 'right', 'top-left', 'top-right']\n",
    "\n",
    "os.makedirs(DATASET_PROCESSED_PATH)\n",
    "os.makedirs(DATASET_OUTPUT_IMG_PATH)\n",
    "os.makedirs(DATASET_OUTPUT_MASKS_PATH)\n",
    "os.makedirs(DATASET_OUTPUT_CHECKS_PATH)\n",
    "\n",
    "#! Régénérer les tuiles dans tile_1024_split depuis les geotiff de 1.4Gb de SITG. Environ 1h\n",
    "#! Utiles si jamais les tuiles de 1024 sont corrompues ou effacées par erreur\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG = False\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_PROCESSES = 2\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_THREADS = 2\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG_COMBINED_METADATA_PARQUET = \"data/notebook_04/geotiff/tile_1024_split_old_20250519-120028/combined_metadata.parquet\"\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TILE_1024 = \"data/notebook_04/geotiff/tile_1024_split\"\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TEMP_1280 = 'data/notebook_04/geotiff/tile_1280_split'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional tile_1024_split Regeneration\n",
    "\n",
    "Note: optionnel, c'est dans le cas ou le tile_1024_split a été modifié par erreur\n",
    "\n",
    "Note: regenerate tile_1024_split tiles from SITG GeoTIFFs if source files are corrupted or missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tile(row, output_dir, debug_dir):\n",
    "    \"\"\"\n",
    "    Process a single tile from source GeoTIFF.\n",
    "    \n",
    "    Extracts the specified bounds from the source GeoTIFF and saves\n",
    "    as a new tile with proper georeferencing.\n",
    "    \n",
    "    Parameters:\n",
    "        row: DataFrame row containing tile metadata\n",
    "        output_dir: Directory to save processed tiles\n",
    "        debug_dir: Directory for error logs\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (index, success, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract bounds from buffered_bounds column\n",
    "        bounds_str = row['buffered_bounds']\n",
    "        if isinstance(bounds_str, str):\n",
    "            bounds_str = bounds_str.replace(' ', '')\n",
    "            bounds = tuple(float(x) for x in bounds_str.strip('()').split(','))\n",
    "        else:\n",
    "            bounds = bounds_str\n",
    "        \n",
    "        min_x, min_y, max_x, max_y = bounds\n",
    "        \n",
    "        # Extract filename and create output path\n",
    "        tile_path = row['tile_path']\n",
    "        output_filename = os.path.basename(tile_path)\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        # Open source GeoTIFF\n",
    "        with rasterio.open(row['geotiff_path']) as src:\n",
    "            # Get window from original bounds\n",
    "            window = from_bounds(min_x, min_y, max_x, max_y, src.transform)\n",
    "            \n",
    "            # Round window coordinates to avoid floating point issues\n",
    "            window = rasterio.windows.Window(\n",
    "                col_off=int(round(window.col_off)),\n",
    "                row_off=int(round(window.row_off)),\n",
    "                width=int(round(window.width)),\n",
    "                height=int(round(window.height))\n",
    "            )\n",
    "            \n",
    "            # Verify window fits within source bounds\n",
    "            if (window.col_off < 0 or window.row_off < 0 or \n",
    "                window.col_off + window.width > src.width or \n",
    "                window.row_off + window.height > src.height):\n",
    "                \n",
    "                # Adjust window to fit within image bounds\n",
    "                window = window.intersection(\n",
    "                    rasterio.windows.Window(0, 0, src.width, src.height)\n",
    "                )\n",
    "            \n",
    "            # Read data from window\n",
    "            data = src.read(window=window)\n",
    "            \n",
    "            # Get transform for the window\n",
    "            window_transform = rasterio.windows.transform(window, src.transform)\n",
    "            \n",
    "            # Create output profile with maximum quality settings\n",
    "            profile = src.profile.copy()\n",
    "            profile.update({\n",
    "                'height': window.height,\n",
    "                'width': window.width,\n",
    "                'transform': window_transform,\n",
    "                'crs': CORRECT_CRS,\n",
    "                'driver': 'GTiff',\n",
    "                'compress': None,  # No compression for highest quality\n",
    "                'predictor': 1,\n",
    "                'tiled': False,\n",
    "                'interleave': 'band',\n",
    "                'bigtiff': True,\n",
    "                'dtype': src.dtypes[0],\n",
    "            })\n",
    "            \n",
    "            # Write new GeoTIFF\n",
    "            with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                dst.write(data)\n",
    "        \n",
    "        return (row.name, True, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Capture full exception traceback\n",
    "        tb = traceback.format_exc()\n",
    "        return (row.name, False, f\"Error: {str(e)}\\n{tb}\")\n",
    "\n",
    "\n",
    "def copy_file(args):\n",
    "    \"\"\"\n",
    "    Copy a single file with error handling.\n",
    "    \n",
    "    Parameters:\n",
    "        args: Tuple of (source_file, destination_file)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (success, message)\n",
    "    \"\"\"\n",
    "    src_file, dst_file = args\n",
    "    try:\n",
    "        shutil.copy2(src_file, dst_file)\n",
    "        return (True, src_file)\n",
    "    except Exception as e:\n",
    "        return (False, f\"Error copying {src_file} to {dst_file}: {str(e)}\")\n",
    "\n",
    "\n",
    "def process_geotiffs(chunk_df, output_dir, debug_dir):\n",
    "    \"\"\"\n",
    "    Process a chunk of GeoTIFF tiles.\n",
    "    \n",
    "    Parameters:\n",
    "        chunk_df: DataFrame chunk to process\n",
    "        output_dir: Output directory for tiles\n",
    "        debug_dir: Directory for error logs\n",
    "        \n",
    "    Returns:\n",
    "        list: Processing results for each tile\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    # Process each row in the chunk\n",
    "    for idx, row in chunk_df.iterrows():\n",
    "        result = process_tile(row, output_dir, debug_dir)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "if REGENERATE_TILE_1024_SPLIT_FROM_SITG:\n",
    "    # Suppress warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    \n",
    "    # Load metadata\n",
    "    df = pd.read_parquet(REGENERATE_TILE_1024_SPLIT_FROM_SITG_COMBINED_METADATA_PARQUET)\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TEMP_1280, exist_ok=True)\n",
    "    debug_dir = os.path.join(REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TEMP_1280, 'debug')\n",
    "    os.makedirs(debug_dir, exist_ok=True)\n",
    "    \n",
    "    # Group by source GeoTIFF for efficient processing\n",
    "    grouped = df.groupby('geotiff_path')\n",
    "    group_dfs = [group for _, group in grouped]\n",
    "    \n",
    "    print(f\"Processing {len(df)} tiles from {len(group_dfs)} source GeoTIFFs using {REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_PROCESSES} processes\")\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    with ProcessPoolExecutor(max_workers=REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_PROCESSES) as executor:\n",
    "        futures = [executor.submit(process_geotiffs, group_df, REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TEMP_1280, debug_dir) \n",
    "                  for group_df in group_dfs]\n",
    "        \n",
    "        # Track progress\n",
    "        all_results = []\n",
    "        for future in tqdm(futures, total=len(futures), desc=\"Processing GeoTIFF groups\"):\n",
    "            results = future.result()\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    # Process results\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for idx, success, error_msg in all_results:\n",
    "        if success:\n",
    "            success_count += 1\n",
    "        else:\n",
    "            error_count += 1\n",
    "            # Log error\n",
    "            error_info_path = os.path.join(debug_dir, f\"error_row_{idx}.txt\")\n",
    "            with open(error_info_path, 'w') as f:\n",
    "                f.write(error_msg)\n",
    "    \n",
    "    print(f\"Tile processing complete: {success_count} successful, {error_count} errors\")\n",
    "    \n",
    "    # Copy additional files maintaining directory structure\n",
    "    src_dir = REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TILE_1024\n",
    "    dst_dir = REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TEMP_1280\n",
    "    \n",
    "    # Get list of processed TIF files\n",
    "    processed_tif_files = set(os.path.basename(row['tile_path']) for _, row in df.iterrows())\n",
    "    \n",
    "    # Build list of files to copy\n",
    "    files_to_copy = []\n",
    "    print(\"\\nScanning directory structure...\")\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        rel_path = os.path.relpath(root, src_dir)\n",
    "        \n",
    "        # Create corresponding directory\n",
    "        if rel_path != '.':\n",
    "            dst_root = os.path.join(dst_dir, rel_path)\n",
    "            os.makedirs(dst_root, exist_ok=True)\n",
    "            print(f\"Created directory: {rel_path}\")\n",
    "        \n",
    "        # Add files to copy list\n",
    "        for file in files:\n",
    "            # Skip already processed TIF files\n",
    "            if rel_path == '.' and file.endswith('.tif') and file in processed_tif_files:\n",
    "                continue\n",
    "            \n",
    "            src_file = os.path.join(root, file)\n",
    "            dst_file = os.path.join(dst_dir, rel_path, file) if rel_path != '.' else os.path.join(dst_dir, file)\n",
    "            files_to_copy.append((src_file, dst_file))\n",
    "    \n",
    "    # Copy files in parallel\n",
    "    print(f\"\\nCopying {len(files_to_copy)} additional files using {REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_THREADS} threads...\")\n",
    "    with ThreadPoolExecutor(max_workers=REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_THREADS) as executor:\n",
    "        results = list(tqdm(executor.map(copy_file, files_to_copy), total=len(files_to_copy), desc=\"Copying files\"))\n",
    "    \n",
    "    # Check for copy errors\n",
    "    copy_errors = [result for result in results if not result[0]]\n",
    "    if copy_errors:\n",
    "        print(f\"Warning: {len(copy_errors)} files failed to copy:\")\n",
    "        for _, error in copy_errors[:10]:  # Show first 10 errors\n",
    "            print(f\"  {error}\")\n",
    "        if len(copy_errors) > 10:\n",
    "            print(f\"  ... and {len(copy_errors) - 10} more errors\")\n",
    "    \n",
    "    # Verify file counts\n",
    "    def count_files(directory):\n",
    "        count = 0\n",
    "        for root, _, files in os.walk(directory):\n",
    "            count += len(files)\n",
    "        return count\n",
    "    \n",
    "    old_count = count_files(src_dir)\n",
    "    new_count = count_files(dst_dir)\n",
    "    \n",
    "    print(f\"\\nTotal files in old directory: {old_count}\")\n",
    "    print(f\"Total files in new directory: {new_count}\")\n",
    "    \n",
    "    # Calculate expected difference\n",
    "    expected_diff = len(processed_tif_files)\n",
    "    actual_diff = new_count - old_count + expected_diff\n",
    "    \n",
    "    print(f\"Expected difference: {expected_diff}\")\n",
    "    print(f\"Actual difference: {actual_diff}\")\n",
    "    \n",
    "    if actual_diff != 0:\n",
    "        print(\"WARNING: File count doesn't match expectations!\")\n",
    "        proceed = input(\"Do you want to proceed with the renaming? (y/n): \")\n",
    "        if proceed.lower() != 'y':\n",
    "            print(\"Operation aborted\")\n",
    "            exit()\n",
    "\n",
    "    # Rename directories\n",
    "    todaysdate = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    old_folder_new_name = \"data/notebook_04/geotiff/tile_1024_split_old_\" + str(todaysdate)\n",
    "    \n",
    "    print(f\"\\nRenaming old folder '{src_dir}' to '{old_folder_new_name}'\")\n",
    "    os.rename(src_dir, old_folder_new_name)\n",
    "    \n",
    "    print(f\"Renaming new folder '{dst_dir}' to '{src_dir}'\")\n",
    "    os.rename(dst_dir, src_dir)\n",
    "\n",
    "    print(\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "### Pre-annotation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_dataset = gpd.read_file(DATASET_TILES_INFORMATION_CSV_PATH)\n",
    "# Remove duplicate geometry columns if present\n",
    "if \"geometry\" in gdf_dataset.columns:\n",
    "    gdf_dataset = gdf_dataset.drop(columns=[\"geometry\"])\n",
    "if \"geometry_x\" in gdf_dataset.columns:\n",
    "    gdf_dataset = gdf_dataset.drop(columns=[\"geometry_x\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gdf_dataset))\n",
    "gdf_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no duplicates in tile_id\n",
    "assert(len(gdf_dataset[gdf_dataset.duplicated(subset=[\"tile_id\"])]) == 0), f\"gdf_dataset has duplicates in tile_id: {gdf_dataset[gdf_dataset.duplicated(subset=['tile_id'])]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image paths\n",
    "dataset_img_path = os.path.join(DATASET_ANNOTATED_BRUT_PATH, [f for f in os.listdir(DATASET_ANNOTATED_BRUT_PATH) if f.startswith(\"dataset\")][0], \"img\")\n",
    "assert(os.path.exists(dataset_img_path)), f\"Path does not exist: {dataset_img_path}\"\n",
    "print(f\"dataset_masks_path: {dataset_img_path}\")\n",
    "print(f\"Number of files: {len(os.listdir(dataset_img_path))}\")\n",
    "\n",
    "# Load mask paths\n",
    "dataset_masks_path = os.path.join(DATASET_ANNOTATED_BRUT_PATH, [f for f in os.listdir(DATASET_ANNOTATED_BRUT_PATH) if f.startswith(\"dataset\")][0], \"masks_machine\")\n",
    "assert(os.path.exists(dataset_masks_path)), f\"Path does not exist: {dataset_masks_path}\"\n",
    "print(f\"dataset_masks_path: {dataset_masks_path}\")\n",
    "print(f\"Number of files: {len(os.listdir(dataset_masks_path))}\")\n",
    "\n",
    "# Verify equal number of images and masks\n",
    "assert(len(os.listdir(dataset_img_path)) == len(os.listdir(dataset_masks_path))), f\"Number of files in {dataset_masks_path} is not equal to number of files in {dataset_masks_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with annotation paths\n",
    "dataset_original_masks_path_list = [os.path.join(dataset_masks_path, f) for f in os.listdir(dataset_masks_path)]\n",
    "dataset_original_img_path_list = [os.path.join(dataset_img_path, f) for f in os.listdir(dataset_img_path)]\n",
    "\n",
    "df_annotations = pd.DataFrame(\n",
    "    {\n",
    "        \"tile_id\": [os.path.basename(f).split(\".\")[0] for f in dataset_original_masks_path_list],\n",
    "        \"original_mask_path_png\": dataset_original_masks_path_list,\n",
    "        \"original_img_path_png\": dataset_original_img_path_list,\n",
    "    }\n",
    ")\n",
    "# Extract tile_id from filename\n",
    "df_annotations[\"tile_id\"] = df_annotations[\"tile_id\"].apply(lambda x: \"_\".join(x.split(\"_\")[2:]))\n",
    "\n",
    "# Validate data integrity\n",
    "assert(len(df_annotations[df_annotations.duplicated(subset=[\"tile_id\"])]) == 0), f\"gdf_dataset has duplicates in tile_id: {df_annotations[df_annotations.duplicated(subset=['tile_id'])]}\"\n",
    "assert(len(df_annotations[df_annotations.duplicated(subset=[\"original_mask_path_png\"])]) == 0), f\"gdf_dataset has duplicates in original_mask_path_png: {df_annotations[df_annotations.duplicated(subset=['original_mask_path_png'])]}\"\n",
    "assert(len(df_annotations[df_annotations.duplicated(subset=[\"original_img_path_png\"])]) == 0), f\"gdf_dataset has duplicates in original_img_path_png: {df_annotations[df_annotations.duplicated(subset=['original_img_path_png'])]}\"\n",
    "\n",
    "# Check for null values\n",
    "assert(df_annotations[\"tile_id\"].notnull().all()), f\"df_annotations has null values in tile_id: {df_annotations[df_annotations[\"tile_id\"].isnull()]}\"\n",
    "assert(df_annotations[\"original_mask_path_png\"].notnull().all()), f\"df_annotations has null values in original_mask_path_png: {df_annotations[df_annotations[\"original_mask_path_png\"].isnull()]}\"\n",
    "assert(df_annotations[\"original_img_path_png\"].notnull().all()), f\"df_annotations has null values in original_img_path_png: {df_annotations[df_annotations[\"original_img_path_png\"].isnull()]}\"\n",
    "\n",
    "# Verify counts match\n",
    "assert(len(df_annotations[\"tile_id\"]) == len(gdf_dataset[\"tile_id\"])), f\"len(df_annotations['tile_id']) is not equal to len(gdf_dataset['tile_id']): {len(df_annotations['tile_id'])} != {len(gdf_dataset['tile_id'])}\"\n",
    "\n",
    "display(df_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge annotation paths with dataset\n",
    "gdf_dataset = gdf_dataset.merge(\n",
    "    df_annotations,\n",
    "    how=\"left\",\n",
    "    left_on=\"tile_id\",\n",
    "    right_on=\"tile_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate merged data\n",
    "assert(len(gdf_dataset[gdf_dataset.duplicated(subset=[\"tile_id\"])]) == 0), f\"gdf_dataset has duplicates in tile_id: {gdf_dataset[gdf_dataset.duplicated(subset=['tile_id'])]}\"\n",
    "assert(len(gdf_dataset[gdf_dataset.duplicated(subset=[\"original_img_path_png\"])]) == 0), f\"gdf_dataset has duplicates in img_path: {gdf_dataset[gdf_dataset.duplicated(subset=['original_img_path_png'])]}\"\n",
    "assert(len(gdf_dataset[gdf_dataset.duplicated(subset=[\"original_mask_path_png\"])]) == 0), f\"gdf_dataset has duplicates in original_mask_path_png: {gdf_dataset[gdf_dataset.duplicated(subset=['original_mask_path_png'])]}\"\n",
    "\n",
    "# Check for null values\n",
    "assert(gdf_dataset[\"tile_id\"].notnull().all()), f\"gdf_dataset has null values in tile_id: {gdf_dataset[gdf_dataset[\"tile_id\"].isnull()]}\"\n",
    "assert(gdf_dataset[\"original_img_path_png\"].notnull().all()), f\"gdf_dataset has null values in original_img_path_png: {gdf_dataset[gdf_dataset[\"original_img_path_png\"].isnull()]}\"\n",
    "assert(gdf_dataset[\"original_mask_path_png\"].notnull().all()), f\"gdf_dataset has null values in original_mask_path_png: {gdf_dataset[gdf_dataset[\"original_mask_path_png\"].isnull()]}\"\n",
    "\n",
    "# Verify record counts\n",
    "assert(len(gdf_dataset[\"tile_id\"]) == len(df_annotations[\"tile_id\"])), f\"len(gdf_dataset['tile_id']) is not equal to len(df_annotations['tile_id']): {len(gdf_dataset['tile_id'])} != {len(df_annotations['tile_id'])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_cad_batiment_horsol = gpd.read_parquet(CAD_BATIMENT_HORSOL_TOIT_MERGE_PARQUET_PATH)\n",
    "print(type(gdf_cad_batiment_horsol))\n",
    "gdf_cad_batiment_horsol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich Dataset with Spatial Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geometry_from_tiff(tiff_path, crs=CORRECT_CRS):\n",
    "    \"\"\"\n",
    "    Extract spatial geometry from a GeoTIFF file.\n",
    "    \n",
    "    Creates a polygon representing the spatial extent of the GeoTIFF\n",
    "    based on its georeferencing information.\n",
    "    \n",
    "    Parameters:\n",
    "        tiff_path: Path to GeoTIFF file\n",
    "        crs: Expected coordinate reference system\n",
    "        \n",
    "    Returns:\n",
    "        Polygon: Spatial footprint of the GeoTIFF, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(tiff_path) as src:\n",
    "            # Extract geospatial information\n",
    "            transform = src.transform\n",
    "            height, width = src.shape\n",
    "            crs_src = src.crs\n",
    "            \n",
    "            # Check CRS consistency\n",
    "            if crs_src != crs:\n",
    "                print(f\"Warning: CRS mismatch in {tiff_path}. Found {crs_src}, expected {crs}\")\n",
    "            \n",
    "            # Calculate corner coordinates\n",
    "            minx = transform[2]\n",
    "            maxy = transform[5]\n",
    "            miny = maxy + height * transform[4]\n",
    "            maxx = minx + width * transform[0]\n",
    "            \n",
    "            # Create polygon\n",
    "            polygon = Polygon([\n",
    "                (minx, miny), (maxx, miny), (maxx, maxy), (minx, maxy), (minx, miny)\n",
    "            ])\n",
    "            \n",
    "            # Validate geometry\n",
    "            if not polygon.is_valid:\n",
    "                print(f\"Warning: Invalid polygon created from {tiff_path}\")\n",
    "                polygon = polygon.buffer(0)  # Attempt to fix invalid geometry\n",
    "            \n",
    "            if polygon.area <= 0:\n",
    "                print(f\"Warning: Zero-area polygon created from {tiff_path}\")\n",
    "                return None\n",
    "                \n",
    "            return polygon\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {tiff_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_image_dimensions(image_path):\n",
    "    \"\"\"\n",
    "    Get dimensions of an image file.\n",
    "    \n",
    "    Parameters:\n",
    "        image_path: Path to image file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (width, height) or (None, None) if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(image_path):\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                return width, height\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {image_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def create_geodataframe_from_tiffs(df):\n",
    "    \"\"\"\n",
    "    Create a GeoDataFrame from TIF file paths with spatial geometries.\n",
    "    \n",
    "    Processes each GeoTIFF to extract its spatial footprint and creates\n",
    "    a GeoDataFrame with proper geometries for spatial operations.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame containing 'tile_path' column\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame: Input dataframe enhanced with geometry column\n",
    "    \"\"\"\n",
    "    # Initialize containers for geometries\n",
    "    geometries = []\n",
    "    indices = []\n",
    "    \n",
    "    # Process each file\n",
    "    print(\"Processing GeoTIFF files...\")\n",
    "    total_files = len(df)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        tiff_path = row['tile_path']\n",
    "        \n",
    "        # Verify file exists\n",
    "        if os.path.exists(tiff_path):\n",
    "            # Extract geometry\n",
    "            geometry = get_geometry_from_tiff(tiff_path)\n",
    "            \n",
    "            # Store valid results\n",
    "            if geometry is not None:\n",
    "                geometries.append(geometry)\n",
    "                indices.append(idx)\n",
    "        else:\n",
    "            print(f\"File not found: {tiff_path}\")\n",
    "    \n",
    "    print(f\"Successfully processed {len(geometries)} out of {total_files} files\")\n",
    "    \n",
    "    # Create filtered dataframe with valid geometries\n",
    "    df_processed = df.loc[indices].copy()\n",
    "    \n",
    "    # Create GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df_processed,\n",
    "        geometry=geometries,\n",
    "        crs=EPSG_SUISSE\n",
    "    )\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Process dataset\n",
    "gdf_dataset = create_geodataframe_from_tiffs(gdf_dataset)\n",
    "\n",
    "# Add image dimensions\n",
    "image_dimensions = gdf_dataset['tile_path'].apply(get_image_dimensions)\n",
    "gdf_dataset['image_width'], gdf_dataset['image_height'] = zip(*image_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate image dimensions\n",
    "assert(gdf_dataset[\"image_width\"].notnull().all()), f\"gdf_dataset has null values in image_width: {gdf_dataset[gdf_dataset['image_width'].isnull()]}\"\n",
    "assert(gdf_dataset[\"image_height\"].notnull().all()), f\"gdf_dataset has null values in image_height: {gdf_dataset[gdf_dataset['image_height'].isnull()]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gdf_dataset))\n",
    "gdf_dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset Images\n",
    "### Clip Images Using Building Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_geotiff_and_png_masks(gdf_dataset, gdf_buildings, output_img_dir, output_mask_dir, convert_masks_to_geotiff=True):\n",
    "    \"\"\"\n",
    "    Clip GeoTIFF images and masks using building polygons.\n",
    "    \n",
    "    Removes pixels outside building footprints by setting them to zero.\n",
    "    Ensures image and mask pairs maintain identical dimensions and alignment.\n",
    "    \n",
    "    Parameters:\n",
    "        gdf_dataset: GeoDataFrame with tile paths and geometries\n",
    "        gdf_buildings: GeoDataFrame containing building polygons\n",
    "        output_img_dir: Directory to save clipped images\n",
    "        output_mask_dir: Directory to save clipped masks\n",
    "        convert_masks_to_geotiff: Convert PNG masks to GeoTIFF format\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (processed_tile_ids, skipped_tiles_dict)\n",
    "    \"\"\"\n",
    "    # Track progress\n",
    "    successful_img_count = 0\n",
    "    successful_mask_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Track processing status\n",
    "    processed_tile_ids = []\n",
    "    skipped_tile_ids = []\n",
    "    skipped_reasons = {}\n",
    "    \n",
    "    # Process each file pair\n",
    "    for idx, row in tqdm(gdf_dataset.iterrows(), total=len(gdf_dataset), desc=\"Clipping files\"):\n",
    "        # Get file paths\n",
    "        tiff_path = row['tile_path']\n",
    "        mask_path = row.get('original_mask_path_png')\n",
    "        tile_id = row['tile_id']\n",
    "        \n",
    "        # Validate paths\n",
    "        if pd.isna(tiff_path):\n",
    "            skipped_tile_ids.append(tile_id)\n",
    "            skipped_reasons[tile_id] = \"Missing tiff_path\"\n",
    "            continue\n",
    "            \n",
    "        if not os.path.exists(tiff_path):\n",
    "            skipped_tile_ids.append(tile_id)\n",
    "            skipped_reasons[tile_id] = f\"TIFF file not found: {tiff_path}\"\n",
    "            continue\n",
    "            \n",
    "        # Create output paths\n",
    "        output_img_path = os.path.join(output_img_dir, os.path.basename(tiff_path))\n",
    "        \n",
    "        # Handle mask path\n",
    "        if pd.isna(mask_path):\n",
    "            mask_path = None\n",
    "            output_mask_path = None\n",
    "            skipped_reasons[tile_id] = \"Missing mask_path\"\n",
    "        elif not os.path.exists(mask_path):\n",
    "            mask_path = None\n",
    "            output_mask_path = None\n",
    "            skipped_reasons[tile_id] = f\"Mask file not found: {mask_path}\"\n",
    "        else:\n",
    "            # Change extension if converting to GeoTIFF\n",
    "            if convert_masks_to_geotiff:\n",
    "                mask_basename = os.path.splitext(os.path.basename(mask_path))[0] + '.tif'\n",
    "                output_mask_path = os.path.join(output_mask_dir, mask_basename)\n",
    "            else:\n",
    "                output_mask_path = os.path.join(output_mask_dir, os.path.basename(mask_path))\n",
    "        \n",
    "        try:\n",
    "            # Get tile geometry\n",
    "            tile_geom = row.geometry\n",
    "            \n",
    "            if tile_geom is None:\n",
    "                skipped_tile_ids.append(tile_id)\n",
    "                skipped_reasons[tile_id] = \"Missing geometry\"\n",
    "                continue\n",
    "                \n",
    "            # Find intersecting buildings\n",
    "            buildings_in_tile = gdf_buildings[gdf_buildings.intersects(tile_geom)]\n",
    "            \n",
    "            if len(buildings_in_tile) == 0:\n",
    "                skipped_tile_ids.append(tile_id)\n",
    "                skipped_reasons[tile_id] = \"No intersecting buildings found\"\n",
    "                continue\n",
    "            \n",
    "            # Process GeoTIFF and mask together\n",
    "            with rasterio.open(tiff_path) as src:\n",
    "                # Get source metadata\n",
    "                src_meta = src.meta.copy()\n",
    "                original_height, original_width = src.height, src.width\n",
    "                \n",
    "                # Convert building geometries to GeoJSON format\n",
    "                shapes = [mapping(geom) for geom in buildings_in_tile.geometry]\n",
    "                \n",
    "                # Create mask for clipping\n",
    "                masked_data, mask_transform = rasterio.mask.mask(\n",
    "                    src, \n",
    "                    shapes, \n",
    "                    crop=False, \n",
    "                    all_touched=True,\n",
    "                    invert=True,\n",
    "                    filled=True,\n",
    "                    nodata=0\n",
    "                )\n",
    "                \n",
    "                # Create binary mask\n",
    "                binary_mask = (masked_data[0] == 0).astype(np.uint8)\n",
    "                \n",
    "                # Apply mask to all bands\n",
    "                original_img = src.read()\n",
    "                masked_img = original_img.copy()\n",
    "                \n",
    "                for i in range(masked_img.shape[0]):\n",
    "                    masked_img[i][binary_mask == 0] = 0\n",
    "                \n",
    "                # Update metadata\n",
    "                out_meta = src_meta.copy()\n",
    "                \n",
    "                # Write masked image\n",
    "                with rasterio.open(output_img_path, 'w', **out_meta) as dest:\n",
    "                    dest.write(masked_img)\n",
    "                \n",
    "                successful_img_count += 1\n",
    "                \n",
    "                # Process mask if it exists\n",
    "                if mask_path is not None:\n",
    "                    try:\n",
    "                        # Open PNG mask\n",
    "                        with Image.open(mask_path) as mask_img:\n",
    "                            mask_array = np.array(mask_img)\n",
    "                            \n",
    "                            # Check dimension consistency\n",
    "                            if mask_array.shape[:2] != (original_height, original_width):\n",
    "                                tiff_dims = f\"{original_width}x{original_height}\"\n",
    "                                mask_dims = f\"{mask_array.shape[1]}x{mask_array.shape[0]}\"\n",
    "                                print(f\"Warning: Mask dimensions don't match GeoTIFF for {os.path.basename(tiff_path)}\")\n",
    "                                print(f\"  - GeoTIFF dimensions: {tiff_dims}\")\n",
    "                                print(f\"  - Mask dimensions: {mask_dims}\")\n",
    "                                skipped_reasons[tile_id] = f\"Mask dimensions don't match GeoTIFF: GeoTIFF={tiff_dims}, Mask={mask_dims}\"\n",
    "                                continue\n",
    "\n",
    "                            # Apply same binary mask to ensure identical dimensions\n",
    "                            if len(mask_array.shape) == 3:  # RGB or RGBA\n",
    "                                for i in range(mask_array.shape[2]):\n",
    "                                    mask_array[:, :, i] = mask_array[:, :, i] * binary_mask\n",
    "                            else:  # Grayscale\n",
    "                                mask_array = mask_array * binary_mask\n",
    "                            \n",
    "                            if convert_masks_to_geotiff:\n",
    "                                # Create GeoTIFF metadata for mask\n",
    "                                mask_meta = src_meta.copy()\n",
    "                                \n",
    "                                # Update metadata\n",
    "                                if len(mask_array.shape) == 3:  # RGB or RGBA\n",
    "                                    mask_meta.update(\n",
    "                                        dtype=mask_array.dtype,\n",
    "                                        count=mask_array.shape[2],\n",
    "                                        nodata=0,\n",
    "                                    )\n",
    "                                else:  # Grayscale\n",
    "                                    mask_meta.update(\n",
    "                                        dtype=mask_array.dtype,\n",
    "                                        count=1,\n",
    "                                        nodata=0,\n",
    "                                    )\n",
    "                                \n",
    "                                # Write mask as GeoTIFF\n",
    "                                with rasterio.open(output_mask_path, 'w', **mask_meta) as dest:\n",
    "                                    if len(mask_array.shape) == 3:  # RGB or RGBA\n",
    "                                        for i in range(mask_array.shape[2]):\n",
    "                                            dest.write(mask_array[:, :, i], i+1)\n",
    "                                    else:  # Grayscale\n",
    "                                        dest.write(mask_array, 1)\n",
    "                            else:\n",
    "                                # Save as PNG\n",
    "                                Image.fromarray(mask_array).save(output_mask_path)\n",
    "                                \n",
    "                            successful_mask_count += 1\n",
    "                            \n",
    "                            # Mark as successfully processed\n",
    "                            processed_tile_ids.append(tile_id)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        error_count += 1\n",
    "                        skipped_tile_ids.append(tile_id)\n",
    "                        skipped_reasons[tile_id] = f\"Error processing mask: {str(e)}\"\n",
    "                        print(f\"Error processing mask {mask_path}: {e}\")\n",
    "                else:\n",
    "                    # No mask but image processed successfully\n",
    "                    processed_tile_ids.append(tile_id)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            skipped_tile_ids.append(tile_id)\n",
    "            skipped_reasons[tile_id] = f\"Error: {str(e)}\"\n",
    "            print(f\"Error processing {tiff_path}: {e}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    processed_tile_ids = list(set(processed_tile_ids))\n",
    "    skipped_tile_ids = list(set(skipped_tile_ids))\n",
    "    \n",
    "    # Check for overlap between processed and skipped\n",
    "    overlap = set(processed_tile_ids) & set(skipped_tile_ids)\n",
    "    \n",
    "    print(f\"Completed processing {len(gdf_dataset)} files:\")\n",
    "    print(f\"- Successfully processed {successful_img_count} GeoTIFFs\")\n",
    "    print(f\"- Successfully processed {successful_mask_count} masks\")\n",
    "    if convert_masks_to_geotiff:\n",
    "        print(f\"- Converted {successful_mask_count} PNG masks to GeoTIFF format\")\n",
    "    print(f\"- Encountered {error_count} errors\")\n",
    "    print(f\"- Successfully processed tiles: {len(processed_tile_ids)}\")\n",
    "    print(f\"- Skipped tiles: {len(skipped_tile_ids)}\")\n",
    "    \n",
    "    if overlap:\n",
    "        print(f\"Warning: {len(overlap)} tile_ids appear in both processed and skipped lists\")\n",
    "     \n",
    "    # Analyze skipped reasons\n",
    "    if skipped_tile_ids:\n",
    "        # Group dimension mismatches\n",
    "        dimension_mismatches = [reason for tile_id, reason in skipped_reasons.items() \n",
    "                            if \"Mask dimensions don't match\" in reason]\n",
    "        \n",
    "        print(\"\\nDimension mismatch summary:\")\n",
    "        print(f\"  - Total files with dimension mismatches: {len(dimension_mismatches)}\")\n",
    "        \n",
    "        # Analyze dimension patterns\n",
    "        if dimension_mismatches:\n",
    "            import re\n",
    "            geotiff_dims = []\n",
    "            mask_dims = []\n",
    "            pattern = r\"GeoTIFF=(\\d+x\\d+), Mask=(\\d+x\\d+)\"\n",
    "            \n",
    "            for reason in dimension_mismatches:\n",
    "                match = re.search(pattern, reason)\n",
    "                if match:\n",
    "                    geotiff_dims.append(match.group(1))\n",
    "                    mask_dims.append(match.group(2))\n",
    "            \n",
    "            # Count frequency\n",
    "            from collections import Counter\n",
    "            geotiff_counter = Counter(geotiff_dims)\n",
    "            mask_counter = Counter(mask_dims)\n",
    "            \n",
    "            print(\"\\nMost common GeoTIFF dimensions:\")\n",
    "            for dims, count in geotiff_counter.most_common(3):\n",
    "                print(f\"  - {dims}: {count} files\")\n",
    "                \n",
    "            print(\"\\nMost common mask dimensions:\")\n",
    "            for dims, count in mask_counter.most_common(3):\n",
    "                print(f\"  - {dims}: {count} files\")\n",
    "        \n",
    "        # Convert to dictionary\n",
    "        skipped_tiles = {tile_id: skipped_reasons[tile_id] for tile_id in skipped_tile_ids}\n",
    "        \n",
    "        return processed_tile_ids, skipped_tiles\n",
    "\n",
    "# Execute clipping\n",
    "processed_tile_ids, skipped_tiles = clip_geotiff_and_png_masks(\n",
    "    gdf_dataset, \n",
    "    gdf_cad_batiment_horsol, \n",
    "    DATASET_OUTPUT_IMG_PATH, \n",
    "    DATASET_OUTPUT_MASKS_PATH, \n",
    "    convert_masks_to_geotiff=True\n",
    ")\n",
    "\n",
    "print(f\"Processed tile_ids: {len(processed_tile_ids)}\")\n",
    "print(f\"Skipped tile_ids: {len(skipped_tiles)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Dataset with Processed File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of processed files\n",
    "dataset_processed_masks_path_list = [os.path.join(DATASET_OUTPUT_MASKS_PATH, f) for f in os.listdir(DATASET_OUTPUT_MASKS_PATH)]\n",
    "dataset_processed_img_path_list = [os.path.join(DATASET_OUTPUT_IMG_PATH, f) for f in os.listdir(DATASET_OUTPUT_IMG_PATH)]\n",
    "\n",
    "df_processed = pd.DataFrame(\n",
    "    {\n",
    "        \"tile_id\": [os.path.basename(f).split(\".\")[0] for f in dataset_processed_masks_path_list],\n",
    "        \"processed_mask_path_tif\": dataset_processed_masks_path_list,\n",
    "        \"processed_img_path_tif\": dataset_processed_img_path_list,\n",
    "    }\n",
    ")\n",
    "# Extract tile_id\n",
    "df_processed[\"tile_id\"] = df_processed[\"tile_id\"].apply(lambda x: \"_\".join(x.split(\"_\")[2:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate processed data\n",
    "assert(len(df_processed[df_processed.duplicated(subset=[\"tile_id\"])]) == 0), f\"df_processed has duplicates in tile_id: {df_processed[df_processed.duplicated(subset=['tile_id'])]}\"\n",
    "assert(len(df_processed[df_processed.duplicated(subset=[\"processed_img_path_tif\"])]) == 0), f\"df_processed has duplicates in processed_img_path_tif: {df_processed[df_processed.duplicated(subset=['processed_img_path_tif'])]}\"\n",
    "assert(len(df_processed[df_processed.duplicated(subset=[\"processed_mask_path_tif\"])]) == 0), f\"df_processed has duplicates in processed_mask_path_tif: {df_processed[df_processed.duplicated(subset=['processed_mask_path_tif'])]}\"\n",
    "\n",
    "# Check for null values\n",
    "assert(df_processed[\"tile_id\"].isnull().sum() == 0), f\"df_processed has null in tile_id: {df_processed[df_processed['tile_id'].isnull()]}\"\n",
    "assert(df_processed[\"processed_img_path_tif\"].isnull().sum() == 0), f\"df_processed has null in processed_img_path_tif: {df_processed[df_processed['processed_img_path_tif'].isnull()]}\"\n",
    "assert(df_processed[\"processed_mask_path_tif\"].isnull().sum() == 0), f\"df_processed has null in processed_mask_path_tif: {df_processed[df_processed['processed_mask_path_tif'].isnull()]}\"\n",
    "\n",
    "display(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge processed paths\n",
    "gdf_dataset = gdf_dataset.merge(\n",
    "    df_processed,\n",
    "    how=\"left\",\n",
    "    left_on=\"tile_id\",\n",
    "    right_on=\"tile_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Tile Overlaps\n",
    "#### Detect Overlap Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_relative_position(geom1, geom2, tolerance=0.5):\n",
    "    \"\"\"\n",
    "    Determine the relative position of geom1 with respect to geom2.\n",
    "    \n",
    "    Accounts for Swiss coordinate system (EPSG:2056) where Y increases northward.\n",
    "    Note: QGIS display orientation differs from coordinate values.\n",
    "    \n",
    "    Parameters:\n",
    "        geom1: First geometry\n",
    "        geom2: Second geometry  \n",
    "        tolerance: Tolerance factor for position determination\n",
    "        \n",
    "    Returns:\n",
    "        str: Relative position descriptor\n",
    "    \"\"\"\n",
    "    # Get bounding boxes\n",
    "    minx1, miny1, maxx1, maxy1 = geom1.bounds\n",
    "    minx2, miny2, maxx2, maxy2 = geom2.bounds\n",
    "    \n",
    "    # Calculate centers\n",
    "    center_x1 = (minx1 + maxx1) / 2\n",
    "    center_y1 = (miny1 + maxy1) / 2\n",
    "    center_x2 = (minx2 + maxx2) / 2\n",
    "    center_y2 = (miny2 + maxy2) / 2\n",
    "    \n",
    "    # Calculate average dimensions for tolerance\n",
    "    avg_width = ((maxx1 - minx1) + (maxx2 - minx2)) / 2\n",
    "    avg_height = ((maxy1 - miny1) + (maxy2 - miny2)) / 2\n",
    "    \n",
    "    # Scale tolerance by dimensions\n",
    "    x_tolerance = tolerance * avg_width\n",
    "    y_tolerance = tolerance * avg_height\n",
    "    \n",
    "    # Calculate overlap percentage\n",
    "    intersection = geom1.intersection(geom2)\n",
    "    intersection_area = intersection.area\n",
    "    \n",
    "    # Initialize position components\n",
    "    vertical_position = None\n",
    "    horizontal_position = None\n",
    "    \n",
    "    # Determine vertical position (Y-axis)\n",
    "    vertical_diff = center_y1 - center_y2\n",
    "    if abs(vertical_diff) <= y_tolerance:\n",
    "        vertical_position = None\n",
    "    elif vertical_diff > 0:\n",
    "        vertical_position = \"bottom\"  # geom1 north of geom2\n",
    "    else:\n",
    "        vertical_position = \"top\"     # geom1 south of geom2\n",
    "    \n",
    "    # Determine horizontal position (X-axis)\n",
    "    horizontal_diff = center_x1 - center_x2\n",
    "    if abs(horizontal_diff) <= x_tolerance:\n",
    "        horizontal_position = None\n",
    "    elif horizontal_diff > 0:\n",
    "        horizontal_position = \"left\"   # geom1 east of geom2\n",
    "    else:\n",
    "        horizontal_position = \"right\"  # geom1 west of geom2\n",
    "    \n",
    "    # Calculate overlap percentage\n",
    "    smaller_area = min(geom1.area, geom2.area)\n",
    "    overlap_percentage = (intersection_area / smaller_area) * 100 if smaller_area > 0 else 0\n",
    "    \n",
    "    # Combine positions\n",
    "    if vertical_position and horizontal_position:\n",
    "        position = f\"{vertical_position}-{horizontal_position}\"\n",
    "    elif vertical_position:\n",
    "        position = vertical_position\n",
    "    elif horizontal_position:\n",
    "        position = horizontal_position\n",
    "    else:\n",
    "        position = \"substantial-overlap\" if overlap_percentage > 90 else \"center\"\n",
    "    \n",
    "    return position\n",
    "\n",
    "\n",
    "def get_opposite_position(position):\n",
    "    \"\"\"\n",
    "    Get the opposite relative position.\n",
    "    \n",
    "    Parameters:\n",
    "        position: Current position string\n",
    "        \n",
    "    Returns:\n",
    "        str: Opposite position\n",
    "    \"\"\"\n",
    "    position_map = {\n",
    "        'top': 'bottom',\n",
    "        'bottom': 'top',\n",
    "        'left': 'right',\n",
    "        'right': 'left',\n",
    "        'top-left': 'bottom-right',\n",
    "        'top-right': 'bottom-left',\n",
    "        'bottom-left': 'top-right',\n",
    "        'bottom-right': 'top-left',\n",
    "        'center': 'center',\n",
    "        'substantial-overlap': 'substantial-overlap'\n",
    "    }\n",
    "    return position_map.get(position, position)\n",
    "\n",
    "\n",
    "def check_geotiffs_overlap(geom1, geom2, min_overlap_area=0.0):\n",
    "    \"\"\"\n",
    "    Calculate overlap information between two geometries.\n",
    "    \n",
    "    Parameters:\n",
    "        geom1: First geometry\n",
    "        geom2: Second geometry\n",
    "        min_overlap_area: Minimum area to consider as overlap\n",
    "        \n",
    "    Returns:\n",
    "        dict: Overlap information including area and position\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'overlaps': False,\n",
    "        'overlap_area': 0.0,\n",
    "        'relative_position': None,\n",
    "        'overlap_percentage_1': 0.0,\n",
    "        'overlap_percentage_2': 0.0\n",
    "    }\n",
    "    \n",
    "    if geom1.intersects(geom2):\n",
    "        intersection = geom1.intersection(geom2)\n",
    "        overlap_area = intersection.area\n",
    "        \n",
    "        if overlap_area > min_overlap_area:\n",
    "            result['overlaps'] = True\n",
    "            result['overlap_area'] = overlap_area\n",
    "            result['relative_position'] = determine_relative_position(geom1, geom2)\n",
    "            result['overlap_percentage_1'] = (overlap_area / geom1.area) * 100\n",
    "            result['overlap_percentage_2'] = (overlap_area / geom2.area) * 100\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def check_overlaps_in_dataframe(gdf, min_overlap_area=1.0, include_symmetric=False, buffer_distance=0.01):\n",
    "    \"\"\"\n",
    "    Check for overlapping geometries in a GeoDataFrame.\n",
    "    \n",
    "    Uses spatial indexing for efficient overlap detection and handles\n",
    "    buffered geometries for near-overlaps.\n",
    "    \n",
    "    Parameters:\n",
    "        gdf: GeoDataFrame with geometry column\n",
    "        min_overlap_area: Minimum overlap area to report\n",
    "        include_symmetric: Include both directions of overlap\n",
    "        buffer_distance: Buffer to apply for near-overlap detection\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Overlap information for each pair\n",
    "    \"\"\"\n",
    "    overlap_results = []\n",
    "    n = len(gdf)\n",
    "    \n",
    "    try:\n",
    "        if not isinstance(gdf, gpd.GeoDataFrame):\n",
    "            raise TypeError(\"Input must be a GeoDataFrame\")\n",
    "        \n",
    "        if n == 0:\n",
    "            raise ValueError(\"GeoDataFrame is empty\")\n",
    "        \n",
    "        print(\"Creating spatial index...\")\n",
    "        sindex = gdf.sindex\n",
    "        \n",
    "        print(f\"Checking overlaps among {n} geometries...\")\n",
    "        with tqdm(total=n, desc=\"Checking overlaps\") as pbar:\n",
    "            for i in range(n):\n",
    "                geom1 = gdf.iloc[i]['geometry']\n",
    "                tile_id1 = gdf.iloc[i]['tile_id']\n",
    "                \n",
    "                if geom1 is None or not geom1.is_valid:\n",
    "                    print(f\"Warning: Skipping invalid geometry for {tile_id1}\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Find potential matches using spatial index\n",
    "                bbox = geom1.bounds\n",
    "                potential_matches_idx = list(sindex.intersection(bbox))\n",
    "                \n",
    "                # Remove self-intersection\n",
    "                if i in potential_matches_idx:\n",
    "                    potential_matches_idx.remove(i)\n",
    "                    \n",
    "                # Only check pairs once\n",
    "                potential_matches_idx = [j for j in potential_matches_idx if j > i]\n",
    "                \n",
    "                for j in potential_matches_idx:\n",
    "                    geom2 = gdf.iloc[j]['geometry']\n",
    "                    tile_id2 = gdf.iloc[j]['tile_id']\n",
    "                    \n",
    "                    if geom2 is None or not geom2.is_valid:\n",
    "                        print(f\"Warning: Skipping invalid geometry for {tile_id2}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Apply buffer if specified\n",
    "                    if buffer_distance > 0:\n",
    "                        buffered_geom1 = geom1.buffer(buffer_distance)\n",
    "                        buffered_geom2 = geom2.buffer(buffer_distance)\n",
    "                    else:\n",
    "                        buffered_geom1 = geom1\n",
    "                        buffered_geom2 = geom2\n",
    "                    \n",
    "                    if buffered_geom1.intersects(buffered_geom2):\n",
    "                        intersection = buffered_geom1.intersection(buffered_geom2)\n",
    "                        \n",
    "                        if not intersection.is_empty and intersection.area > min_overlap_area:\n",
    "                            result = check_geotiffs_overlap(buffered_geom1, buffered_geom2, min_overlap_area)\n",
    "                            \n",
    "                            if result['overlaps']:\n",
    "                                overlap_results.append({\n",
    "                                    'tile_id1': tile_id1,\n",
    "                                    'tile_id2': tile_id2,\n",
    "                                    'index1': i,\n",
    "                                    'index2': j,\n",
    "                                    'overlap_area': result['overlap_area'],\n",
    "                                    'relative_position': result['relative_position'],\n",
    "                                    'overlap_percentage_1': result['overlap_percentage_1'],\n",
    "                                    'overlap_percentage_2': result['overlap_percentage_2'],\n",
    "                                    'buffered': buffer_distance > 0\n",
    "                                })\n",
    "                                \n",
    "                                if include_symmetric:\n",
    "                                    opposite_position = get_opposite_position(result['relative_position'])\n",
    "                                    \n",
    "                                    overlap_results.append({\n",
    "                                        'tile_id1': tile_id2,\n",
    "                                        'tile_id2': tile_id1,\n",
    "                                        'index1': j,\n",
    "                                        'index2': i,\n",
    "                                        'overlap_area': result['overlap_area'],\n",
    "                                        'relative_position': opposite_position,\n",
    "                                        'overlap_percentage_1': result['overlap_percentage_2'],\n",
    "                                        'overlap_percentage_2': result['overlap_percentage_1'],\n",
    "                                        'buffered': buffer_distance > 0\n",
    "                                    })\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during overlap check: {e}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    if overlap_results:\n",
    "        overlap_df = pd.DataFrame(overlap_results)\n",
    "        print(f\"Found {len(overlap_df)} overlapping pairs\")\n",
    "        \n",
    "        # Summary by position\n",
    "        position_counts = overlap_df['relative_position'].value_counts()\n",
    "        print(\"\\nOverlap positions before filtering:\")\n",
    "        for pos, count in position_counts.items():\n",
    "            print(f\"  {pos}: {count}\")\n",
    "            \n",
    "        if 'buffered' in overlap_df.columns:\n",
    "            buffered_count = overlap_df['buffered'].sum()\n",
    "            print(f\"\\nOverlaps using buffered geometries: {buffered_count} ({(buffered_count/len(overlap_df))*100:.1f}%)\")\n",
    "            \n",
    "        return overlap_df\n",
    "    else:\n",
    "        print(\"No overlapping pairs found\")\n",
    "        return pd.DataFrame(columns=['tile_id1', 'tile_id2', 'index1', 'index2', \n",
    "                                    'overlap_area', 'relative_position', \n",
    "                                    'overlap_percentage_1', 'overlap_percentage_2',\n",
    "                                    'buffered'])\n",
    "\n",
    "# Detect overlaps\n",
    "overlap_df = check_overlaps_in_dataframe(\n",
    "    gdf_dataset, \n",
    "    min_overlap_area=1.0, \n",
    "    include_symmetric=True, \n",
    "    buffer_distance=BUFFER_DISTANCE\n",
    ")\n",
    "\n",
    "# Display position types\n",
    "print(\"\\nUnique position types found:\")\n",
    "for pos in sorted(overlap_df['relative_position'].unique()):\n",
    "    print(f\"  {pos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Overlapping Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_overlap_in_geotiffs(overlap_df, gdf_dataset, overlap_positions=None, overwrite=True, buffer_distance=0.01):\n",
    "    \"\"\"\n",
    "    Remove overlapping regions by setting pixels to zero in one of the tiles.\n",
    "    \n",
    "    For overlapping tile pairs, determines which tile should have its overlap\n",
    "    region set to background (zero) based on relative position.\n",
    "    \n",
    "    Parameters:\n",
    "        overlap_df: DataFrame with overlap information\n",
    "        gdf_dataset: GeoDataFrame with file paths and geometries\n",
    "        overlap_positions: List of positions to process\n",
    "        overwrite: Whether to overwrite original files\n",
    "        buffer_distance: Buffer distance for overlap calculation\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Information about processed files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default positions to process\n",
    "    if overlap_positions is None:\n",
    "        overlap_positions = ['right', 'top', 'top-right', 'bottom-right']\n",
    "    \n",
    "    # Track processed files\n",
    "    processed_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    if len(overlap_df) == 0:\n",
    "        print(\"No overlaps to process\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter by position\n",
    "    filtered_df = overlap_df.copy()\n",
    "    if overlap_positions:\n",
    "        position_filter = filtered_df['relative_position'].apply(\n",
    "            lambda pos: any(p in pos for p in overlap_positions)\n",
    "        )\n",
    "        filtered_df = filtered_df[position_filter]\n",
    "        print(f\"Processing {len(filtered_df)} out of {len(overlap_df)} overlaps that match position criteria\")\n",
    "    \n",
    "    if len(filtered_df) == 0:\n",
    "        print(\"No overlaps match the specified positions\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Process each overlap\n",
    "    with tqdm(total=len(filtered_df), desc=\"Processing overlaps\") as pbar:\n",
    "        for idx, row in filtered_df.iterrows():\n",
    "            # Get indices and position\n",
    "            index1 = row['index1']\n",
    "            index2 = row['index2']\n",
    "            position = row['relative_position']\n",
    "            \n",
    "            # Get file paths\n",
    "            tiff_path1 = gdf_dataset.iloc[index1]['processed_img_path_tif']\n",
    "            tiff_path2 = gdf_dataset.iloc[index2]['processed_img_path_tif']\n",
    "            \n",
    "            mask_path1 = gdf_dataset.iloc[index1]['processed_mask_path_tif']\n",
    "            mask_path2 = gdf_dataset.iloc[index2]['processed_mask_path_tif']\n",
    "            \n",
    "            # Get geometries\n",
    "            geom1 = gdf_dataset.iloc[index1]['geometry']\n",
    "            geom2 = gdf_dataset.iloc[index2]['geometry']\n",
    "            \n",
    "            # Apply buffer if needed\n",
    "            use_buffer = row.get('buffered', True)\n",
    "            \n",
    "            if use_buffer:\n",
    "                buffered_geom1 = geom1.buffer(buffer_distance)\n",
    "                buffered_geom2 = geom2.buffer(buffer_distance)\n",
    "            else:\n",
    "                buffered_geom1 = geom1\n",
    "                buffered_geom2 = geom2\n",
    "            \n",
    "            # Get intersection\n",
    "            intersection = buffered_geom1.intersection(buffered_geom2)\n",
    "            \n",
    "            # Check if intersection is valid\n",
    "            if intersection.is_empty or intersection.area <= 0:\n",
    "                failed_files.append({\n",
    "                    'file_path': f\"{tiff_path1} / {tiff_path2}\",\n",
    "                    'file_type': \"both\",\n",
    "                    'position': position,\n",
    "                    'error': \"Empty intersection\"\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # Determine which file to modify based on position\n",
    "            modify_idx1 = False\n",
    "            \n",
    "            if 'right' in position and 'left' not in position:\n",
    "                modify_idx1 = True  # Modify left file\n",
    "            elif 'left' in position and 'right' not in position:\n",
    "                modify_idx1 = False  # Modify right file\n",
    "            elif 'top' in position and 'bottom' not in position:\n",
    "                modify_idx1 = False  # Modify bottom file\n",
    "            elif 'bottom' in position and 'top' not in position:\n",
    "                modify_idx1 = True  # Modify top file\n",
    "            elif 'center' in position or 'substantial' in position:\n",
    "                # Modify smaller area\n",
    "                if geom1.area <= geom2.area:\n",
    "                    modify_idx1 = True\n",
    "                else:\n",
    "                    modify_idx1 = False\n",
    "            else:\n",
    "                # Complex cases - use overlap percentage\n",
    "                if row['overlap_percentage_1'] <= row['overlap_percentage_2']:\n",
    "                    modify_idx1 = True\n",
    "                else:\n",
    "                    modify_idx1 = False\n",
    "            \n",
    "            # Set file paths based on decision\n",
    "            if modify_idx1:\n",
    "                img_to_modify = tiff_path1\n",
    "                mask_to_modify = mask_path1\n",
    "                overlap_with_img = tiff_path2\n",
    "                overlap_with_mask = mask_path2\n",
    "                tile_id = gdf_dataset.iloc[index1]['tile_id']\n",
    "            else:\n",
    "                img_to_modify = tiff_path2\n",
    "                mask_to_modify = mask_path2\n",
    "                overlap_with_img = tiff_path1\n",
    "                overlap_with_mask = mask_path1\n",
    "                tile_id = gdf_dataset.iloc[index2]['tile_id']\n",
    "            \n",
    "            # Process both image and mask\n",
    "            for file_type, file_to_modify in [(\"image\", img_to_modify), (\"mask\", mask_to_modify)]:\n",
    "                if not os.path.exists(file_to_modify):\n",
    "                    failed_files.append({\n",
    "                        'file_path': file_to_modify,\n",
    "                        'file_type': file_type,\n",
    "                        'position': position,\n",
    "                        'error': \"File does not exist\"\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                # Determine output path\n",
    "                if overwrite:\n",
    "                    output_file = file_to_modify\n",
    "                else:\n",
    "                    output_dir = os.path.dirname(file_to_modify)\n",
    "                    base_name = os.path.basename(file_to_modify)\n",
    "                    output_file = os.path.join(output_dir, f\"overlap_fixed_{base_name}\")\n",
    "                \n",
    "                try:\n",
    "                    # Create temporary file if overwriting\n",
    "                    temp_file = None\n",
    "                    if overwrite:\n",
    "                        temp_dir = os.path.dirname(file_to_modify)\n",
    "                        temp_file = os.path.join(temp_dir, f\"temp_{os.path.basename(file_to_modify)}\")\n",
    "                    \n",
    "                    with rasterio.open(file_to_modify) as src:\n",
    "                        # Read data\n",
    "                        data = src.read()\n",
    "                        \n",
    "                        # Get intersection bounds\n",
    "                        minx, miny, maxx, maxy = intersection.bounds\n",
    "                        \n",
    "                        # Convert to pixel coordinates\n",
    "                        window = from_bounds(minx, miny, maxx, maxy, src.transform)\n",
    "                        \n",
    "                        # Validate window\n",
    "                        if (np.isnan(window.col_off) or np.isnan(window.row_off) or \n",
    "                            np.isnan(window.width) or np.isnan(window.height)):\n",
    "                            failed_files.append({\n",
    "                                'file_path': file_to_modify,\n",
    "                                'file_type': file_type,\n",
    "                                'position': position,\n",
    "                                'error': \"Invalid window coordinates\"\n",
    "                            })\n",
    "                            continue\n",
    "                        \n",
    "                        # Round to integers\n",
    "                        col_off = max(0, int(window.col_off))\n",
    "                        row_off = max(0, int(window.row_off))\n",
    "                        width = min(int(np.ceil(window.width)), src.width - col_off)\n",
    "                        height = min(int(np.ceil(window.height)), src.height - row_off)\n",
    "                        \n",
    "                        # Validate dimensions\n",
    "                        if width <= 0 or height <= 0:\n",
    "                            failed_files.append({\n",
    "                                'file_path': file_to_modify,\n",
    "                                'file_type': file_type,\n",
    "                                'position': position,\n",
    "                                'error': \"Invalid window dimensions\"\n",
    "                            })\n",
    "                            continue\n",
    "                        \n",
    "                        # Set overlap area to zero\n",
    "                        for band in range(data.shape[0]):\n",
    "                            data[band, row_off:row_off+height, col_off:col_off+width] = 0\n",
    "                        \n",
    "                        # Get profile\n",
    "                        profile = src.profile\n",
    "                    \n",
    "                    # Write to temporary file\n",
    "                    write_path = temp_file if overwrite else output_file\n",
    "                    \n",
    "                    with rasterio.open(write_path, 'w', **profile) as dst:\n",
    "                        dst.write(data)\n",
    "                    \n",
    "                    # Replace original if overwriting\n",
    "                    if overwrite and temp_file:\n",
    "                        if os.path.exists(file_to_modify):\n",
    "                            os.remove(file_to_modify)\n",
    "                        shutil.move(temp_file, file_to_modify)\n",
    "                    \n",
    "                    # Record success\n",
    "                    processed_files.append({\n",
    "                        'file_path': file_to_modify,\n",
    "                        'file_type': file_type,\n",
    "                        'position': position,\n",
    "                        'overlap_with': overlap_with_img if file_type == 'image' else overlap_with_mask,\n",
    "                        'overlap_area': row['overlap_area'],\n",
    "                        'modified_pixels': width * height,\n",
    "                        'tile_id': tile_id\n",
    "                    })\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    error_msg = str(e)\n",
    "                    print(f\"Error processing {file_type} file {file_to_modify}: {error_msg}\")\n",
    "                    # Clean up temp file\n",
    "                    if overwrite and temp_file and os.path.exists(temp_file):\n",
    "                        os.remove(temp_file)\n",
    "                    \n",
    "                    failed_files.append({\n",
    "                        'file_path': file_to_modify,\n",
    "                        'file_type': file_type,\n",
    "                        'position': position,\n",
    "                        'error': error_msg\n",
    "                    })\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    if processed_files:\n",
    "        results_df = pd.DataFrame(processed_files)\n",
    "        print(f\"Successfully processed {len(results_df)} files\")\n",
    "        \n",
    "        # Report failures\n",
    "        if failed_files:\n",
    "            failed_df = pd.DataFrame(failed_files)\n",
    "            print(f\"Failed to process {len(failed_df)} files\")\n",
    "            print(\"First few failures:\")\n",
    "            print(failed_df.head())\n",
    "            \n",
    "        return results_df\n",
    "    else:\n",
    "        if failed_files:\n",
    "            failed_df = pd.DataFrame(failed_files)\n",
    "            print(f\"Failed to process all {len(failed_df)} files\")\n",
    "            print(\"First few failures:\")\n",
    "            print(failed_df.head())\n",
    "            \n",
    "        print(\"No files were processed successfully\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Remove overlaps\n",
    "results = remove_overlap_in_geotiffs(\n",
    "    overlap_df, \n",
    "    gdf_dataset, \n",
    "    overlap_positions=OVERLAP_POSITIONS, \n",
    "    buffer_distance=BUFFER_DISTANCE\n",
    ")\n",
    "\n",
    "# Display results\n",
    "if len(results) > 0:\n",
    "    print(\"\\nSummary of processed files:\")\n",
    "    print(f\"Total modified files: {len(results)}\")\n",
    "    \n",
    "    # Group by file type\n",
    "    file_type_counts = results['file_type'].value_counts()\n",
    "    print(\"\\nFiles by type:\")\n",
    "    print(file_type_counts)\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample of processed files:\")\n",
    "    display(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Overlap Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_overlap_corrections(overlap_df, gdf_dataset, buffer_distance=0.01):\n",
    "    \"\"\"\n",
    "    Verify that overlap corrections were applied correctly.\n",
    "    \n",
    "    Checks that overlapping regions have been set to background (zero)\n",
    "    in at least one of the overlapping tiles.\n",
    "    \n",
    "    Parameters:\n",
    "        overlap_df: DataFrame with overlap information\n",
    "        gdf_dataset: GeoDataFrame with file paths\n",
    "        buffer_distance: Buffer distance used for overlaps\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Verification results for each overlap\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize results list\n",
    "    verification_results = []\n",
    "    skipped_pairs = 0\n",
    "    \n",
    "    if len(overlap_df) == 0:\n",
    "        print(\"No overlaps to verify\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Process each overlap\n",
    "    print(f\"Verifying {len(overlap_df)} overlapping pairs...\")\n",
    "    with tqdm(total=len(overlap_df), desc=\"Verifying overlaps\") as pbar:\n",
    "        for idx, row in overlap_df.iterrows():\n",
    "            # Get indices and paths\n",
    "            index1 = row['index1']\n",
    "            index2 = row['index2']\n",
    "            position = row['relative_position']\n",
    "            \n",
    "            tiff_path1 = gdf_dataset.iloc[index1]['processed_img_path_tif']\n",
    "            tiff_path2 = gdf_dataset.iloc[index2]['processed_img_path_tif']\n",
    "            \n",
    "            # Get geometries\n",
    "            geom1 = gdf_dataset.iloc[index1]['geometry']\n",
    "            geom2 = gdf_dataset.iloc[index2]['geometry']\n",
    "            \n",
    "            # Initialize result\n",
    "            result = {\n",
    "                'tile_id1': row['tile_id1'],\n",
    "                'tile_id2': row['tile_id2'],\n",
    "                'position': position,\n",
    "                'overlap_area': row['overlap_area'],\n",
    "                'file1_has_zeros': False,\n",
    "                'file2_has_zeros': False,\n",
    "                'file1_zero_percentage': 0.0,\n",
    "                'file2_zero_percentage': 0.0,\n",
    "                'both_have_zeros': False,\n",
    "                'either_has_zeros': False,\n",
    "                'avg_zero_percentage': 0.0,\n",
    "                'status': 'unchecked'\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # Validate geometries\n",
    "                if geom1 is None or not geom1.is_valid or geom2 is None or not geom2.is_valid:\n",
    "                    result['status'] = 'invalid_geometry'\n",
    "                    verification_results.append(result)\n",
    "                    pbar.update(1)\n",
    "                    skipped_pairs += 1\n",
    "                    continue\n",
    "                \n",
    "                # Apply buffer if needed\n",
    "                use_buffer = row.get('buffered', True)\n",
    "                \n",
    "                if use_buffer:\n",
    "                    buffered_geom1 = geom1.buffer(buffer_distance)\n",
    "                    buffered_geom2 = geom2.buffer(buffer_distance)\n",
    "                else:\n",
    "                    buffered_geom1 = geom1\n",
    "                    buffered_geom2 = geom2\n",
    "                \n",
    "                # Get intersection\n",
    "                intersection = buffered_geom1.intersection(buffered_geom2)\n",
    "                \n",
    "                if intersection.is_empty or intersection.area <= 0:\n",
    "                    result['status'] = 'empty_intersection'\n",
    "                    verification_results.append(result)\n",
    "                    pbar.update(1)\n",
    "                    skipped_pairs += 1\n",
    "                    continue\n",
    "                \n",
    "                # Check first file\n",
    "                with rasterio.open(tiff_path1) as src1:\n",
    "                    # Get intersection bounds\n",
    "                    minx, miny, maxx, maxy = intersection.bounds\n",
    "                    \n",
    "                    # Convert to pixel coordinates\n",
    "                    window1 = from_bounds(minx, miny, maxx, maxy, src1.transform)\n",
    "                    \n",
    "                    # Validate window\n",
    "                    if (np.isnan(window1.col_off) or np.isnan(window1.row_off) or \n",
    "                        np.isnan(window1.width) or np.isnan(window1.height)):\n",
    "                        result['status'] = 'invalid_window_file1'\n",
    "                        verification_results.append(result)\n",
    "                        pbar.update(1)\n",
    "                        skipped_pairs += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Round to integers\n",
    "                    col_off1 = max(0, int(window1.col_off))\n",
    "                    row_off1 = max(0, int(window1.row_off))\n",
    "                    width1 = min(int(np.ceil(window1.width)), src1.width - col_off1)\n",
    "                    height1 = min(int(np.ceil(window1.height)), src1.height - row_off1)\n",
    "                    \n",
    "                    if width1 <= 0 or height1 <= 0:\n",
    "                        result['status'] = 'invalid_dimensions_file1'\n",
    "                        verification_results.append(result)\n",
    "                        pbar.update(1)\n",
    "                        skipped_pairs += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Read overlap region\n",
    "                    data1 = src1.read(1, window=((row_off1, row_off1+height1), (col_off1, col_off1+width1)))\n",
    "                    \n",
    "                    # Calculate zero percentage\n",
    "                    zero_count1 = np.sum(data1 == 0)\n",
    "                    total_pixels1 = data1.size\n",
    "                    zero_percentage1 = (zero_count1 / total_pixels1) * 100\n",
    "                    \n",
    "                    result['file1_has_zeros'] = zero_count1 > 0\n",
    "                    result['file1_zero_percentage'] = zero_percentage1\n",
    "                \n",
    "                # Check second file\n",
    "                with rasterio.open(tiff_path2) as src2:\n",
    "                    # Get intersection bounds\n",
    "                    minx, miny, maxx, maxy = intersection.bounds\n",
    "                    \n",
    "                    # Convert to pixel coordinates\n",
    "                    window2 = from_bounds(minx, miny, maxx, maxy, src2.transform)\n",
    "                    \n",
    "                    # Validate window\n",
    "                    if (np.isnan(window2.col_off) or np.isnan(window2.row_off) or \n",
    "                        np.isnan(window2.width) or np.isnan(window2.height)):\n",
    "                        result['status'] = 'invalid_window_file2' if result['status'] == 'unchecked' else 'invalid_windows_both'\n",
    "                        verification_results.append(result)\n",
    "                        pbar.update(1)\n",
    "                        skipped_pairs += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Round to integers\n",
    "                    col_off2 = max(0, int(window2.col_off))\n",
    "                    row_off2 = max(0, int(window2.row_off))\n",
    "                    width2 = min(int(np.ceil(window2.width)), src2.width - col_off2)\n",
    "                    height2 = min(int(np.ceil(window2.height)), src2.height - row_off2)\n",
    "                    \n",
    "                    if width2 <= 0 or height2 <= 0:\n",
    "                        result['status'] = 'invalid_dimensions_file2' if result['status'] == 'unchecked' else 'invalid_dimensions_both'\n",
    "                        verification_results.append(result)\n",
    "                        pbar.update(1)\n",
    "                        skipped_pairs += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Read overlap region\n",
    "                    data2 = src2.read(1, window=((row_off2, row_off2+height2), (col_off2, col_off2+width2)))\n",
    "                    \n",
    "                    # Calculate zero percentage\n",
    "                    zero_count2 = np.sum(data2 == 0)\n",
    "                    total_pixels2 = data2.size\n",
    "                    zero_percentage2 = (zero_count2 / total_pixels2) * 100\n",
    "                    \n",
    "                    result['file2_has_zeros'] = zero_count2 > 0\n",
    "                    result['file2_zero_percentage'] = zero_percentage2\n",
    "                \n",
    "                # Calculate final metrics\n",
    "                if result['status'] == 'unchecked':\n",
    "                    result['both_have_zeros'] = result['file1_has_zeros'] and result['file2_has_zeros']\n",
    "                    result['either_has_zeros'] = result['file1_has_zeros'] or result['file2_has_zeros']\n",
    "                    result['avg_zero_percentage'] = (result['file1_zero_percentage'] + result['file2_zero_percentage']) / 2\n",
    "                    \n",
    "                    if result['both_have_zeros']:\n",
    "                        result['status'] = 'both_have_zeros'\n",
    "                    elif result['either_has_zeros']:\n",
    "                        result['status'] = 'one_has_zeros'\n",
    "                    else:\n",
    "                        result['status'] = 'no_zeros'\n",
    "            \n",
    "            except Exception as e:\n",
    "                result['status'] = f\"error: {str(e)}\"\n",
    "                skipped_pairs += 1\n",
    "            \n",
    "            verification_results.append(result)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    if verification_results:\n",
    "        df_verification = pd.DataFrame(verification_results)\n",
    "        \n",
    "        # Summary statistics\n",
    "        status_counts = df_verification['status'].value_counts()\n",
    "        print(\"\\nVerification results:\")\n",
    "        for status, count in status_counts.items():\n",
    "            print(f\"  {status}: {count} pairs ({count/len(df_verification)*100:.1f}%)\")\n",
    "        \n",
    "        # Calculate statistics for valid results\n",
    "        valid_df = df_verification[df_verification['status'].isin(['both_have_zeros', 'one_has_zeros', 'no_zeros'])]\n",
    "        \n",
    "        if len(valid_df) > 0:\n",
    "            both_zeros_count = valid_df['both_have_zeros'].sum()\n",
    "            either_zeros_count = valid_df['either_has_zeros'].sum()\n",
    "            \n",
    "            print(f\"\\n  Pairs where both tiles have zeros in overlap: {both_zeros_count} ({both_zeros_count/len(valid_df)*100:.1f}%)\")\n",
    "            print(f\"  Pairs where at least one tile has zeros in overlap: {either_zeros_count} ({either_zeros_count/len(valid_df)*100:.1f}%)\")\n",
    "            \n",
    "            avg_zero_pct = valid_df['avg_zero_percentage'].mean()\n",
    "            print(f\"  Average percentage of zeros in overlap areas: {avg_zero_pct:.1f}%\")\n",
    "            \n",
    "            # Check for failed corrections\n",
    "            failed_verification = valid_df[valid_df['status'] == 'no_zeros']\n",
    "            if len(failed_verification) > 0:\n",
    "                print(f\"\\nWARNING: {len(failed_verification)} pairs have no background pixels in overlap regions!\")\n",
    "                print(\"\\nSample of problematic pairs:\")\n",
    "                display(failed_verification.head(5))\n",
    "        \n",
    "        print(f\"\\nSkipped {skipped_pairs} pairs due to geometry or window issues\")\n",
    "        \n",
    "        return df_verification\n",
    "    else:\n",
    "        print(\"No verification results\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run verification\n",
    "df_verification = verify_overlap_corrections(overlap_df, gdf_dataset, buffer_distance=BUFFER_DISTANCE)\n",
    "\n",
    "if len(df_verification) > 0:\n",
    "    # Show low zero percentages\n",
    "    low_zeros = df_verification[df_verification['avg_zero_percentage'] < 10]\n",
    "    if len(low_zeros) > 0:\n",
    "        print(\"\\nPairs with low zero percentage (<10%):\")\n",
    "        display(low_zeros[['tile_id1', 'tile_id2', 'position', 'file1_zero_percentage', 'file2_zero_percentage', 'status']])\n",
    "    \n",
    "    # Show high zero percentages\n",
    "    high_zeros = df_verification[(df_verification['file1_zero_percentage'] > 90) & \n",
    "                                (df_verification['file2_zero_percentage'] > 90)]\n",
    "    if len(high_zeros) > 0:\n",
    "        print(\"\\nPairs where both files have high zero percentage (>90%):\")\n",
    "        display(high_zeros[['tile_id1', 'tile_id2', 'position', 'file1_zero_percentage', 'file2_zero_percentage', 'status']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_overlap_corrections(overlap_df, df_verification, gdf_dataset, dataset_output_checks_path, zero_threshold=99.9):\n",
    "    \"\"\"\n",
    "    Create visualizations of overlap corrections including mask overlays.\n",
    "    \n",
    "    Generates diagnostic images showing how overlaps were handled and\n",
    "    checks for mask conflicts in overlapping regions.\n",
    "    \n",
    "    Parameters:\n",
    "        overlap_df: DataFrame with overlap information\n",
    "        df_verification: DataFrame with verification results\n",
    "        gdf_dataset: GeoDataFrame with file paths\n",
    "        dataset_output_checks_path: Output directory for visualizations\n",
    "        zero_threshold: Percentage to consider as background\n",
    "        \n",
    "    Returns:\n",
    "        dict: Visualization statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_dir = os.path.join(dataset_output_checks_path, f\"overlap_check_{timestamp}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Track statistics\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    mask_issues = 0\n",
    "    \n",
    "    def get_safe_window_data(src, intersection_bounds):\n",
    "        \"\"\"\n",
    "        Extract window data with consistent dimensions.\n",
    "        \n",
    "        Parameters:\n",
    "            src: Rasterio dataset\n",
    "            intersection_bounds: Bounds tuple\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (data, window)\n",
    "        \"\"\"\n",
    "        minx, miny, maxx, maxy = intersection_bounds\n",
    "        \n",
    "        # Get window\n",
    "        window = from_bounds(minx, miny, maxx, maxy, src.transform)\n",
    "        \n",
    "        # Round and ensure bounds\n",
    "        col_off = max(0, min(int(round(window.col_off)), src.width - 1))\n",
    "        row_off = max(0, min(int(round(window.row_off)), src.height - 1))\n",
    "        width = max(1, min(int(round(window.width)), src.width - col_off))\n",
    "        height = max(1, min(int(round(window.height)), src.height - row_off))\n",
    "        \n",
    "        # Create safe window\n",
    "        safe_window = Window(col_off, row_off, width, height)\n",
    "        \n",
    "        # Read data\n",
    "        data = src.read(1, window=safe_window)\n",
    "        \n",
    "        return data, safe_window\n",
    "    \n",
    "    def visualize_pair(row, output_path):\n",
    "        \"\"\"\n",
    "        Create visualization for a single overlap pair.\n",
    "        \n",
    "        Parameters:\n",
    "            row: DataFrame row with overlap information\n",
    "            output_path: Path to save visualization\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (success, has_mask_issue)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get tile IDs\n",
    "            tile_id1 = row['tile_id1']\n",
    "            tile_id2 = row['tile_id2']\n",
    "            \n",
    "            # Find indices\n",
    "            idx1 = gdf_dataset[gdf_dataset['tile_id'] == tile_id1].index[0]\n",
    "            idx2 = gdf_dataset[gdf_dataset['tile_id'] == tile_id2].index[0]\n",
    "            \n",
    "            # Get file paths\n",
    "            tiff_path1 = gdf_dataset.loc[idx1, 'processed_img_path_tif']\n",
    "            tiff_path2 = gdf_dataset.loc[idx2, 'processed_img_path_tif']\n",
    "            \n",
    "            # Check file existence\n",
    "            if not os.path.exists(tiff_path1) or not os.path.exists(tiff_path2):\n",
    "                print(f\"Files not found for {tile_id1} and {tile_id2}\")\n",
    "                return False\n",
    "            \n",
    "            # Get mask paths\n",
    "            has_masks = False\n",
    "            if 'processed_mask_path_tif' in gdf_dataset.columns:\n",
    "                mask_path1 = gdf_dataset.loc[idx1, 'processed_mask_path_tif']\n",
    "                mask_path2 = gdf_dataset.loc[idx2, 'processed_mask_path_tif']\n",
    "                \n",
    "                has_masks = (os.path.exists(mask_path1) and os.path.exists(mask_path2))\n",
    "                if not has_masks:\n",
    "                    print(f\"Warning: Mask files not found for {tile_id1} and/or {tile_id2}\")\n",
    "            else:\n",
    "                print(\"Warning: 'processed_mask_path_tif' column not found\")\n",
    "            \n",
    "            # Open and analyze images\n",
    "            with rasterio.open(tiff_path1) as src1, rasterio.open(tiff_path2) as src2:\n",
    "                # Get bounds\n",
    "                bounds1 = src1.bounds\n",
    "                bounds2 = src2.bounds\n",
    "                \n",
    "                # Calculate intersection\n",
    "                intersection = (\n",
    "                    max(bounds1.left, bounds2.left),\n",
    "                    max(bounds1.bottom, bounds2.bottom),\n",
    "                    min(bounds1.right, bounds2.right),\n",
    "                    min(bounds1.top, bounds2.top)\n",
    "                )\n",
    "                \n",
    "                # Validate intersection\n",
    "                if intersection[2] <= intersection[0] or intersection[3] <= intersection[1]:\n",
    "                    print(f\"No valid intersection for {tile_id1} and {tile_id2}\")\n",
    "                    return False\n",
    "                \n",
    "                # Get window data safely\n",
    "                data1, window1 = get_safe_window_data(src1, intersection)\n",
    "                data2, window2 = get_safe_window_data(src2, intersection)\n",
    "                \n",
    "                # Read full images\n",
    "                full_data1 = src1.read(1)\n",
    "                full_data2 = src2.read(1)\n",
    "                \n",
    "                # Create overlap masks\n",
    "                overlap_mask1 = np.zeros_like(full_data1, dtype=bool)\n",
    "                overlap_mask1[window1.row_off:window1.row_off+window1.height, \n",
    "                             window1.col_off:window1.col_off+window1.width] = True\n",
    "                \n",
    "                overlap_mask2 = np.zeros_like(full_data2, dtype=bool)\n",
    "                overlap_mask2[window2.row_off:window2.row_off+window2.height, \n",
    "                             window2.col_off:window2.col_off+window2.width] = True\n",
    "                \n",
    "                # Process masks if available\n",
    "                has_mask_conflict = False\n",
    "                mask_conflict_percentage = 0\n",
    "                mask_data1 = None\n",
    "                mask_data2 = None\n",
    "                mask_overlap1 = None\n",
    "                mask_overlap2 = None\n",
    "                \n",
    "                if has_masks:\n",
    "                    try:\n",
    "                        with rasterio.open(mask_path1) as mask_src1, rasterio.open(mask_path2) as mask_src2:\n",
    "                            # Read full masks\n",
    "                            mask_data1 = mask_src1.read(1)\n",
    "                            mask_data2 = mask_src2.read(1)\n",
    "                            \n",
    "                            # Use same windows as images\n",
    "                            mask_overlap1 = mask_src1.read(1, window=window1)\n",
    "                            mask_overlap2 = mask_src2.read(1, window=window2)\n",
    "                            \n",
    "                            # Handle shape mismatches\n",
    "                            if mask_overlap1.shape != mask_overlap2.shape:\n",
    "                                print(f\"Mask shape mismatch for {tile_id1} and {tile_id2}\")\n",
    "                                \n",
    "                                # Resize to smaller dimensions\n",
    "                                min_height = min(mask_overlap1.shape[0], mask_overlap2.shape[0])\n",
    "                                min_width = min(mask_overlap1.shape[1], mask_overlap2.shape[1])\n",
    "                                \n",
    "                                mask_overlap1 = mask_overlap1[:min_height, :min_width]\n",
    "                                mask_overlap2 = mask_overlap2[:min_height, :min_width]\n",
    "                                \n",
    "                                # Also resize image data\n",
    "                                data1 = data1[:min_height, :min_width]\n",
    "                                data2 = data2[:min_height, :min_width]\n",
    "                            \n",
    "                            # Check for mask conflicts\n",
    "                            if mask_overlap1.shape == mask_overlap2.shape and mask_overlap1.size > 0:\n",
    "                                mask_conflict = np.logical_and(mask_overlap1 > 0, mask_overlap2 > 0)\n",
    "                                has_mask_conflict = np.any(mask_conflict)\n",
    "                                mask_conflict_percentage = np.sum(mask_conflict) / mask_conflict.size * 100\n",
    "                            else:\n",
    "                                has_mask_conflict = False\n",
    "                                mask_conflict_percentage = 0\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading mask files: {str(e)}\")\n",
    "                        has_masks = False\n",
    "                \n",
    "                # Ensure consistent dimensions\n",
    "                if data1.shape != data2.shape:\n",
    "                    min_height = min(data1.shape[0], data2.shape[0])\n",
    "                    min_width = min(data1.shape[1], data2.shape[1])\n",
    "                    data1 = data1[:min_height, :min_width]\n",
    "                    data2 = data2[:min_height, :min_width]\n",
    "                \n",
    "                # Create figure\n",
    "                fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "                \n",
    "                # Row 1: Image analysis\n",
    "                \n",
    "                # Tile 1 with overlap\n",
    "                axs[0, 0].imshow(full_data1, cmap='gray')\n",
    "                highlighted1 = np.zeros((*full_data1.shape, 4))\n",
    "                highlighted1[..., 0] = 1  # Red\n",
    "                highlighted1[..., 3] = np.where(overlap_mask1, 0.4, 0)  # Alpha\n",
    "                axs[0, 0].imshow(highlighted1)\n",
    "                axs[0, 0].set_title(f\"Tile {tile_id1}\\nZero %: {row.get('file1_zero_percentage', 'N/A'):.1f}%\")\n",
    "                axs[0, 0].axis('off')\n",
    "                \n",
    "                # Tile 2 with overlap\n",
    "                axs[0, 1].imshow(full_data2, cmap='gray')\n",
    "                highlighted2 = np.zeros((*full_data2.shape, 4))\n",
    "                highlighted2[..., 2] = 1  # Blue\n",
    "                highlighted2[..., 3] = np.where(overlap_mask2, 0.4, 0)  # Alpha\n",
    "                axs[0, 1].imshow(highlighted2)\n",
    "                axs[0, 1].set_title(f\"Tile {tile_id2}\\nZero %: {row.get('file2_zero_percentage', 'N/A'):.1f}%\")\n",
    "                axs[0, 1].axis('off')\n",
    "                \n",
    "                # Overlap comparison\n",
    "                if data1.size > 0 and data2.size > 0:\n",
    "                    composite = np.zeros((data1.shape[0], data1.shape[1] * 2))\n",
    "                    composite[:, :data1.shape[1]] = data1\n",
    "                    composite[:, data1.shape[1]:] = data2\n",
    "                    \n",
    "                    axs[0, 2].imshow(composite, cmap='gray')\n",
    "                    axs[0, 2].axvline(x=data1.shape[1], color='r', linestyle='--')\n",
    "                    \n",
    "                    # Calculate zero percentages\n",
    "                    zeros1 = np.sum(data1 == 0) / data1.size * 100\n",
    "                    zeros2 = np.sum(data2 == 0) / data2.size * 100\n",
    "                    \n",
    "                    # Determine status\n",
    "                    if zeros1 >= zero_threshold and zeros2 >= zero_threshold:\n",
    "                        content_status = \"both_background\"\n",
    "                    else:\n",
    "                        content_status = \"partial_image\"\n",
    "                    \n",
    "                    axs[0, 2].set_title(f\"Overlap Comparison\\nPosition: {row.get('position', 'N/A')}, Status: {content_status}\")\n",
    "                    \n",
    "                    # Add zero percentage labels\n",
    "                    axs[0, 2].text(data1.shape[1] * 0.5, data1.shape[0] * 0.9, \n",
    "                                  f\"{zeros1:.1f}% zeros\", ha='center', color='white',\n",
    "                                  bbox=dict(facecolor='red', alpha=0.7))\n",
    "                    axs[0, 2].text(data1.shape[1] * 1.5, data1.shape[0] * 0.9, \n",
    "                                  f\"{zeros2:.1f}% zeros\", ha='center', color='white',\n",
    "                                  bbox=dict(facecolor='blue', alpha=0.7))\n",
    "                else:\n",
    "                    axs[0, 2].text(0.5, 0.5, \"No overlap data available\", \n",
    "                                 ha='center', va='center', fontsize=12)\n",
    "                    content_status = \"no_data\"\n",
    "                    zeros1 = zeros2 = 0\n",
    "                \n",
    "                axs[0, 2].axis('off')\n",
    "                \n",
    "                # Row 2: Mask analysis\n",
    "                \n",
    "                if has_masks and mask_data1 is not None and mask_data2 is not None:\n",
    "                    # Tile 1 with mask overlay\n",
    "                    axs[1, 0].imshow(full_data1, cmap='gray')\n",
    "                    mask_overlay1 = np.zeros((*full_data1.shape, 4))\n",
    "                    mask_overlay1[..., 0] = 1  # Red\n",
    "                    mask_overlay1[..., 3] = np.where(mask_data1 > 0, 0.5, 0)\n",
    "                    axs[1, 0].imshow(mask_overlay1)\n",
    "                    axs[1, 0].set_title(f\"Tile {tile_id1} with mask overlay\")\n",
    "                    axs[1, 0].axis('off')\n",
    "                    \n",
    "                    # Tile 2 with mask overlay\n",
    "                    axs[1, 1].imshow(full_data2, cmap='gray')\n",
    "                    mask_overlay2 = np.zeros((*full_data2.shape, 4))\n",
    "                    mask_overlay2[..., 2] = 1  # Blue\n",
    "                    mask_overlay2[..., 3] = np.where(mask_data2 > 0, 0.5, 0)\n",
    "                    axs[1, 1].imshow(mask_overlay2)\n",
    "                    axs[1, 1].set_title(f\"Tile {tile_id2} with mask overlay\")\n",
    "                    axs[1, 1].axis('off')\n",
    "                    \n",
    "                    # Mask overlap comparison\n",
    "                    if (mask_overlap1 is not None and mask_overlap2 is not None and \n",
    "                        data1.size > 0 and data2.size > 0):\n",
    "                        \n",
    "                        mask_composite = np.zeros((data1.shape[0], data1.shape[1] * 2, 4))\n",
    "                        \n",
    "                        # Set base image\n",
    "                        for c in range(3):\n",
    "                            if np.max(data1) > 0:\n",
    "                                mask_composite[:, :data1.shape[1], c] = data1 / np.max(data1)\n",
    "                            if np.max(data2) > 0:\n",
    "                                mask_composite[:, data1.shape[1]:, c] = data2 / np.max(data2)\n",
    "                        mask_composite[..., 3] = 1.0\n",
    "                        \n",
    "                        # Ensure mask dimensions match\n",
    "                        if mask_overlap1.shape == data1.shape and mask_overlap2.shape == data2.shape:\n",
    "                            # Add mask overlays\n",
    "                            mask_overlay_left = np.zeros((data1.shape[0], data1.shape[1], 4))\n",
    "                            mask_overlay_left[..., 0] = 1.0  # Red\n",
    "                            mask_overlay_left[..., 3] = np.where(mask_overlap1 > 0, 0.5, 0)\n",
    "                            \n",
    "                            mask_overlay_right = np.zeros((data2.shape[0], data2.shape[1], 4))\n",
    "                            mask_overlay_right[..., 2] = 1.0  # Blue\n",
    "                            mask_overlay_right[..., 3] = np.where(mask_overlap2 > 0, 0.5, 0)\n",
    "                            \n",
    "                            # Plot composite\n",
    "                            axs[1, 2].imshow(mask_composite)\n",
    "                            axs[1, 2].imshow(np.pad(mask_overlay_left, ((0,0), (0,data1.shape[1]), (0,0)), 'constant'))\n",
    "                            axs[1, 2].imshow(np.pad(mask_overlay_right, ((0,0), (data1.shape[1],0), (0,0)), 'constant'))\n",
    "                            \n",
    "                            # Add dividing line\n",
    "                            axs[1, 2].axvline(x=data1.shape[1], color='yellow', linestyle='--')\n",
    "                            \n",
    "                            # Set title based on conflicts\n",
    "                            if has_mask_conflict:\n",
    "                                title = f\"Mask Overlap Comparison\\nWarning: {mask_conflict_percentage:.1f}% mask conflict!\"\n",
    "                            else:\n",
    "                                title = \"Mask Overlap Comparison\\nNo mask conflicts\"\n",
    "                            \n",
    "                            axs[1, 2].set_title(title)\n",
    "                            axs[1, 2].axis('off')\n",
    "                        else:\n",
    "                            axs[1, 2].text(0.5, 0.5, \"Mask-image dimension mismatch\", \n",
    "                                         ha='center', va='center', fontsize=12)\n",
    "                            axs[1, 2].axis('off')\n",
    "                    else:\n",
    "                        axs[1, 2].text(0.5, 0.5, \"Unable to process mask overlaps\", \n",
    "                                     ha='center', va='center', fontsize=12)\n",
    "                        axs[1, 2].axis('off')\n",
    "                    \n",
    "                else:\n",
    "                    # No masks available\n",
    "                    for i in range(3):\n",
    "                        axs[1, i].text(0.5, 0.5, \"No mask files found\", \n",
    "                                     ha='center', va='center', fontsize=12)\n",
    "                        axs[1, i].axis('off')\n",
    "                \n",
    "                # Add main title\n",
    "                plt.suptitle(f\"Overlap Analysis: {tile_id1} and {tile_id2}\", \n",
    "                            fontsize=16, y=0.98)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.subplots_adjust(top=0.92)\n",
    "                plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "                \n",
    "                # Update row with analysis\n",
    "                row['content_status'] = content_status\n",
    "                row['zeros1'] = zeros1\n",
    "                row['zeros2'] = zeros2\n",
    "                \n",
    "                if has_masks:\n",
    "                    row['has_masks'] = True\n",
    "                    row['has_mask_conflict'] = has_mask_conflict\n",
    "                    row['mask_conflict_percentage'] = mask_conflict_percentage\n",
    "                else:\n",
    "                    row['has_masks'] = False\n",
    "                \n",
    "                return True, has_masks and has_mask_conflict\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing pair {tile_id1} and {tile_id2}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return False, False\n",
    "    \n",
    "    # Process verification results\n",
    "    if len(df_verification) > 0:\n",
    "        print(f\"Processing {len(df_verification)} verified pairs...\")\n",
    "        \n",
    "        # Store results\n",
    "        results_df = pd.DataFrame()\n",
    "        \n",
    "        # Process each row\n",
    "        for idx, row in tqdm(df_verification.iterrows(), total=len(df_verification)):\n",
    "            # Create filename\n",
    "            tile_id1 = row['tile_id1']\n",
    "            tile_id2 = row['tile_id2']\n",
    "            position = row.get('position', 'unknown')\n",
    "            \n",
    "            # Copy row for updates\n",
    "            row_copy = row.copy()\n",
    "            \n",
    "            filename = f\"{tile_id1}_{tile_id2}_{position}.png\"\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            success, has_mask_issue = visualize_pair(row_copy, output_path)\n",
    "            if success:\n",
    "                successful += 1\n",
    "                if has_mask_issue:\n",
    "                    mask_issues += 1\n",
    "                # Append result\n",
    "                results_df = pd.concat([results_df, pd.DataFrame([row_copy])], ignore_index=True)\n",
    "            else:\n",
    "                failed += 1\n",
    "        \n",
    "        # Save results\n",
    "        results_path = os.path.join(output_dir, \"overlap_analysis_results.csv\")\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"Saved results to {results_path}\")\n",
    "        \n",
    "        # Save mask issues separately\n",
    "        if mask_issues > 0:\n",
    "            mask_issues_df = results_df[results_df.get('has_mask_conflict', False) == True]\n",
    "            mask_issues_path = os.path.join(output_dir, \"mask_issues.csv\")\n",
    "            mask_issues_df.to_csv(mask_issues_path, index=False)\n",
    "            print(f\"Found {mask_issues} tile pairs with mask issues. Saved to {mask_issues_path}\")\n",
    "    \n",
    "    print(f\"Visualization complete. Created {successful} visualizations in {output_dir}\")\n",
    "    print(f\"- Successful: {successful}\")\n",
    "    print(f\"- Failed: {failed}\")\n",
    "    print(f\"- Mask issues: {mask_issues}\")\n",
    "    \n",
    "    return {\n",
    "        \"successful\": successful,\n",
    "        \"failed\": failed,\n",
    "        \"mask_issues\": mask_issues,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"results_path\": results_path if len(df_verification) > 0 else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "visualization_results = visualize_overlap_corrections(\n",
    "    overlap_df=overlap_df,\n",
    "    df_verification=df_verification,\n",
    "    gdf_dataset=gdf_dataset,\n",
    "    dataset_output_checks_path=DATASET_OUTPUT_CHECKS_PATH,\n",
    ")\n",
    "\n",
    "print(f\"Results saved to: {visualization_results['output_dir']}\")\n",
    "print(f\"Successful visualizations: {visualization_results['successful']}\")\n",
    "print(f\"Failed visualizations: {visualization_results['failed']}\")\n",
    "print(f\"Tiles with mask issues: {visualization_results['mask_issues']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Dimensions to 1280x1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_image_dimensions(img_dir, mask_dir, target_size=(1280, 1280), overwrite=True):\n",
    "    \"\"\"\n",
    "    Pad images and masks to standard dimensions with consistent alignment.\n",
    "    \n",
    "    Centers the original image within the target dimensions and applies\n",
    "    identical padding to both image and mask files.\n",
    "    \n",
    "    Parameters:\n",
    "        img_dir: Directory containing images\n",
    "        mask_dir: Directory containing masks\n",
    "        target_size: Target dimensions (width, height)\n",
    "        overwrite: Whether to overwrite original files\n",
    "        \n",
    "    Returns:\n",
    "        list: Information about modified files\n",
    "    \"\"\"\n",
    "    # Get file lists\n",
    "    img_files = [f for f in os.listdir(img_dir) if f.endswith(('.tif', '.tiff'))]\n",
    "    mask_files = [f for f in os.listdir(mask_dir) if f.endswith(('.tif', '.tiff'))]\n",
    "    \n",
    "    # Create mask mapping\n",
    "    mask_map = {}\n",
    "    for mask_file in mask_files:\n",
    "        mask_basename = os.path.splitext(mask_file)[0]\n",
    "        mask_map[mask_basename] = mask_file\n",
    "    \n",
    "    # Track statistics\n",
    "    total_images = len(img_files)\n",
    "    resized_pairs = 0\n",
    "    errors = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    # Track modified files\n",
    "    modified_files = []\n",
    "    \n",
    "    print(f\"Processing {total_images} images to ensure {target_size[0]}x{target_size[1]} dimensions...\")\n",
    "    \n",
    "    # Process each image\n",
    "    for img_filename in tqdm(img_files, desc=\"Standardizing images\"):\n",
    "        try:\n",
    "            img_path = os.path.join(img_dir, img_filename)\n",
    "            \n",
    "            # Find corresponding mask\n",
    "            img_basename = os.path.splitext(img_filename)[0]\n",
    "            mask_filename = mask_map.get(img_basename)\n",
    "            \n",
    "            if mask_filename:\n",
    "                mask_path = os.path.join(mask_dir, mask_filename)\n",
    "                \n",
    "                # Verify mask exists\n",
    "                if not os.path.exists(mask_path):\n",
    "                    print(f\"Warning: Mask file {mask_path} not found. Skipping pair.\")\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Warning: No matching mask found for {img_filename}. Skipping.\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Open image to check dimensions\n",
    "            with rasterio.open(img_path) as src:\n",
    "                height, width = src.height, src.width\n",
    "                \n",
    "                # Skip if already target size\n",
    "                if (width, height) == target_size:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate padding\n",
    "                pad_width = max(0, target_size[0] - width)\n",
    "                pad_height = max(0, target_size[1] - height)\n",
    "                \n",
    "                # Calculate padding offsets for centering\n",
    "                start_x = pad_width // 2\n",
    "                start_y = pad_height // 2\n",
    "                \n",
    "                # Skip if larger than target\n",
    "                if pad_width < 0 or pad_height < 0:\n",
    "                    print(f\"Warning: {img_filename} is larger than target size. Skipping pair.\")\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Process image\n",
    "                with rasterio.open(img_path) as src:\n",
    "                    # Read data\n",
    "                    img_data = src.read()\n",
    "                    \n",
    "                    # Create padded array\n",
    "                    bands = img_data.shape[0]\n",
    "                    padded_data = np.zeros((bands, target_size[1], target_size[0]), dtype=img_data.dtype)\n",
    "                    \n",
    "                    # Copy original data centered\n",
    "                    for b in range(bands):\n",
    "                        padded_data[b, start_y:start_y+height, start_x:start_x+width] = img_data[b]\n",
    "                    \n",
    "                    # Update transform for georeferencing\n",
    "                    transform = src.transform\n",
    "                    xoff = transform.c - start_x * transform.a\n",
    "                    yoff = transform.f - start_y * transform.e\n",
    "                    new_transform = rasterio.Affine(transform.a, transform.b, xoff,\n",
    "                                                   transform.d, transform.e, yoff)\n",
    "                    \n",
    "                    # Update metadata\n",
    "                    meta = src.meta.copy()\n",
    "                    meta.update({\n",
    "                        'height': target_size[1],\n",
    "                        'width': target_size[0],\n",
    "                        'transform': new_transform\n",
    "                    })\n",
    "                    \n",
    "                    # Write to temporary file\n",
    "                    with tempfile.NamedTemporaryFile(suffix='.tif', delete=False) as tmp:\n",
    "                        tmp_path = tmp.name\n",
    "                    \n",
    "                    # Write padded image\n",
    "                    with rasterio.open(tmp_path, 'w', **meta) as dst:\n",
    "                        dst.write(padded_data)\n",
    "                    \n",
    "                    # Replace original\n",
    "                    shutil.move(tmp_path, img_path)\n",
    "                \n",
    "                # Process mask (TIF format)\n",
    "                if mask_filename.lower().endswith(('.tif', '.tiff')):\n",
    "                    with rasterio.open(mask_path) as mask_src:\n",
    "                        mask_height, mask_width = mask_src.height, mask_src.width\n",
    "                        \n",
    "                        # Verify dimensions match\n",
    "                        if (mask_width, mask_height) != (width, height):\n",
    "                            print(f\"Warning: Dimension mismatch between {img_filename} and {mask_filename}\")\n",
    "                        \n",
    "                        # Read mask data\n",
    "                        mask_data = mask_src.read()\n",
    "                        \n",
    "                        # Create padded array\n",
    "                        mask_bands = mask_data.shape[0]\n",
    "                        padded_mask_data = np.zeros((mask_bands, target_size[1], target_size[0]), dtype=mask_data.dtype)\n",
    "                        \n",
    "                        # Copy original data with same offsets as image\n",
    "                        for b in range(mask_bands):\n",
    "                            padded_mask_data[b, start_y:start_y+mask_height, start_x:start_x+mask_width] = mask_data[b]\n",
    "                        \n",
    "                        # Update metadata (use same transform as image)\n",
    "                        mask_meta = mask_src.meta.copy()\n",
    "                        mask_meta.update({\n",
    "                            'height': target_size[1],\n",
    "                            'width': target_size[0],\n",
    "                            'transform': new_transform\n",
    "                        })\n",
    "                        \n",
    "                        # Write to temporary file\n",
    "                        with tempfile.NamedTemporaryFile(suffix='.tif', delete=False) as tmp:\n",
    "                            tmp_path = tmp.name\n",
    "                        \n",
    "                        # Write padded mask\n",
    "                        with rasterio.open(tmp_path, 'w', **mask_meta) as dst:\n",
    "                            dst.write(padded_mask_data)\n",
    "                        \n",
    "                        # Replace original\n",
    "                        shutil.move(tmp_path, mask_path)\n",
    "                \n",
    "                # Record modification\n",
    "                modified_files.append({\n",
    "                    'img_file': img_path,\n",
    "                    'mask_file': mask_path,\n",
    "                    'from_size': (width, height),\n",
    "                    'to_size': target_size,\n",
    "                    'padding': (start_x, start_y, pad_width, pad_height)\n",
    "                })\n",
    "                \n",
    "                resized_pairs += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            print(f\"Error processing {img_filename}: {e}\")\n",
    "    \n",
    "    print(\"Standardization complete:\")\n",
    "    print(f\"- Total images processed: {total_images}\")\n",
    "    print(f\"- Image/mask pairs resized: {resized_pairs}\")\n",
    "    print(f\"- Pairs skipped: {skipped}\")\n",
    "    print(f\"- Errors encountered: {errors}\")\n",
    "    print(f\"- Already at target size: {total_images - resized_pairs - skipped - errors}\")\n",
    "    \n",
    "    return modified_files\n",
    "\n",
    "# Standardize dimensions\n",
    "modified_files = standardize_image_dimensions(\n",
    "    img_dir=DATASET_OUTPUT_IMG_PATH,\n",
    "    mask_dir=DATASET_OUTPUT_MASKS_PATH,\n",
    "    target_size=(1280, 1280)\n",
    ")\n",
    "\n",
    "# Display results\n",
    "if modified_files:\n",
    "    print(f\"\\nModified {len(modified_files)} file pairs. First 5 examples:\")\n",
    "    for i, file_info in enumerate(modified_files[:5]):\n",
    "        print(f\"{i+1}. {os.path.basename(file_info['img_file'])}: {file_info['from_size']} -> {file_info['to_size']}\")\n",
    "else:\n",
    "    print(\"\\nNo files were modified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_padding(processed_img_dir, processed_mask_dir, output_dir, show_images=False, modified_files=None, \n",
    "                   modified_sample_count=5, unmodified_sample_count=5):\n",
    "    \"\"\"\n",
    "    Verify padding consistency and create visualizations.\n",
    "    \n",
    "    Checks that all files have correct dimensions and that padding\n",
    "    was applied consistently to image-mask pairs.\n",
    "    \n",
    "    Parameters:\n",
    "        processed_img_dir: Directory with processed images\n",
    "        processed_mask_dir: Directory with processed masks\n",
    "        output_dir: Output directory for visualizations\n",
    "        show_images: Whether to display images\n",
    "        modified_files: List of modified file information\n",
    "        modified_sample_count: Number of modified samples to visualize\n",
    "        unmodified_sample_count: Number of unmodified samples to visualize\n",
    "        \n",
    "    Returns:\n",
    "        dict: Verification statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all files\n",
    "    all_tiff_files = [f for f in os.listdir(processed_img_dir) if f.endswith(('.tif', '.tiff'))]\n",
    "    \n",
    "    if not all_tiff_files:\n",
    "        print(\"No GeoTIFF files found for verification.\")\n",
    "        return\n",
    "    \n",
    "    # Create lookup for modified files\n",
    "    modified_info = {}\n",
    "    if modified_files:\n",
    "        for info in modified_files:\n",
    "            filename = os.path.basename(info['img_file'])\n",
    "            modified_info[filename] = info\n",
    "    \n",
    "    modified_paths = set(modified_info.keys())\n",
    "    \n",
    "    # Separate modified and unmodified\n",
    "    modified_tiff_files = [f for f in all_tiff_files if f in modified_paths]\n",
    "    unmodified_tiff_files = [f for f in all_tiff_files if f not in modified_paths]\n",
    "    \n",
    "    print(f\"Found {len(modified_tiff_files)} modified files and {len(unmodified_tiff_files)} unmodified files\")\n",
    "    \n",
    "    # Track verification results\n",
    "    total_files = len(all_tiff_files)\n",
    "    verified_files = 0\n",
    "    dimension_mismatches = 0\n",
    "    missing_masks = 0\n",
    "    \n",
    "    print(f\"Verifying all {total_files} image-mask pairs...\")\n",
    "    \n",
    "    # Verify all files\n",
    "    for img_filename in tqdm(all_tiff_files, desc=\"Verifying files\"):\n",
    "        img_path = os.path.join(processed_img_dir, img_filename)\n",
    "        img_basename = os.path.splitext(img_filename)[0]\n",
    "        \n",
    "        # Find corresponding mask\n",
    "        mask_filename = None\n",
    "        for ext in ['.tif', '.tiff', '.png', '.PNG']:\n",
    "            candidate_mask = img_basename + ext\n",
    "            if os.path.exists(os.path.join(processed_mask_dir, candidate_mask)):\n",
    "                mask_filename = candidate_mask\n",
    "                break\n",
    "        \n",
    "        if not mask_filename:\n",
    "            print(f\"No matching mask found for {img_filename}.\")\n",
    "            missing_masks += 1\n",
    "            continue\n",
    "        \n",
    "        mask_path = os.path.join(processed_mask_dir, mask_filename)\n",
    "        \n",
    "        try:\n",
    "            # Read dimensions\n",
    "            with rasterio.open(img_path) as src:\n",
    "                geotiff_height, geotiff_width = src.height, src.width\n",
    "            \n",
    "            # Read mask dimensions\n",
    "            if mask_filename.lower().endswith(('.tif', '.tiff')):\n",
    "                with rasterio.open(mask_path) as mask_src:\n",
    "                    mask_height, mask_width = mask_src.height, mask_src.width\n",
    "            else:\n",
    "                with Image.open(mask_path) as mask_img:\n",
    "                    mask_width, mask_height = mask_img.size\n",
    "            \n",
    "            # Check dimension match\n",
    "            if (geotiff_height, geotiff_width) != (mask_height, mask_width):\n",
    "                print(f\"Dimension mismatch for {img_basename}: GeoTIFF {geotiff_width}x{geotiff_height}, \"\n",
    "                      f\"Mask {mask_width}x{mask_height}\")\n",
    "                dimension_mismatches += 1\n",
    "            else:\n",
    "                verified_files += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error verifying {img_filename}: {e}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nVerification Summary:\")\n",
    "    print(f\"- Total files checked: {total_files}\")\n",
    "    print(f\"- Successfully verified pairs: {verified_files}\")\n",
    "    print(f\"- Dimension mismatches: {dimension_mismatches}\")\n",
    "    print(f\"- Missing masks: {missing_masks}\")\n",
    "    \n",
    "    def visualize_sample(img_filename, sample_type):\n",
    "        \"\"\"Create visualization for a single sample.\"\"\"\n",
    "        img_path = os.path.join(processed_img_dir, img_filename)\n",
    "        img_basename = os.path.splitext(img_filename)[0]\n",
    "        \n",
    "        # Find mask\n",
    "        mask_filename = None\n",
    "        for ext in ['.tif', '.tiff', '.png', '.PNG']:\n",
    "            candidate_mask = img_basename + ext\n",
    "            if os.path.exists(os.path.join(processed_mask_dir, candidate_mask)):\n",
    "                mask_filename = candidate_mask\n",
    "                break\n",
    "        \n",
    "        if not mask_filename:\n",
    "            print(f\"No mask found for {img_filename}\")\n",
    "            return False\n",
    "        \n",
    "        mask_path = os.path.join(processed_mask_dir, mask_filename)\n",
    "        \n",
    "        try:\n",
    "            # Read image\n",
    "            with rasterio.open(img_path) as src:\n",
    "                geotiff_data = src.read(1)\n",
    "            \n",
    "            # Read mask\n",
    "            if mask_filename.lower().endswith(('.tif', '.tiff')):\n",
    "                with rasterio.open(mask_path) as mask_src:\n",
    "                    mask_data = mask_src.read(1)\n",
    "            else:\n",
    "                mask_data = np.array(Image.open(mask_path))\n",
    "                if len(mask_data.shape) == 3:\n",
    "                    mask_data = mask_data[:, :, 0]\n",
    "            \n",
    "            # Get dimension information\n",
    "            is_modified = img_filename in modified_paths\n",
    "            if is_modified:\n",
    "                padding_info = modified_info[img_filename]\n",
    "                original_size = padding_info['from_size']\n",
    "                dimension_text = f\"Original: {original_size[0]}×{original_size[1]} → Current: 1280×1280\"\n",
    "            else:\n",
    "                dimension_text = \"Original: 1280×1280 (no change needed)\"\n",
    "            \n",
    "            # Create visualization\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "            \n",
    "            # Plot image\n",
    "            axes[0].imshow(geotiff_data, cmap='gray')\n",
    "            axes[0].set_title(f\"GeoTIFF: {img_filename}\")\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # Plot mask\n",
    "            axes[1].imshow(mask_data, cmap='gray')\n",
    "            axes[1].set_title(f\"Mask: {mask_filename}\")\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            # Create overlay\n",
    "            if geotiff_data.max() > geotiff_data.min():\n",
    "                normalized_geotiff = (geotiff_data - geotiff_data.min()) / (geotiff_data.max() - geotiff_data.min())\n",
    "            else:\n",
    "                normalized_geotiff = np.zeros_like(geotiff_data)\n",
    "            \n",
    "            if mask_data.max() > mask_data.min():\n",
    "                normalized_mask = (mask_data - mask_data.min()) / (mask_data.max() - mask_data.min())\n",
    "            else:\n",
    "                normalized_mask = np.zeros_like(mask_data)\n",
    "            \n",
    "            # Create RGB overlay\n",
    "            overlay = np.zeros((geotiff_data.shape[0], geotiff_data.shape[1], 3))\n",
    "            overlay[:, :, 0] = normalized_geotiff  # Red (image)\n",
    "            overlay[:, :, 2] = normalized_mask     # Blue (mask)\n",
    "            \n",
    "            # Display overlay\n",
    "            axes[2].imshow(overlay)\n",
    "            axes[2].set_title(\"Overlay (purple shows alignment)\")\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            # Add boundaries for modified files\n",
    "            if is_modified:\n",
    "                padding_info = modified_info[img_filename]\n",
    "                padding = padding_info['padding']\n",
    "                original_size = padding_info['from_size']\n",
    "                \n",
    "                start_x, start_y = padding[0], padding[1]\n",
    "                width, height = original_size\n",
    "                \n",
    "                from matplotlib.patches import Rectangle\n",
    "                rect_style = dict(linewidth=2, edgecolor='yellow', facecolor='none', linestyle='--')\n",
    "                \n",
    "                # Add rectangle showing original area\n",
    "                axes[0].add_patch(Rectangle((start_x, start_y), width, height, **rect_style))\n",
    "                axes[1].add_patch(Rectangle((start_x, start_y), width, height, **rect_style))\n",
    "                axes[2].add_patch(Rectangle((start_x, start_y), width, height, **rect_style))\n",
    "            \n",
    "            plt.suptitle(f\"{sample_type} Sample: {img_basename}\\n{dimension_text}\", fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            save_path = os.path.join(output_dir, f\"{sample_type.lower()}_sample_{img_basename}.png\")\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            \n",
    "            if show_images:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "            \n",
    "            print(f\"Visualization for {img_basename} ({sample_type}):\")\n",
    "            print(f\"- GeoTIFF dimensions: {geotiff_data.shape}\")\n",
    "            print(f\"- Mask dimensions: {mask_data.shape}\")\n",
    "            print(f\"- {dimension_text}\")\n",
    "            print(f\"- Saved to: {save_path}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing {img_filename}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Generate visualizations for modified samples\n",
    "    if modified_sample_count > 0 and modified_tiff_files:\n",
    "        print(f\"\\nGenerating visualizations for {min(modified_sample_count, len(modified_tiff_files))} modified samples...\")\n",
    "        \n",
    "        modified_samples = random.sample(modified_tiff_files, min(modified_sample_count, len(modified_tiff_files)))\n",
    "        \n",
    "        successful_visualizations = 0\n",
    "        for img_filename in modified_samples:\n",
    "            if visualize_sample(img_filename, \"Modified\"):\n",
    "                successful_visualizations += 1\n",
    "        \n",
    "        print(f\"Successfully created {successful_visualizations} modified sample visualizations\")\n",
    "    \n",
    "    # Generate visualizations for unmodified samples\n",
    "    if unmodified_sample_count > 0 and unmodified_tiff_files:\n",
    "        print(f\"\\nGenerating visualizations for {min(unmodified_sample_count, len(unmodified_tiff_files))} unmodified samples...\")\n",
    "        \n",
    "        unmodified_samples = random.sample(unmodified_tiff_files, min(unmodified_sample_count, len(unmodified_tiff_files)))\n",
    "        \n",
    "        successful_visualizations = 0\n",
    "        for img_filename in unmodified_samples:\n",
    "            if visualize_sample(img_filename, \"Unmodified\"):\n",
    "                successful_visualizations += 1\n",
    "        \n",
    "        print(f\"Successfully created {successful_visualizations} unmodified sample visualizations\")\n",
    "    \n",
    "    return {\n",
    "        'total_files': total_files,\n",
    "        'verified_files': verified_files, \n",
    "        'dimension_mismatches': dimension_mismatches,\n",
    "        'missing_masks': missing_masks,\n",
    "        'modified_files_count': len(modified_tiff_files),\n",
    "        'unmodified_files_count': len(unmodified_tiff_files)\n",
    "    }\n",
    "\n",
    "# Run verification\n",
    "verification_results = verify_padding(\n",
    "    processed_img_dir=DATASET_OUTPUT_IMG_PATH,\n",
    "    processed_mask_dir=DATASET_OUTPUT_MASKS_PATH,\n",
    "    output_dir=DATASET_OUTPUT_CHECKS_PATH + \"/padding_verification\",\n",
    "    modified_files=modified_files,\n",
    "    modified_sample_count=5,\n",
    "    unmodified_sample_count=3,\n",
    "    show_images=False\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Verification Results:\")\n",
    "for key, value in verification_results.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-cleanup: Remove files with 100% zero content\n",
    "print(\"PRE-VERIFICATION CLEANUP: REMOVING 100% ZERO FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def remove_zero_files_and_records(gdf_dataset, dataset_output_img_path, dataset_output_masks_path):\n",
    "    \"\"\"\n",
    "    Remove image files that are 100% zeros and their records.\n",
    "    \n",
    "    Only removes pairs where the image is completely zero, regardless\n",
    "    of mask content. Masks with zero content but non-zero images are kept.\n",
    "    \n",
    "    Parameters:\n",
    "        gdf_dataset: GeoDataFrame with file information\n",
    "        dataset_output_img_path: Image directory path\n",
    "        dataset_output_masks_path: Mask directory path\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame: Cleaned dataset with zero-content records removed\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting with {len(gdf_dataset)} records\")\n",
    "    \n",
    "    # Make copy to avoid warnings\n",
    "    gdf_cleaned = gdf_dataset.copy()\n",
    "    \n",
    "    zero_files_to_remove = []\n",
    "    files_deleted = []\n",
    "    records_to_remove = []\n",
    "    \n",
    "    print(\"\\n1. Scanning for 100% zero files...\")\n",
    "    \n",
    "    for idx, row in gdf_cleaned.iterrows():\n",
    "        img_path = row.get('processed_img_path_tif')\n",
    "        mask_path = row.get('processed_mask_path_tif')\n",
    "        tile_id = row['tile_id']\n",
    "        \n",
    "        img_all_zero = False\n",
    "        mask_all_zero = False\n",
    "        should_remove = False\n",
    "        \n",
    "        # Check if image is all zeros\n",
    "        if pd.notna(img_path) and os.path.exists(img_path):\n",
    "            try:\n",
    "                with rasterio.open(img_path) as src:\n",
    "                    img_data = src.read(1)\n",
    "                    if np.all(img_data == 0):\n",
    "                        img_all_zero = True\n",
    "                        print(f\"   Image is 100% zeros: {tile_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Error reading image {tile_id}: {e}\")\n",
    "                should_remove = True\n",
    "        \n",
    "        # Check if mask is all zeros\n",
    "        if pd.notna(mask_path) and os.path.exists(mask_path):\n",
    "            try:\n",
    "                with rasterio.open(mask_path) as src:\n",
    "                    mask_data = src.read(1)\n",
    "                    if np.all(mask_data == 0):\n",
    "                        mask_all_zero = True\n",
    "                        print(f\"   Mask is 100% zeros: {tile_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Error reading mask {tile_id}: {e}\")\n",
    "                should_remove = True\n",
    "        \n",
    "        # Remove only if image is 100% zeros\n",
    "        if img_all_zero or should_remove:\n",
    "            zero_files_to_remove.append(tile_id)\n",
    "            records_to_remove.append(idx)\n",
    "            \n",
    "            # Log removal reason\n",
    "            if img_all_zero and mask_all_zero:\n",
    "                print(f\"   MARKED FOR REMOVAL: {tile_id} (both image and mask are 100% zeros)\")\n",
    "            elif img_all_zero and not mask_all_zero:\n",
    "                print(f\"   MARKED FOR REMOVAL: {tile_id} (image is 100% zeros, removing both)\")\n",
    "            elif should_remove:\n",
    "                print(f\"   MARKED FOR REMOVAL: {tile_id} (file read errors)\")\n",
    "            # Add files to deletion list\n",
    "            if pd.notna(img_path) and os.path.exists(img_path):\n",
    "                files_deleted.append(img_path)\n",
    "            if pd.notna(mask_path) and os.path.exists(mask_path):\n",
    "                files_deleted.append(mask_path)\n",
    "        else:\n",
    "            # Log kept files\n",
    "            if not img_all_zero and mask_all_zero:\n",
    "                print(f\"   KEEPING: {tile_id} (image has content, mask is 100% zeros - acceptable)\")\n",
    "            elif not img_all_zero and not mask_all_zero:\n",
    "                print(f\"   KEEPING: {tile_id} (both image and mask have content)\")\n",
    "    \n",
    "    print(f\"\\n2. Found {len(zero_files_to_remove)} records where image is 100% zeros\")\n",
    "    if zero_files_to_remove:\n",
    "        print(f\"   Records to remove: {zero_files_to_remove}\")\n",
    "        print(\"   Note: Removing pairs ONLY when image is 100% zeros (mask state doesn't matter)\")\n",
    "    \n",
    "    # Delete files from disk\n",
    "    print(f\"\\n3. Deleting {len(files_deleted)} files from disk...\")\n",
    "    deleted_count = 0\n",
    "    delete_errors = 0\n",
    "    \n",
    "    for file_path in files_deleted:\n",
    "        try:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "                deleted_count += 1\n",
    "                print(f\"   Deleted: {os.path.basename(file_path)}\")\n",
    "            else:\n",
    "                print(f\"   File already missing: {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error deleting {os.path.basename(file_path)}: {e}\")\n",
    "            delete_errors += 1\n",
    "    \n",
    "    print(f\"   Successfully deleted {deleted_count} files\")\n",
    "    if delete_errors > 0:\n",
    "        print(f\"   Failed to delete {delete_errors} files\")\n",
    "    \n",
    "    # Remove records from dataframe\n",
    "    if records_to_remove:\n",
    "        print(f\"\\n4. Removing {len(records_to_remove)} records from dataframe...\")\n",
    "        gdf_cleaned = gdf_cleaned.drop(records_to_remove)\n",
    "        gdf_cleaned = gdf_cleaned.reset_index(drop=True)\n",
    "        print(f\"   Dataframe now has {len(gdf_cleaned)} records\")\n",
    "    else:\n",
    "        print(\"\\n4. No records to remove from dataframe\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n5. Cleanup summary:\")\n",
    "    print(f\"   Original records: {len(gdf_dataset)}\")\n",
    "    print(f\"   Records removed: {len(records_to_remove)}\")\n",
    "    print(f\"   Final records: {len(gdf_cleaned)}\")\n",
    "    print(f\"   Files deleted: {deleted_count}\")\n",
    "    \n",
    "    return gdf_cleaned\n",
    "\n",
    "# Remove records with missing files\n",
    "print(\"STEP 1: Filter out records with missing files\")\n",
    "gdf_dataset_filtered = gdf_dataset.copy()\n",
    "\n",
    "missing_file_records = []\n",
    "for idx, row in gdf_dataset_filtered.iterrows():\n",
    "    img_path = row.get('processed_img_path_tif')\n",
    "    mask_path = row.get('processed_mask_path_tif')\n",
    "    \n",
    "    img_exists = pd.notna(img_path) and os.path.exists(img_path)\n",
    "    mask_exists = pd.notna(mask_path) and os.path.exists(mask_path)\n",
    "    \n",
    "    if not (img_exists and mask_exists):\n",
    "        missing_file_records.append(idx)\n",
    "\n",
    "if missing_file_records:\n",
    "    gdf_dataset_filtered = gdf_dataset_filtered.drop(missing_file_records)\n",
    "    print(f\"Removed {len(missing_file_records)} records with missing files\")\n",
    "else:\n",
    "    print(\"No records with missing files found\")\n",
    "\n",
    "print(f\"After filtering missing files: {len(gdf_dataset_filtered)} records\")\n",
    "\n",
    "# Remove zero content files\n",
    "print(\"\\nSTEP 2: Remove 100% zero files and records\")\n",
    "gdf_dataset_cleaned = remove_zero_files_and_records(\n",
    "    gdf_dataset=gdf_dataset_filtered,\n",
    "    dataset_output_img_path=DATASET_OUTPUT_IMG_PATH,\n",
    "    dataset_output_masks_path=DATASET_OUTPUT_MASKS_PATH\n",
    ")\n",
    "\n",
    "# Run final verification\n",
    "print(\"\\nSTEP 3: Running final verification on cleaned dataset\")\n",
    "print(f\"Records going into verification: {len(gdf_dataset_cleaned)}\")\n",
    "\n",
    "def final_verification_checks_no_zeros(gdf_dataset, dataset_output_img_path, dataset_output_masks_path):\n",
    "    \"\"\"\n",
    "    Perform final verification checks on the dataset.\n",
    "    \n",
    "    Validates file existence, dimensions, data integrity, and identifies\n",
    "    duplicate images. Zero-content check is skipped as it was done in preprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "        gdf_dataset: GeoDataFrame to verify\n",
    "        dataset_output_img_path: Path to image directory\n",
    "        dataset_output_masks_path: Path to mask directory\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame: Dataset with validation_processing column added\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"RUNNING FINAL VERIFICATION CHECKS (NO ZERO CHECK)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Make copy to avoid warnings\n",
    "    gdf_dataset = gdf_dataset.copy()\n",
    "    \n",
    "    # Initialize validation column\n",
    "    gdf_dataset['validation_processing'] = 'ok'\n",
    "    \n",
    "    print(f\"\\n0. Initialized validation_processing column for {len(gdf_dataset)} records\")\n",
    "    \n",
    "    # File existence verification\n",
    "    print(\"\\n1. Verifying all referenced files exist...\")\n",
    "    \n",
    "    missing_images = []\n",
    "    missing_masks = []\n",
    "    \n",
    "    for idx, row in gdf_dataset.iterrows():\n",
    "        # Check image files\n",
    "        img_path = row.get('processed_img_path_tif')\n",
    "        if pd.notna(img_path) and not os.path.exists(img_path):\n",
    "            missing_images.append(row['tile_id'])\n",
    "            gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "        \n",
    "        # Check mask files  \n",
    "        mask_path = row.get('processed_mask_path_tif')\n",
    "        if pd.notna(mask_path) and not os.path.exists(mask_path):\n",
    "            missing_masks.append(row['tile_id'])\n",
    "            gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "    \n",
    "    assert len(missing_images) == 0, f\"Missing image files for tiles: {missing_images[:10]}\"\n",
    "    assert len(missing_masks) == 0, f\"Missing mask files for tiles: {missing_masks[:10]}\"\n",
    "    print(f\"   All {len(gdf_dataset)} image and mask files exist\")\n",
    "    \n",
    "    # Dimension consistency check\n",
    "    print(\"\\n2. Verifying file dimensions are 1280x1280...\")\n",
    "    \n",
    "    target_size = (1280, 1280)\n",
    "    dimension_errors = []\n",
    "    \n",
    "    # Check sample for performance\n",
    "    sample_size = min(10, len(gdf_dataset))\n",
    "    sample_indices = np.random.choice(len(gdf_dataset), sample_size, replace=False)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        row = gdf_dataset.iloc[idx]\n",
    "        tile_id = row['tile_id']\n",
    "        \n",
    "        # Check image dimensions\n",
    "        img_path = row.get('processed_img_path_tif')\n",
    "        if pd.notna(img_path):\n",
    "            with rasterio.open(img_path) as src:\n",
    "                if (src.width, src.height) != target_size:\n",
    "                    dimension_errors.append(f\"Image {tile_id}: {src.width}x{src.height}\")\n",
    "                    gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "        \n",
    "        # Check mask dimensions\n",
    "        mask_path = row.get('processed_mask_path_tif')\n",
    "        if pd.notna(mask_path):\n",
    "            with rasterio.open(mask_path) as mask_src:\n",
    "                if (mask_src.width, mask_src.height) != target_size:\n",
    "                    dimension_errors.append(f\"Mask {tile_id}: {mask_src.width}x{mask_src.height}\")\n",
    "                    gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "    \n",
    "    assert len(dimension_errors) == 0, f\"Dimension errors found: {dimension_errors}\"\n",
    "    print(f\"   Sample check: All {sample_size} files have correct 1280x1280 dimensions\")\n",
    "    \n",
    "    # Column validation\n",
    "    print(\"\\n3. Validating critical columns...\")\n",
    "    \n",
    "    required_columns = ['tile_id', 'processed_img_path_tif', 'processed_mask_path_tif']\n",
    "    for col in required_columns:\n",
    "        assert col in gdf_dataset.columns, f\"Required column missing: {col}\"\n",
    "        null_count = gdf_dataset[col].isnull().sum()\n",
    "        assert null_count == 0, f\"Found {null_count} null values in required column: {col}\"\n",
    "    \n",
    "    print(\"   All required columns present with no null values\")\n",
    "    \n",
    "    # Geometry validation\n",
    "    print(\"\\n4. Validating geometries...\")\n",
    "    \n",
    "    if 'geometry' in gdf_dataset.columns:\n",
    "        null_geoms = gdf_dataset['geometry'].isnull().sum()\n",
    "        assert null_geoms == 0, f\"Found {null_geoms} null geometries\"\n",
    "        \n",
    "        invalid_geoms = []\n",
    "        for idx, row in gdf_dataset.iterrows():\n",
    "            if not row['geometry'].is_valid:\n",
    "                invalid_geoms.append(row['tile_id'])\n",
    "                gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "        \n",
    "        assert len(invalid_geoms) == 0, f\"Invalid geometries found for tiles: {invalid_geoms[:10]}\"\n",
    "        print(f\"   All {len(gdf_dataset)} geometries are valid\")\n",
    "    \n",
    "    # Directory structure check\n",
    "    print(\"\\n5. Verifying directory structure...\")\n",
    "    \n",
    "    assert os.path.exists(dataset_output_img_path), f\"Image directory does not exist: {dataset_output_img_path}\"\n",
    "    assert os.path.exists(dataset_output_masks_path), f\"Mask directory does not exist: {dataset_output_masks_path}\"\n",
    "    \n",
    "    # Count expected files\n",
    "    expected_img_files = set()\n",
    "    expected_mask_files = set()\n",
    "    \n",
    "    for idx, row in gdf_dataset.iterrows():\n",
    "        img_path = row.get('processed_img_path_tif')\n",
    "        mask_path = row.get('processed_mask_path_tif')\n",
    "        \n",
    "        if pd.notna(img_path):\n",
    "            expected_img_files.add(os.path.basename(img_path))\n",
    "        if pd.notna(mask_path):\n",
    "            expected_mask_files.add(os.path.basename(mask_path))\n",
    "    \n",
    "    actual_img_files = set([f for f in os.listdir(dataset_output_img_path) if f.endswith(('.tif', '.tiff'))])\n",
    "    actual_mask_files = set([f for f in os.listdir(dataset_output_masks_path) if f.endswith(('.tif', '.tiff'))])\n",
    "    \n",
    "    # Verify all expected files exist\n",
    "    missing_expected_imgs = expected_img_files - actual_img_files\n",
    "    missing_expected_masks = expected_mask_files - actual_mask_files\n",
    "    \n",
    "    assert len(missing_expected_imgs) == 0, f\"Expected image files missing: {list(missing_expected_imgs)[:5]}\"\n",
    "    assert len(missing_expected_masks) == 0, f\"Expected mask files missing: {list(missing_expected_masks)[:5]}\"\n",
    "    \n",
    "    print(f\"   Directory structure correct: {len(expected_img_files)} expected images, {len(expected_mask_files)} expected masks\")\n",
    "    \n",
    "    # Data integrity check\n",
    "    print(\"\\n6. Checking basic data integrity...\")\n",
    "    \n",
    "    # Check for empty rows\n",
    "    empty_rows = gdf_dataset.isnull().all(axis=1).sum()\n",
    "    assert empty_rows == 0, f\"Found {empty_rows} completely empty rows\"\n",
    "    \n",
    "    # Check dataset not empty\n",
    "    assert len(gdf_dataset) > 0, \"Dataset is empty\"\n",
    "    \n",
    "    # Check memory usage\n",
    "    memory_mb = gdf_dataset.memory_usage(deep=True).sum() / 1024**2\n",
    "    assert memory_mb < 1000, f\"Dataset unusually large: {memory_mb:.1f} MB\"\n",
    "    \n",
    "    print(f\"   Dataset integrity OK: {len(gdf_dataset)} records, {memory_mb:.1f} MB\")\n",
    "    \n",
    "    # Skip zero content check\n",
    "    print(\"\\n7. Zero content check: SKIPPED (already done in preprocessing)\")\n",
    "    \n",
    "    # Duplicate image detection\n",
    "    print(\"\\n8. Checking for duplicate images...\")\n",
    "    \n",
    "    # Calculate image hashes\n",
    "    image_hashes = {}\n",
    "    duplicate_groups = []\n",
    "    \n",
    "    for idx, row in gdf_dataset.iterrows():\n",
    "        img_path = row.get('processed_img_path_tif')\n",
    "        tile_id = row['tile_id']\n",
    "        \n",
    "        if pd.notna(img_path) and os.path.exists(img_path):\n",
    "            try:\n",
    "                with rasterio.open(img_path) as src:\n",
    "                    img_data = src.read()\n",
    "                    # Create hash of image data\n",
    "                    img_hash = hash(img_data.tobytes())\n",
    "                    \n",
    "                    if img_hash in image_hashes:\n",
    "                        # Found duplicate\n",
    "                        if len(image_hashes[img_hash]) == 1:\n",
    "                            # First duplicate, create group\n",
    "                            duplicate_groups.append(image_hashes[img_hash] + [tile_id])\n",
    "                        else:\n",
    "                            # Add to existing group\n",
    "                            for group in duplicate_groups:\n",
    "                                if image_hashes[img_hash][0] in group:\n",
    "                                    group.append(tile_id)\n",
    "                                    break\n",
    "                        \n",
    "                        image_hashes[img_hash].append(tile_id)\n",
    "                    else:\n",
    "                        image_hashes[img_hash] = [tile_id]\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   Error processing image {tile_id}: {e}\")\n",
    "                gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "    \n",
    "    # Mark duplicates as 'ko'\n",
    "    duplicate_count = 0\n",
    "    for group in duplicate_groups:\n",
    "        for tile_id in group:\n",
    "            tile_idx = gdf_dataset[gdf_dataset['tile_id'] == tile_id].index[0]\n",
    "            gdf_dataset.loc[tile_idx, 'validation_processing'] = 'ko'\n",
    "            duplicate_count += 1\n",
    "    \n",
    "    print(f\"   Found {len(duplicate_groups)} duplicate groups affecting {duplicate_count} files\")\n",
    "    if duplicate_groups:\n",
    "        print(f\"   Example duplicate group: {duplicate_groups[0]}\")\n",
    "    \n",
    "    # Validation summary\n",
    "    print(\"\\n9. Validation summary...\")\n",
    "    \n",
    "    validation_counts = gdf_dataset['validation_processing'].value_counts()\n",
    "    ok_count = validation_counts.get('ok', 0)\n",
    "    ko_count = validation_counts.get('ko', 0)\n",
    "    \n",
    "    print(f\"   Records marked 'ok': {ok_count}\")\n",
    "    print(f\"   Records marked 'ko': {ko_count}\")\n",
    "    print(f\"   Success rate: {ok_count/len(gdf_dataset)*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ALL VERIFICATION CHECKS PASSED\")\n",
    "    print(f\"Dataset validated: {ok_count} OK, {ko_count} KO\")\n",
    "    print(\"Dataset is ready for saving\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return gdf_dataset\n",
    "\n",
    "# Run verification\n",
    "gdf_dataset_final = final_verification_checks_no_zeros(\n",
    "    gdf_dataset=gdf_dataset_cleaned,\n",
    "    dataset_output_img_path=DATASET_OUTPUT_IMG_PATH,\n",
    "    dataset_output_masks_path=DATASET_OUTPUT_MASKS_PATH\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL RESULTS AFTER CLEANUP AND VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Final validation counts:\")\n",
    "print(gdf_dataset_final['validation_processing'].value_counts())\n",
    "print(f\"\\nDataset ready for saving: {len(gdf_dataset_final)} total records\")\n",
    "print(f\"High-quality records: {(gdf_dataset_final['validation_processing'] == 'ok').sum()}\")\n",
    "\n",
    "assert (gdf_dataset_final['validation_processing'] == 'ko').sum() == 0, \"There are still 'ko' records in the dataset\"           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final datasets as parquet files\n",
    "df_verification.to_parquet(VERIFICATION_OUTPUT_PARQUET_PATH, index=False)\n",
    "gdf_dataset.to_parquet(DATASET_OUTPUT_PARQUET_PATH, index=False)\n",
    "gdf_dataset_final.to_parquet(DATASET_FINAL_OUTPUT_PARQUET_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
