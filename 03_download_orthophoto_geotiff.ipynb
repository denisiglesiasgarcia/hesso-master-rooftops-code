{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthophoto Download and Processing\n",
    " \n",
    "**Objective:** Download SITG orthophotos, generate tiles and visualizations.\n",
    " \n",
    "**Workflow:**\n",
    "1. URL discovery and validation for orthophoto archives\n",
    "2. Parallel download management\n",
    "3. Extraction and processing\n",
    "4. Tile generation and coverage analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from loguru import logger\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from asyncio import Queue\n",
    "from collections import Counter\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "import hashlib\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Suppress non-critical warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_URL = False  # Set to True to update URLs from the source file\n",
    "OUTPUT_URL_CSV_PATH = \"data/notebook_03/url\"\n",
    "OUTPUT_LOGS_PATH = \"data/notebook_03/logs\"\n",
    "\n",
    "GEOTIFF_PATH = \"data/SITG/ortho2019\"\n",
    "GEOTIFF_ZIP_PATH = \"data/SITG/ortho_2019_zip_2024-11-10\"\n",
    "\n",
    "OUTPUT_QUADRILLAGE_PARQUET_PATH = \"data/notebook_03/parquet/03_quadrillage.parquet\"\n",
    "VIZ_OUTPUT_PATH = \"data/notebook_03/graphics\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured directory exists: data/notebook_03/url\n",
      "Ensured directory exists: data/notebook_03/logs\n",
      "Ensured directory exists: data/notebook_03/graphics\n",
      "Ensured directory exists: data/SITG/ortho2019\n",
      "Ensured directory exists: data/SITG/ortho_2019_zip_2024-11-10\n",
      "Ensured directory exists: data/notebook_03/parquet\n"
     ]
    }
   ],
   "source": [
    "# Ensure all output directories exist\n",
    "directories = [\n",
    "    OUTPUT_URL_CSV_PATH,\n",
    "    OUTPUT_LOGS_PATH,\n",
    "    VIZ_OUTPUT_PATH,\n",
    "    GEOTIFF_PATH,\n",
    "    GEOTIFF_ZIP_PATH,\n",
    "    Path(OUTPUT_QUADRILLAGE_PARQUET_PATH).parent\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Ensured directory exists: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Discovery for GeoTIFF Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure structured logging\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    Path(OUTPUT_LOGS_PATH) / \"url_checker_{time}.log\",\n",
    "    rotation=\"1 day\",\n",
    "    retention=\"7 days\",\n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\",\n",
    "    level=\"INFO\",\n",
    ")\n",
    "\n",
    "# Add console output for real-time monitoring\n",
    "logger.add(lambda msg: tqdm.write(msg), level=\"INFO\", format=\"{message}\")\n",
    "\n",
    "async def check_url(session, number, sem, queue):\n",
    "    \"\"\"\n",
    "    Check if a URL exists for a given orthophoto tile number.\n",
    "    \n",
    "    Uses HEAD requests for efficiency - only checks if resource exists\n",
    "    without downloading the actual content.\n",
    "    \n",
    "    Parameters:\n",
    "        session: aiohttp session for connection pooling\n",
    "        number: Tile number to check\n",
    "        sem: Semaphore for concurrency control\n",
    "        queue: Queue for collecting valid URLs\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if URL is valid (200 status)\n",
    "    \"\"\"\n",
    "    base_url = \"https://ge.ch/sitg/geodata/SITG/TELECHARGEMENT/ORTHO_2019\"\n",
    "    filename = f\"{number}.tif.zip\"\n",
    "    url = f\"{base_url}/{filename}\"\n",
    "\n",
    "    async with sem:  # Respect concurrency limits\n",
    "        try:\n",
    "            async with session.head(\n",
    "                url, timeout=aiohttp.ClientTimeout(total=3)\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    await queue.put(url)\n",
    "                    return True\n",
    "                return False\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "async def progress_reporter(queue, total_urls):\n",
    "    \"\"\"\n",
    "    Report progress statistics during URL discovery.\n",
    "    \n",
    "    Provides real-time feedback on discovery rate and estimated completion.\n",
    "    \"\"\"\n",
    "    valid_urls = []\n",
    "    start_time = time.time()\n",
    "    counter = Counter()\n",
    "\n",
    "    while True:\n",
    "        url = await queue.get()\n",
    "        if url == \"DONE\":\n",
    "            break\n",
    "\n",
    "        valid_urls.append(url)\n",
    "        counter[\"found\"] += 1\n",
    "\n",
    "        # Report progress at regular intervals\n",
    "        if counter[\"found\"] % 10 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            speed = counter[\"found\"] / elapsed_time if elapsed_time > 0 else 0\n",
    "            logger.info(\n",
    "                f\"Found {counter['found']} URLs | \"\n",
    "                f\"Speed: {speed:.2f} URLs/s | \"\n",
    "                f\"Progress: {(counter['found']/total_urls*100):.2f}%\"\n",
    "            )\n",
    "\n",
    "    return valid_urls\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main async function for URL discovery.\n",
    "    \n",
    "    Scans a range of potential tile numbers to find valid orthophoto URLs.\n",
    "    Uses high concurrency for efficiency while respecting server limits.\n",
    "    \"\"\"\n",
    "    # Define search range based on known tile numbering pattern\n",
    "    start_number = 24_800_000\n",
    "    end_number = 25_100_000\n",
    "    total_urls = end_number - start_number + 1\n",
    "\n",
    "    # Performance tuning parameters\n",
    "    max_concurrent = 1000  # Concurrent connections limit\n",
    "    chunk_size = 1500      # URLs to process per batch\n",
    "\n",
    "    sem = asyncio.Semaphore(max_concurrent)\n",
    "    queue = Queue()\n",
    "    valid_urls = []\n",
    "\n",
    "    # Start progress monitoring\n",
    "    reporter_task = asyncio.create_task(progress_reporter(queue, total_urls))\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Process URLs in chunks to avoid overwhelming memory\n",
    "        for chunk_start in tqdm(range(start_number, end_number + 1, chunk_size)):\n",
    "            chunk_end = min(chunk_start + chunk_size, end_number + 1)\n",
    "            chunk_numbers = range(chunk_start, chunk_end)\n",
    "\n",
    "            tasks = [check_url(session, num, sem, queue) for num in chunk_numbers]\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    # Signal completion and collect results\n",
    "    await queue.put(\"DONE\")\n",
    "    valid_urls = await reporter_task\n",
    "\n",
    "    # Save results with timestamp for tracking\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = f\"{OUTPUT_URL_CSV_PATH}/valid_urls_{timestamp}.txt\"\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for url in valid_urls:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "    logger.info(f\"Discovery complete: {len(valid_urls)} valid URLs found\")\n",
    "    logger.info(f\"Results saved to: {output_file}\")\n",
    "\n",
    "if UPDATE_URL:\n",
    "    # Execute URL discovery\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class OrthophotoDownloader:\n",
    "    \"\"\"\n",
    "    Async downloader for SITG orthophoto tiles.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir, max_concurrent=10, max_retries=3):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.max_retries = max_retries\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        \n",
    "        self.stats = {\n",
    "            'downloaded': 0,\n",
    "            'failed': 0,\n",
    "            'skipped': 0,\n",
    "            'total_bytes': 0,\n",
    "            'start_time': None\n",
    "        }\n",
    "        \n",
    "        self.failed_downloads = []\n",
    "        \n",
    "    async def download_file(self, session, url, progress_bar):\n",
    "        \"\"\"\n",
    "        Download a single file with retry logic.\n",
    "        \"\"\"\n",
    "        filename = url.split('/')[-1].strip()\n",
    "        filepath = self.output_dir / filename\n",
    "        \n",
    "        if filepath.exists() and filepath.stat().st_size > 0:\n",
    "            self.stats['skipped'] += 1\n",
    "            progress_bar.update(1)\n",
    "            return True\n",
    "        \n",
    "        async with self.semaphore:\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    timeout = aiohttp.ClientTimeout(total=300)\n",
    "                    async with session.get(url, timeout=timeout) as response:\n",
    "                        if response.status == 200:\n",
    "                            total_size = int(response.headers.get('Content-Length', 0))\n",
    "                            temp_filepath = filepath.with_suffix('.tmp')\n",
    "                            \n",
    "                            with open(temp_filepath, 'wb') as f:\n",
    "                                downloaded = 0\n",
    "                                async for chunk in response.content.iter_chunked(8192):\n",
    "                                    f.write(chunk)\n",
    "                                    downloaded += len(chunk)\n",
    "                            \n",
    "                            if total_size > 0 and downloaded != total_size:\n",
    "                                raise Exception(f\"Incomplete download: {downloaded}/{total_size} bytes\")\n",
    "                            \n",
    "                            temp_filepath.rename(filepath)\n",
    "                            \n",
    "                            self.stats['downloaded'] += 1\n",
    "                            self.stats['total_bytes'] += downloaded\n",
    "                            progress_bar.update(1)\n",
    "                            return True\n",
    "                        else:\n",
    "                            raise Exception(f\"HTTP {response.status}\")\n",
    "                            \n",
    "                except asyncio.TimeoutError:\n",
    "                    logger.warning(f\"Timeout downloading {filename} (attempt {attempt + 1}/{self.max_retries})\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error downloading {filename}: {str(e)} (attempt {attempt + 1}/{self.max_retries})\")\n",
    "                \n",
    "                if attempt < self.max_retries - 1:\n",
    "                    await asyncio.sleep(2 ** attempt)\n",
    "            \n",
    "            self.stats['failed'] += 1\n",
    "            self.failed_downloads.append(url)\n",
    "            progress_bar.update(1)\n",
    "            return False\n",
    "    \n",
    "    async def download_batch(self, urls):\n",
    "        \"\"\"\n",
    "        Download a batch of URLs with progress tracking.\n",
    "        \"\"\"\n",
    "        self.stats['start_time'] = time.time()\n",
    "        \n",
    "        connector = aiohttp.TCPConnector(\n",
    "            limit=self.max_concurrent * 2,\n",
    "            limit_per_host=self.max_concurrent\n",
    "        )\n",
    "        \n",
    "        async with aiohttp.ClientSession(connector=connector) as session:\n",
    "            progress_bar = async_tqdm(\n",
    "                total=len(urls),\n",
    "                desc=\"Downloading orthophotos\",\n",
    "                unit=\"files\"\n",
    "            )\n",
    "            \n",
    "            tasks = [\n",
    "                self.download_file(session, url, progress_bar) \n",
    "                for url in urls\n",
    "            ]\n",
    "            \n",
    "            await asyncio.gather(*tasks)\n",
    "            progress_bar.close()\n",
    "    \n",
    "    def get_report(self):\n",
    "        \"\"\"\n",
    "        Generate download report.\n",
    "        \"\"\"\n",
    "        elapsed_time = time.time() - self.stats['start_time']\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"DOWNLOAD REPORT\")\n",
    "        report.append(f\"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report.append(f\"Duration: {elapsed_time:.1f} seconds\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"Results:\")\n",
    "        report.append(f\"  Downloaded: {self.stats['downloaded']} files\")\n",
    "        report.append(f\"  Skipped (existing): {self.stats['skipped']} files\")\n",
    "        report.append(f\"  Failed: {self.stats['failed']} files\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"Performance:\")\n",
    "        report.append(f\"  Total data: {self.stats['total_bytes'] / (1024**3):.2f} GB\")\n",
    "        report.append(f\"  Average speed: {(self.stats['total_bytes'] / elapsed_time) / (1024**2):.2f} MB/s\")\n",
    "        report.append(f\"  Files per minute: {(self.stats['downloaded'] / elapsed_time) * 60:.1f}\")\n",
    "        \n",
    "        if self.failed_downloads:\n",
    "            report.append(\"\")\n",
    "            report.append(f\"Failed downloads ({len(self.failed_downloads)} files):\")\n",
    "            for url in self.failed_downloads[:10]:\n",
    "                report.append(f\"  {url}\")\n",
    "            if len(self.failed_downloads) > 10:\n",
    "                report.append(f\"  ... and {len(self.failed_downloads) - 10} more\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "    def save_failed_urls(self, filepath):\n",
    "        \"\"\"\n",
    "        Save failed URLs for retry.\n",
    "        \"\"\"\n",
    "        if self.failed_downloads:\n",
    "            with open(filepath, 'w') as f:\n",
    "                for url in self.failed_downloads:\n",
    "                    f.write(url + '\\n')\n",
    "            print(f\"Saved {len(self.failed_downloads)} failed URLs to {filepath}\")\n",
    "\n",
    "\n",
    "async def download_missing_orthophotos(missing_urls_file, output_dir, max_concurrent=20):\n",
    "    \"\"\"\n",
    "    Download missing orthophoto files.\n",
    "    \n",
    "    Parameters:\n",
    "        missing_urls_file: Path to file containing URLs to download\n",
    "        output_dir: Directory to save downloaded files\n",
    "        max_concurrent: Maximum concurrent downloads\n",
    "    \"\"\"\n",
    "    with open(missing_urls_file, 'r') as f:\n",
    "        urls = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"Found {len(urls)} URLs to download\")\n",
    "    \n",
    "    if not urls:\n",
    "        print(\"No URLs to download\")\n",
    "        return\n",
    "    \n",
    "    downloader = OrthophotoDownloader(\n",
    "        output_dir=output_dir,\n",
    "        max_concurrent=max_concurrent,\n",
    "        max_retries=3\n",
    "    )\n",
    "    \n",
    "    await downloader.download_batch(urls)\n",
    "    \n",
    "    print(downloader.get_report())\n",
    "    \n",
    "    if downloader.failed_downloads:\n",
    "        failed_file = Path(missing_urls_file).parent / \"failed_downloads.txt\"\n",
    "        downloader.save_failed_urls(failed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_all_zips():\n",
    "    \"\"\"\n",
    "    Extract all downloaded ZIP files to GeoTIFF directory.\n",
    "    \"\"\"\n",
    "    zip_path = Path(GEOTIFF_ZIP_PATH)\n",
    "    output_path = Path(GEOTIFF_PATH)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    zip_files = list(zip_path.glob(\"*.zip\"))\n",
    "    print(f\"Found {len(zip_files)} ZIP files to process\")\n",
    "    \n",
    "    if not zip_files:\n",
    "        return\n",
    "    \n",
    "    def extract_single_zip(zip_file):\n",
    "        \"\"\"Extract a single ZIP file.\"\"\"\n",
    "        try:\n",
    "            tif_name = zip_file.stem\n",
    "            tif_path = output_path / tif_name\n",
    "            \n",
    "            if tif_path.exists():\n",
    "                return f\"Skipped {zip_file.name} (already extracted)\"\n",
    "            \n",
    "            with zipfile.ZipFile(zip_file, 'r') as zf:\n",
    "                temp_dir = output_path / f\".tmp_{zip_file.stem}\"\n",
    "                temp_dir.mkdir(exist_ok=True)\n",
    "                \n",
    "                zf.extractall(temp_dir)\n",
    "                \n",
    "                for extracted_file in temp_dir.iterdir():\n",
    "                    extracted_file.rename(output_path / extracted_file.name)\n",
    "                \n",
    "                temp_dir.rmdir()\n",
    "                \n",
    "            return f\"Extracted {zip_file.name}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Failed to extract {zip_file.name}: {str(e)}\"\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = {executor.submit(extract_single_zip, zf): zf for zf in zip_files}\n",
    "        \n",
    "        with tqdm(total=len(zip_files), desc=\"Extracting ZIPs\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                logger.info(result)\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_complete_workflow():\n",
    "    \"\"\"\n",
    "    Execute the complete orthophoto processing pipeline.\n",
    "    \"\"\"\n",
    "    print(\"ORTHOPHOTO PROCESSING WORKFLOW\\n\")\n",
    "    \n",
    "    # URL Discovery\n",
    "    if UPDATE_URL:\n",
    "        print(\"Discovering orthophoto URLs...\")\n",
    "        await main()\n",
    "    else:\n",
    "        print(\"Skipping URL discovery (UPDATE_URL = False)\")\n",
    "    \n",
    "    # Consolidate URLs and check download status\n",
    "    print(\"\\nChecking download status...\")\n",
    "    valid_urls = consolidate_url_lists()\n",
    "    downloaded, missing = check_download_status(valid_urls)\n",
    "    \n",
    "    # Download missing files\n",
    "    if missing:\n",
    "        print(f\"\\nDownloading {len(missing)} missing files...\")\n",
    "        missing_urls_file = f\"{OUTPUT_URL_CSV_PATH}/missing_urls.txt\"\n",
    "        \n",
    "        downloader = OrthophotoDownloader(\n",
    "            output_dir=GEOTIFF_ZIP_PATH,\n",
    "            max_concurrent=20,\n",
    "            max_retries=3\n",
    "        )\n",
    "        \n",
    "        with open(missing_urls_file, 'r') as f:\n",
    "            urls_to_download = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        await downloader.download_batch(urls_to_download)\n",
    "        print(downloader.get_report())\n",
    "    else:\n",
    "        print(\"\\nNo files to download - all files present\")\n",
    "    \n",
    "    # Extract ZIP files\n",
    "    print(\"\\nExtracting ZIP files...\")\n",
    "    await extract_all_zips()\n",
    "    \n",
    "    # Create spatial index\n",
    "    print(\"\\nCreating spatial index...\")\n",
    "    gdf_quadrillage_geotiff = create_geotiff_index(\n",
    "        GEOTIFF_PATH, \n",
    "        OUTPUT_QUADRILLAGE_PARQUET_PATH\n",
    "    )\n",
    "    \n",
    "    # Analyze metadata and quality\n",
    "    print(\"\\nAnalyzing data quality...\")\n",
    "    analyze_tfw_geotiff(gdf_quadrillage_geotiff)\n",
    "    check_duplicates(gdf_quadrillage_geotiff)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(\"\\nGenerating coverage visualizations...\")\n",
    "    stats = visualize_coverage(gdf_quadrillage_geotiff, VIZ_OUTPUT_PATH)\n",
    "    \n",
    "    print(\"\\nWORKFLOW COMPLETE\")\n",
    "    return gdf_quadrillage_geotiff, stats\n",
    "\n",
    "\n",
    "# Directory setup\n",
    "def setup_directories():\n",
    "    \"\"\"\n",
    "    Ensure all required directories exist.\n",
    "    \"\"\"\n",
    "    directories = [\n",
    "        OUTPUT_URL_CSV_PATH,\n",
    "        OUTPUT_LOGS_PATH,\n",
    "        VIZ_OUTPUT_PATH,\n",
    "        GEOTIFF_PATH,\n",
    "        GEOTIFF_ZIP_PATH,\n",
    "        Path(OUTPUT_QUADRILLAGE_PARQUET_PATH).parent\n",
    "    ]\n",
    "\n",
    "    for directory in directories:\n",
    "        Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    return directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Downloaded Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_url_lists():\n",
    "    \"\"\"\n",
    "    Combine multiple URL discovery runs and check download status.\n",
    "    \n",
    "    Handles duplicate URLs across multiple discovery sessions and\n",
    "    identifies which files still need to be downloaded.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "\n",
    "    # Gather all URL lists except combined ones\n",
    "    txt_files = glob.glob(f\"{OUTPUT_URL_CSV_PATH}/*.txt\")\n",
    "    txt_files = [f for f in txt_files if \"combined\" not in f]\n",
    "\n",
    "    # Consolidate unique URLs\n",
    "    valid_urls = set()\n",
    "    for txt_file in txt_files:\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            urls = f.readlines()\n",
    "            valid_urls.update(urls)\n",
    "\n",
    "    # Save consolidated list\n",
    "    output_file = f\"{OUTPUT_URL_CSV_PATH}/00_valid_urls_combined.txt\"\n",
    "    print(f\"Saving {len(valid_urls)} unique URLs to {output_file}\")\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.writelines(valid_urls)\n",
    "\n",
    "    return valid_urls\n",
    "\n",
    "def check_download_status(valid_urls):\n",
    "    \"\"\"\n",
    "    Compare discovered URLs against downloaded files.\n",
    "    \n",
    "    Identifies which files have been successfully downloaded and\n",
    "    which are still pending.\n",
    "    \"\"\"\n",
    "    valid_filenames = [url.split(\"/\")[-1].strip() for url in valid_urls]\n",
    "    filename_to_url = {url.split(\"/\")[-1].strip(): url.strip() for url in valid_urls}\n",
    "\n",
    "    if not os.path.exists(GEOTIFF_ZIP_PATH):\n",
    "        print(f\"Download directory does not exist: {GEOTIFF_ZIP_PATH}\")\n",
    "        os.makedirs(GEOTIFF_ZIP_PATH, exist_ok=True)\n",
    "        zip_files = []\n",
    "    else:\n",
    "        zip_files = os.listdir(GEOTIFF_ZIP_PATH)\n",
    "    \n",
    "    downloaded_files = set(valid_filenames).intersection(zip_files)\n",
    "\n",
    "    print(f\"Download status:\")\n",
    "    print(f\"  Download directory: {GEOTIFF_ZIP_PATH}\")\n",
    "    print(f\"  Already downloaded: {len(downloaded_files)} files\")\n",
    "    \n",
    "    missing_files = set(valid_filenames) - downloaded_files\n",
    "    print(f\"  Still needed: {len(missing_files)} files\")\n",
    "\n",
    "    if missing_files:\n",
    "        missing_urls_file = f\"{OUTPUT_URL_CSV_PATH}/missing_urls.txt\"\n",
    "        print(f\"\\nSaving {len(missing_files)} missing URLs to: {missing_urls_file}\")\n",
    "        \n",
    "        with open(missing_urls_file, \"w\") as f:\n",
    "            for filename in missing_files:\n",
    "                f.write(filename_to_url[filename] + \"\\n\")\n",
    "\n",
    "    return downloaded_files, missing_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Orthophoto Tile Index\n",
    "### Tile Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfw(tif_path):\n",
    "    \"\"\"\n",
    "    Read TFW (World File) associated with a GeoTIFF.\n",
    "    \n",
    "    TFW files contain georeferencing information in a simple text format:\n",
    "    - Line 1: Pixel X size\n",
    "    - Line 2: Rotation about Y axis\n",
    "    - Line 3: Rotation about X axis\n",
    "    - Line 4: Pixel Y size (usually negative)\n",
    "    - Line 5: X coordinate of upper left pixel center\n",
    "    - Line 6: Y coordinate of upper left pixel center\n",
    "    \n",
    "    Parameters:\n",
    "        tif_path (Path): Path to the GeoTIFF file\n",
    "        \n",
    "    Returns:\n",
    "        dict: TFW parameters if found, None otherwise\n",
    "    \"\"\"\n",
    "    tif_path = Path(tif_path)\n",
    "    \n",
    "    # Check common TFW file extensions\n",
    "    tfw_paths = [\n",
    "        tif_path.with_suffix(\".tfw\"),\n",
    "        tif_path.with_suffix(\".tifw\"),\n",
    "        Path(str(tif_path.with_suffix(\"\")) + \".tfw\"),\n",
    "    ]\n",
    "\n",
    "    for tfw_path in tfw_paths:\n",
    "        if tfw_path.exists():\n",
    "            try:\n",
    "                with open(tfw_path, \"r\") as f:\n",
    "                    lines = [float(line.strip()) for line in f.readlines()]\n",
    "\n",
    "                if len(lines) != 6:\n",
    "                    print(\n",
    "                        f\"Warning: Invalid TFW format in {tfw_path} \"\n",
    "                        f\"(expected 6 lines, found {len(lines)})\"\n",
    "                    )\n",
    "                    return None\n",
    "\n",
    "                return {\n",
    "                    \"x_scale\": lines[0],\n",
    "                    \"y_rotation\": lines[1],\n",
    "                    \"x_rotation\": lines[2],\n",
    "                    \"y_scale\": lines[3],\n",
    "                    \"x_origin\": lines[4],\n",
    "                    \"y_origin\": lines[5],\n",
    "                    \"tfw_path\": str(tfw_path),\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error reading TFW file {tfw_path}: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def process_single_geotiff(tif_path):\n",
    "    \"\"\"\n",
    "    Extract comprehensive metadata from a single GeoTIFF file.\n",
    "    \n",
    "    Processes both embedded GeoTIFF metadata and external TFW files,\n",
    "    creating a unified spatial footprint for the image.\n",
    "    \n",
    "    Parameters:\n",
    "        tif_path (Path): Path to GeoTIFF file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Metadata including geometry, CRS, and file properties\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tif_path = Path(tif_path)\n",
    "        result = {\n",
    "            \"file_path\": str(tif_path),\n",
    "            \"file_name\": tif_path.name,\n",
    "            \"size_mb\": os.path.getsize(tif_path) / (1024 * 1024),\n",
    "            \"errors\": [],\n",
    "        }\n",
    "\n",
    "        # Prioritize TFW data as it's often more reliable for Swiss data\n",
    "        tfw_data = read_tfw(tif_path)\n",
    "        if tfw_data:\n",
    "            result.update(\n",
    "                {\n",
    "                    \"has_tfw\": True,\n",
    "                    \"tfw_path\": tfw_data[\"tfw_path\"],\n",
    "                    \"tfw_x_scale\": tfw_data[\"x_scale\"],\n",
    "                    \"tfw_y_scale\": tfw_data[\"y_scale\"],\n",
    "                    \"tfw_x_rotation\": tfw_data[\"x_rotation\"],\n",
    "                    \"tfw_y_rotation\": tfw_data[\"y_rotation\"],\n",
    "                    \"tfw_x_origin\": tfw_data[\"x_origin\"],\n",
    "                    \"tfw_y_origin\": tfw_data[\"y_origin\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Infer CRS from coordinate ranges (Swiss specific)\n",
    "            if (\n",
    "                2485000 <= tfw_data[\"x_origin\"] <= 2834000\n",
    "                and 1075000 <= tfw_data[\"y_origin\"] <= 1299000\n",
    "            ):\n",
    "                result[\"suggested_crs\"] = \"EPSG:2056\"  # CH1903+/LV95\n",
    "            elif (\n",
    "                485000 <= tfw_data[\"x_origin\"] <= 834000\n",
    "                and 75000 <= tfw_data[\"y_origin\"] <= 299000\n",
    "            ):\n",
    "                result[\"suggested_crs\"] = \"EPSG:21781\"  # CH1903/LV03\n",
    "            else:\n",
    "                result[\"suggested_crs\"] = None\n",
    "        else:\n",
    "            result[\"has_tfw\"] = False\n",
    "            result[\"suggested_crs\"] = None\n",
    "\n",
    "        # Read embedded GeoTIFF metadata\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            # Basic raster properties\n",
    "            result.update(\n",
    "                {\n",
    "                    \"width\": src.width,\n",
    "                    \"height\": src.height,\n",
    "                    \"band_count\": src.count,\n",
    "                    \"data_type\": str(src.profile.get(\"dtype\")),\n",
    "                    \"resolution_x\": src.res[0] if src.res else None,\n",
    "                    \"resolution_y\": src.res[1] if src.res else None,\n",
    "                    \"has_crs\": src.crs is not None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Handle CRS detection\n",
    "            if src.crs:\n",
    "                result[\"original_crs\"] = src.crs.to_string()\n",
    "            else:\n",
    "                result[\"original_crs\"] = result.get(\"suggested_crs\")\n",
    "                if result[\"original_crs\"]:\n",
    "                    result[\"errors\"].append(\n",
    "                        f\"No CRS in GeoTIFF, using suggested CRS from TFW: {result['original_crs']}\"\n",
    "                    )\n",
    "                else:\n",
    "                    result[\"errors\"].append(\n",
    "                        \"No CRS found and cannot determine from coordinates\"\n",
    "                    )\n",
    "\n",
    "            # Extract transform information\n",
    "            transform = src.transform\n",
    "            if transform and transform.is_identity is False:\n",
    "                result.update(\n",
    "                    {\n",
    "                        \"geotiff_x_scale\": transform.a,\n",
    "                        \"geotiff_y_scale\": transform.e,\n",
    "                        \"geotiff_x_rotation\": transform.b,\n",
    "                        \"geotiff_y_rotation\": transform.d,\n",
    "                        \"geotiff_x_origin\": transform.c,\n",
    "                        \"geotiff_y_origin\": transform.f,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Create spatial footprint - prefer TFW if available\n",
    "            if result.get(\"has_tfw\"):\n",
    "                # Calculate bounds from TFW parameters\n",
    "                left = result[\"tfw_x_origin\"]\n",
    "                top = result[\"tfw_y_origin\"]\n",
    "                right = left + (src.width * result[\"tfw_x_scale\"])\n",
    "                bottom = top + (src.height * result[\"tfw_y_scale\"])\n",
    "                footprint = box(left, bottom, right, top)\n",
    "                crs = result.get(\"suggested_crs\", \"EPSG:2056\")\n",
    "            else:\n",
    "                # Use GeoTIFF bounds\n",
    "                bounds = src.bounds\n",
    "                footprint = box(bounds.left, bounds.bottom, bounds.right, bounds.top)\n",
    "                crs = src.crs or \"EPSG:2056\"\n",
    "\n",
    "            # Create temporary GeoDataFrame for CRS transformation\n",
    "            temp_gdf = gpd.GeoDataFrame(geometry=[footprint], crs=crs)\n",
    "\n",
    "            # Ensure consistent CRS (Swiss LV95)\n",
    "            if temp_gdf.crs != \"EPSG:2056\":\n",
    "                try:\n",
    "                    temp_gdf = temp_gdf.to_crs(\"EPSG:2056\")\n",
    "                except Exception as e:\n",
    "                    result[\"errors\"].append(\n",
    "                        f\"CRS transformation failed: {str(e)}\"\n",
    "                    )\n",
    "                    return None\n",
    "\n",
    "            # Validate geometry\n",
    "            if not temp_gdf.geometry[0].is_valid:\n",
    "                temp_gdf.geometry = temp_gdf.geometry.buffer(0)\n",
    "                result[\"errors\"].append(\"Applied geometry correction\")\n",
    "\n",
    "            if np.any(np.isinf(temp_gdf.geometry[0].bounds)):\n",
    "                result[\"errors\"].append(\"Invalid bounds detected\")\n",
    "                return None\n",
    "\n",
    "            result[\"geometry\"] = temp_gdf.geometry[0]\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {tif_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_geotiff_index(input_folder, output_parquet, recursive=True):\n",
    "    \"\"\"\n",
    "    Create a comprehensive spatial index of all GeoTIFF files.\n",
    "    \n",
    "    Generates a GeoParquet file containing footprints and metadata\n",
    "    for efficient spatial queries and coverage analysis.\n",
    "    \n",
    "    Parameters:\n",
    "        input_folder (str): Directory containing GeoTIFF files\n",
    "        output_parquet (str): Output path for GeoParquet file\n",
    "        recursive (bool): Search subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame: Spatial index of all GeoTIFF files\n",
    "    \"\"\"\n",
    "    input_path = Path(input_folder)\n",
    "    if not input_path.exists():\n",
    "        raise ValueError(f\"Input folder does not exist: {input_folder}\")\n",
    "\n",
    "    # Locate all GeoTIFF files\n",
    "    if recursive:\n",
    "        tif_files = list(input_path.rglob(\"*.tif\")) + list(input_path.rglob(\"*.tiff\"))\n",
    "    else:\n",
    "        tif_files = list(input_path.glob(\"*.tif\")) + list(input_path.glob(\"*.tiff\"))\n",
    "\n",
    "    if not tif_files:\n",
    "        raise ValueError(f\"No GeoTIFF files found in {input_folder}\")\n",
    "\n",
    "    print(f\"Found {len(tif_files)} GeoTIFF files\")\n",
    "\n",
    "    # Process files with progress tracking\n",
    "    results = []\n",
    "    for tif_path in tqdm(tif_files, desc=\"Processing GeoTIFFs\"):\n",
    "        try:\n",
    "            result = process_single_geotiff(tif_path)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {tif_path}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if not results:\n",
    "        raise ValueError(\"No GeoTIFF files could be processed successfully\")\n",
    "\n",
    "    # Create spatial dataframe\n",
    "    gdf = gpd.GeoDataFrame(results, crs=\"EPSG:2056\")\n",
    "\n",
    "    # Add relative paths for portability\n",
    "    gdf[\"relative_path\"] = gdf[\"file_path\"].apply(\n",
    "        lambda x: str(Path(x).relative_to(input_path))\n",
    "    )\n",
    "\n",
    "    # Ensure geometry integrity\n",
    "    gdf.set_geometry('geometry', inplace=True)\n",
    "    \n",
    "    if gdf.crs is None:\n",
    "        gdf.set_crs(epsg=2056, inplace=True)\n",
    "\n",
    "    print(\"Validating geometries...\")\n",
    "    gdf['geometry'] = gdf['geometry'].buffer(0)\n",
    "\n",
    "    # Convert error lists to strings for Parquet compatibility\n",
    "    gdf['errors'] = gdf['errors'].apply(lambda x: str(x))\n",
    "\n",
    "    # Save in multiple formats for flexibility\n",
    "    gdf.to_parquet(output_parquet)\n",
    "    gdf.to_file(output_parquet.replace(\".parquet\", \".gpkg\"), driver=\"GPKG\")\n",
    "    print(\"Saved spatial index to:\")\n",
    "    print(f\"  - {output_parquet}\")\n",
    "    print(f\"  - {output_parquet.replace('.parquet', '.gpkg')}\")\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def analyze_tfw_geotiff(gdf):\n",
    "    \"\"\"\n",
    "    Analyze consistency between TFW and GeoTIFF metadata.\n",
    "    \n",
    "    Identifies discrepancies that might indicate data quality issues.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== METADATA ANALYSIS ===\")\n",
    "    print(f\"Total files processed: {len(gdf)}\")\n",
    "    print(f\"Files with TFW: {gdf['has_tfw'].sum()}\")\n",
    "    print(f\"Files with embedded CRS: {gdf['has_crs'].sum()}\")\n",
    "\n",
    "    # CRS distribution\n",
    "    print(\"\\nCRS Distribution:\")\n",
    "    print(gdf[\"original_crs\"].value_counts())\n",
    "\n",
    "    # Suggested CRS from coordinates\n",
    "    print(\"\\nInferred CRS from coordinates:\")\n",
    "    print(gdf[\"suggested_crs\"].value_counts())\n",
    "\n",
    "    # Resolution analysis for TFW files\n",
    "    tfw_files = gdf[gdf[\"has_tfw\"]]\n",
    "    if not tfw_files.empty:\n",
    "        print(\"\\nTFW Resolution Statistics:\")\n",
    "        print(f\"X resolution: {tfw_files['tfw_x_scale'].min():.3f} to {tfw_files['tfw_x_scale'].max():.3f}\")\n",
    "        print(f\"Y resolution: {tfw_files['tfw_y_scale'].min():.3f} to {tfw_files['tfw_y_scale'].max():.3f}\")\n",
    "\n",
    "        # Check for rotated images\n",
    "        rotated = tfw_files[\n",
    "            (tfw_files[\"tfw_x_rotation\"] != 0) | (tfw_files[\"tfw_y_rotation\"] != 0)\n",
    "        ]\n",
    "        if not rotated.empty:\n",
    "            print(f\"\\nFound {len(rotated)} rotated images\")\n",
    "\n",
    "    # Compare TFW vs GeoTIFF parameters\n",
    "    mask = gdf[\"has_tfw\"] & gdf[\"has_crs\"]\n",
    "    if mask.any():\n",
    "        comparison = gdf[mask].copy()\n",
    "        print(\"\\nTFW vs GeoTIFF Parameter Comparison:\")\n",
    "        \n",
    "        matches = 0\n",
    "        total = len(comparison)\n",
    "\n",
    "        for _, row in comparison.iterrows():\n",
    "            if all([\n",
    "                np.isclose(row[\"tfw_x_scale\"], row[\"geotiff_x_scale\"]),\n",
    "                np.isclose(row[\"tfw_y_scale\"], row[\"geotiff_y_scale\"]),\n",
    "                np.isclose(row[\"tfw_x_origin\"], row[\"geotiff_x_origin\"]),\n",
    "                np.isclose(row[\"tfw_y_origin\"], row[\"geotiff_y_origin\"]),\n",
    "            ]):\n",
    "                matches += 1\n",
    "\n",
    "        print(f\"Files with matching parameters: {matches}/{total}\")\n",
    "\n",
    "    # Report files with errors\n",
    "    error_mask = gdf[\"errors\"].apply(lambda x: len(x) > 0)\n",
    "    if error_mask.any():\n",
    "        print(f\"\\nFiles with processing notes: {error_mask.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create the spatial index\n",
    "# gdf_quadrillage_geotiff = create_geotiff_index(GEOTIFF_PATH, OUTPUT_QUADRILLAGE_PARQUET_PATH)\n",
    "\n",
    "# # Analyze results\n",
    "# analyze_tfw_geotiff(gdf_quadrillage_geotiff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract tile number from filename.\n",
    "    Example: '24851110.tif' -> '24851110'\n",
    "    \"\"\"\n",
    "    return filename.replace(\".tif\", \"\")\n",
    "\n",
    "def visualize_coverage(gdf, output_path):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of orthophoto coverage.\n",
    "    \n",
    "    Generates multiple views to assess data completeness and quality:\n",
    "    1. Overall coverage map\n",
    "    2. Tile grid with identifiers\n",
    "    3. File size distribution heatmap\n",
    "    \n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Spatial index of orthophotos\n",
    "        output_path (str): Directory for output visualizations\n",
    "        \n",
    "    Returns:\n",
    "        dict: Coverage statistics\n",
    "    \"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Plot 1: Coverage Overview\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "    # Define Swiss national extent\n",
    "    swiss_bounds = box(2485000, 1075000, 2834000, 1299000)\n",
    "    x, y = swiss_bounds.exterior.xy\n",
    "    ax.plot(x, y, \"r--\", label=\"Swiss National Extent\", linewidth=2)\n",
    "\n",
    "    # Plot all tile footprints\n",
    "    gdf.plot(ax=ax, alpha=0.5, edgecolor=\"blue\", facecolor=\"none\")\n",
    "    ax.set_title(\"Orthophoto Coverage Overview\", fontsize=16)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.savefig(output_path / \"01_coverage_overview.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Detailed Tile Grid\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "    # Color-code tiles by latitude for visual organization\n",
    "    y_coords = np.array([geom.bounds[1] for geom in gdf.geometry])\n",
    "    norm = Normalize(vmin=y_coords.min(), vmax=y_coords.max())\n",
    "\n",
    "    # Plot each tile with its identifier\n",
    "    for idx, row in gdf.iterrows():\n",
    "        color = plt.cm.viridis(norm(y_coords[idx]))\n",
    "        x, y = row.geometry.exterior.xy\n",
    "        ax.fill(x, y, alpha=0.5, facecolor=color, edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "        # Add tile number at centroid\n",
    "        centroid = row.geometry.centroid\n",
    "        ax.annotate(\n",
    "            clean_filename(row.file_name),\n",
    "            (centroid.x, centroid.y),\n",
    "            fontsize=3,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "            weight=\"light\",\n",
    "        )\n",
    "\n",
    "    ax.set_title(\"Tile Grid with Identifiers\", fontsize=16)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add colorbar for North-South gradient\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n",
    "    plt.colorbar(sm, ax=ax, label=\"Y-coordinate (North-South)\")\n",
    "\n",
    "    plt.savefig(output_path / \"02_tile_grid.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 3: File Size Distribution\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create scatter plot colored by file size\n",
    "    scatter = ax.scatter(\n",
    "        [geom.centroid.x for geom in gdf.geometry],\n",
    "        [geom.centroid.y for geom in gdf.geometry],\n",
    "        c=gdf[\"size_mb\"],\n",
    "        s=100,\n",
    "        alpha=0.6,\n",
    "        cmap=\"YlOrRd\",\n",
    "    )\n",
    "\n",
    "    plt.colorbar(scatter, ax=ax, label=\"File Size (MB)\")\n",
    "    ax.set_title(\"File Size Distribution Across Coverage Area\", fontsize=16)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.savefig(output_path / \"03_size_distribution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Calculate comprehensive statistics\n",
    "    bounds = gdf.total_bounds\n",
    "    x_coords = sorted(set([int(geom.bounds[0]) for geom in gdf.geometry]))\n",
    "    y_coords = sorted(set([int(geom.bounds[1]) for geom in gdf.geometry]))\n",
    "\n",
    "    # Determine tile dimensions\n",
    "    tile_width = abs(gdf.iloc[0].geometry.bounds[2] - gdf.iloc[0].geometry.bounds[0])\n",
    "    tile_height = abs(gdf.iloc[0].geometry.bounds[3] - gdf.iloc[0].geometry.bounds[1])\n",
    "\n",
    "    stats = {\n",
    "        \"total_area_km2\": gdf.geometry.area.sum() / 1_000_000,\n",
    "        \"tile_count\": len(gdf),\n",
    "        \"x_min\": bounds[0],\n",
    "        \"y_min\": bounds[1],\n",
    "        \"x_max\": bounds[2],\n",
    "        \"y_max\": bounds[3],\n",
    "        \"width_km\": (bounds[2] - bounds[0]) / 1000,\n",
    "        \"height_km\": (bounds[3] - bounds[1]) / 1000,\n",
    "        \"grid_width\": len(x_coords),\n",
    "        \"grid_height\": len(y_coords),\n",
    "        \"tile_width_m\": tile_width,\n",
    "        \"tile_height_m\": tile_height,\n",
    "        \"total_size_gb\": gdf[\"size_mb\"].sum() / 1024,\n",
    "        \"mean_size_mb\": gdf[\"size_mb\"].mean(),\n",
    "        \"min_size_mb\": gdf[\"size_mb\"].min(),\n",
    "        \"max_size_mb\": gdf[\"size_mb\"].max(),\n",
    "    }\n",
    "\n",
    "    # Generate and save comprehensive statistics report\n",
    "    with open(output_path / \"04_coverage_statistics.txt\", \"w\") as f:\n",
    "        print(\"\\n=== COVERAGE STATISTICS ===\", file=f)\n",
    "        print(\"\\nSpatial Coverage:\", file=f)\n",
    "        print(f\"  Total area: {stats['total_area_km2']:.2f} km²\", file=f)\n",
    "        print(f\"  Number of tiles: {stats['tile_count']}\", file=f)\n",
    "        print(\"\\nGeographic Extent:\", file=f)\n",
    "        print(f\"  X range: {stats['x_min']:.2f} to {stats['x_max']:.2f}\", file=f)\n",
    "        print(f\"  Y range: {stats['y_min']:.2f} to {stats['y_max']:.2f}\", file=f)\n",
    "        print(f\"  Width: {stats['width_km']:.2f} km\", file=f)\n",
    "        print(f\"  Height: {stats['height_km']:.2f} km\", file=f)\n",
    "        print(\"\\nGrid Structure:\", file=f)\n",
    "        print(f\"  Grid dimensions: {stats['grid_width']} × {stats['grid_height']} tiles\", file=f)\n",
    "        print(f\"  Tile size: {stats['tile_width_m']:.2f}m × {stats['tile_height_m']:.2f}m\", file=f)\n",
    "        print(\"\\nData Volume:\", file=f)\n",
    "        print(f\"  Total size: {stats['total_size_gb']:.2f} GB\", file=f)\n",
    "        print(f\"  Average file size: {stats['mean_size_mb']:.2f} MB\", file=f)\n",
    "        print(f\"  Size range: {stats['min_size_mb']:.2f} - {stats['max_size_mb']:.2f} MB\", file=f)\n",
    "\n",
    "    # Display statistics in console\n",
    "    with open(output_path / \"04_coverage_statistics.txt\", \"r\") as f:\n",
    "        print(f.read())\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Generate coverage analysis\n",
    "# stats = visualize_coverage(gdf_quadrillage_geotiff, VIZ_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(gdf):\n",
    "    \"\"\"\n",
    "    Comprehensive duplicate detection and data quality analysis.\n",
    "    \n",
    "    Identifies various types of duplicates and inconsistencies:\n",
    "    - File system duplicates\n",
    "    - Coordinate duplicates\n",
    "    - Geometry overlaps\n",
    "    - Metadata inconsistencies\n",
    "    \n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Spatial index to analyze\n",
    "    \"\"\"\n",
    "    print(\"\\n=== COMPREHENSIVE DATA QUALITY ANALYSIS ===\")\n",
    "    print(f\"\\nAnalyzing {len(gdf)} files...\\n\")\n",
    "\n",
    "    print(\"1. FILE SYSTEM INTEGRITY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check for duplicate file paths\n",
    "    print(\"Checking for duplicate file references...\")\n",
    "    file_dups = gdf[gdf['file_path'].duplicated(keep='first')]\n",
    "    if len(file_dups) > 0:\n",
    "        print(f\"WARNING: Found {len(file_dups)} duplicate file paths\")\n",
    "        for idx, row in file_dups.iterrows():\n",
    "            print(f\"  - {row['file_path']}\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate file paths found\")\n",
    "\n",
    "    # Check for duplicate filenames\n",
    "    print(\"\\nChecking for duplicate filenames...\")\n",
    "    name_dups = gdf[gdf['file_name'].duplicated(keep='first')]\n",
    "    if len(name_dups) > 0:\n",
    "        print(f\"WARNING: Found {len(name_dups)} duplicate filenames\")\n",
    "        for idx, row in name_dups.iterrows():\n",
    "            print(f\"  - {row['file_name']} at {row['file_path']}\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate filenames found\")\n",
    "\n",
    "    print(\"\\n2. SPATIAL INTEGRITY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check TFW coordinate duplicates\n",
    "    print(\"Checking for duplicate TFW coordinates...\")\n",
    "    tfw_dups = gdf[gdf.duplicated(\n",
    "        subset=['tfw_x_origin', 'tfw_y_origin'], \n",
    "        keep='first'\n",
    "    )]\n",
    "    if len(tfw_dups) > 0:\n",
    "        print(f\"WARNING: Found {len(tfw_dups)} files with duplicate TFW origins\")\n",
    "        for idx, row in tfw_dups.iterrows():\n",
    "            print(f\"  - {row['file_name']} at ({row['tfw_x_origin']}, {row['tfw_y_origin']})\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate TFW origins found\")\n",
    "\n",
    "    # Check GeoTIFF coordinate duplicates\n",
    "    print(\"\\nChecking for duplicate GeoTIFF coordinates...\")\n",
    "    geotiff_dups = gdf[gdf.duplicated(\n",
    "        subset=['geotiff_x_origin', 'geotiff_y_origin'], \n",
    "        keep='first'\n",
    "    )]\n",
    "    if len(geotiff_dups) > 0:\n",
    "        print(f\"WARNING: Found {len(geotiff_dups)} files with duplicate GeoTIFF origins\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate GeoTIFF origins found\")\n",
    "\n",
    "    print(\"\\n3. GEOMETRY ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check for exact geometry duplicates\n",
    "    print(\"Checking for identical geometries...\")\n",
    "    geom_dups = gdf[gdf.geometry.apply(lambda x: x.wkb).duplicated(keep='first')]\n",
    "    if len(geom_dups) > 0:\n",
    "        print(f\"WARNING: Found {len(geom_dups)} duplicate geometries\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate geometries found\")\n",
    "\n",
    "    # Check for overlapping tiles\n",
    "    print(\"\\nAnalyzing tile overlaps...\")\n",
    "    overlaps = []\n",
    "    \n",
    "    # Use spatial index for efficient overlap detection\n",
    "    for idx1, row1 in gdf.iterrows():\n",
    "        possible_matches_idx = list(gdf.sindex.intersection(row1.geometry.bounds))\n",
    "        possible_matches = gdf.iloc[possible_matches_idx]\n",
    "\n",
    "        for idx2, row2 in possible_matches.iterrows():\n",
    "            if idx1 < idx2:  # Check each pair only once\n",
    "                if row1.geometry.intersects(row2.geometry):\n",
    "                    intersection = row1.geometry.intersection(row2.geometry)\n",
    "                    if intersection.area > 0:\n",
    "                        overlap_pct = (intersection.area / row1.geometry.area) * 100\n",
    "                        if overlap_pct > 1:  # Report significant overlaps only\n",
    "                            overlaps.append({\n",
    "                                'file1': row1['file_name'],\n",
    "                                'file2': row2['file_name'],\n",
    "                                'overlap_area_m2': intersection.area,\n",
    "                                'overlap_percentage': overlap_pct,\n",
    "                                'bounds': intersection.bounds\n",
    "                            })\n",
    "\n",
    "    if overlaps:\n",
    "        print(f\"WARNING: Found {len(overlaps)} overlapping tile pairs (>1% overlap)\")\n",
    "        # Show most significant overlaps\n",
    "        overlaps.sort(key=lambda x: x['overlap_percentage'], reverse=True)\n",
    "        print(\"\\nTop overlaps:\")\n",
    "        for overlap in overlaps[:5]:\n",
    "            print(f\"  - {overlap['file1']} × {overlap['file2']}: {overlap['overlap_percentage']:.2f}%\")\n",
    "        if len(overlaps) > 5:\n",
    "            print(f\"  ... and {len(overlaps) - 5} more overlaps\")\n",
    "    else:\n",
    "        print(\"✓ No significant overlaps found\")\n",
    "\n",
    "    print(\"\\n4. METADATA CONSISTENCY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check TFW vs GeoTIFF consistency\n",
    "    print(\"Checking TFW vs GeoTIFF parameter consistency...\")\n",
    "    has_both = gdf['has_tfw'] & gdf['has_crs']\n",
    "    files_with_both = gdf[has_both]\n",
    "\n",
    "    if len(files_with_both) == 0:\n",
    "        print(\"No files have both TFW and GeoTIFF metadata for comparison\")\n",
    "    else:\n",
    "        # Check for parameter mismatches\n",
    "        inconsistencies = files_with_both[\n",
    "            (files_with_both['tfw_x_scale'] != files_with_both['geotiff_x_scale']) |\n",
    "            (files_with_both['tfw_y_scale'] != files_with_both['geotiff_y_scale']) |\n",
    "            (files_with_both['tfw_x_origin'] != files_with_both['geotiff_x_origin']) |\n",
    "            (files_with_both['tfw_y_origin'] != files_with_both['geotiff_y_origin'])\n",
    "        ]\n",
    "\n",
    "        if len(inconsistencies) > 0:\n",
    "            print(f\"WARNING: Found {len(inconsistencies)} files with parameter inconsistencies\")\n",
    "            # Show first few examples\n",
    "            for idx, row in inconsistencies.head(3).iterrows():\n",
    "                print(f\"\\n  File: {row['file_name']}\")\n",
    "                if row['tfw_x_scale'] != row['geotiff_x_scale']:\n",
    "                    print(f\"    X scale: TFW={row['tfw_x_scale']}, GeoTIFF={row['geotiff_x_scale']}\")\n",
    "                if row['tfw_y_scale'] != row['geotiff_y_scale']:\n",
    "                    print(f\"    Y scale: TFW={row['tfw_y_scale']}, GeoTIFF={row['geotiff_y_scale']}\")\n",
    "        else:\n",
    "            print(\"✓ All TFW and GeoTIFF parameters are consistent\")\n",
    "\n",
    "    print(\"\\n=== ANALYSIS COMPLETE ===\\n\")\n",
    "\n",
    "# Execute comprehensive quality checks\n",
    "# check_duplicates(gdf_quadrillage_geotiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORTHOPHOTO PROCESSING WORKFLOW\n",
      "\n",
      "Skipping URL discovery (UPDATE_URL = False)\n",
      "\n",
      "Checking download status...\n",
      "Saving 3 unique URLs to data/notebook_03/url/00_valid_urls_combined.txt\n",
      "Download status:\n",
      "  Download directory: data/SITG/ortho_2019_zip_2024-11-10\n",
      "  Already downloaded: 3 files\n",
      "  Still needed: 0 files\n",
      "\n",
      "No files to download - all files present\n",
      "\n",
      "Extracting ZIP files...\n",
      "Found 6 ZIP files to process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b305da958faa4dd69d4ccdc243defb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting ZIPs:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 24851109.tif.zip (already extracted)\n",
      "\n",
      "Skipped 25071121.tif.zip (already extracted)\n",
      "\n",
      "Skipped 24851111.tif.zip (already extracted)\n",
      "\n",
      "Skipped 24981110.tif.zip (already extracted)\n",
      "\n",
      "Skipped 24851110.tif.zip (already extracted)\n",
      "\n",
      "Skipped 25041134.tif.zip (already extracted)\n",
      "\n",
      "\n",
      "Creating spatial index...\n",
      "Found 6 GeoTIFF files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca22fad972d49caa718790cdfe57aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing GeoTIFFs:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating geometries...\n",
      "Saved spatial index to:\n",
      "  - data/notebook_03/parquet/03_quadrillage.parquet\n",
      "  - data/notebook_03/parquet/03_quadrillage.gpkg\n",
      "\n",
      "Analyzing data quality...\n",
      "\n",
      "=== METADATA ANALYSIS ===\n",
      "Total files processed: 6\n",
      "Files with TFW: 6\n",
      "Files with embedded CRS: 0\n",
      "\n",
      "CRS Distribution:\n",
      "original_crs\n",
      "EPSG:2056    6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Inferred CRS from coordinates:\n",
      "suggested_crs\n",
      "EPSG:2056    6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TFW Resolution Statistics:\n",
      "X resolution: 0.050 to 0.050\n",
      "Y resolution: -0.050 to -0.050\n",
      "\n",
      "Files with processing notes: 6\n",
      "\n",
      "=== COMPREHENSIVE DATA QUALITY ANALYSIS ===\n",
      "\n",
      "Analyzing 6 files...\n",
      "\n",
      "1. FILE SYSTEM INTEGRITY\n",
      "--------------------------------------------------\n",
      "Checking for duplicate file references...\n",
      "✓ No duplicate file paths found\n",
      "\n",
      "Checking for duplicate filenames...\n",
      "✓ No duplicate filenames found\n",
      "\n",
      "2. SPATIAL INTEGRITY\n",
      "--------------------------------------------------\n",
      "Checking for duplicate TFW coordinates...\n",
      "✓ No duplicate TFW origins found\n",
      "\n",
      "Checking for duplicate GeoTIFF coordinates...\n",
      "✓ No duplicate GeoTIFF origins found\n",
      "\n",
      "3. GEOMETRY ANALYSIS\n",
      "--------------------------------------------------\n",
      "Checking for identical geometries...\n",
      "✓ No duplicate geometries found\n",
      "\n",
      "Analyzing tile overlaps...\n",
      "✓ No significant overlaps found\n",
      "\n",
      "4. METADATA CONSISTENCY\n",
      "--------------------------------------------------\n",
      "Checking TFW vs GeoTIFF parameter consistency...\n",
      "No files have both TFW and GeoTIFF metadata for comparison\n",
      "\n",
      "=== ANALYSIS COMPLETE ===\n",
      "\n",
      "\n",
      "Generating coverage visualizations...\n",
      "\n",
      "=== COVERAGE STATISTICS ===\n",
      "\n",
      "Spatial Coverage:\n",
      "  Total area: 6.00 km²\n",
      "  Number of tiles: 6\n",
      "\n",
      "Geographic Extent:\n",
      "  X range: 2485000.02 to 2508000.02\n",
      "  Y range: 1108999.98 to 1134999.98\n",
      "  Width: 23.00 km\n",
      "  Height: 26.00 km\n",
      "\n",
      "Grid Structure:\n",
      "  Grid dimensions: 4 × 5 tiles\n",
      "  Tile size: 1000.00m × 1000.00m\n",
      "\n",
      "Data Volume:\n",
      "  Total size: 9.40 GB\n",
      "  Average file size: 1603.91 MB\n",
      "  Size range: 1603.91 - 1603.91 MB\n",
      "\n",
      "\n",
      "WORKFLOW COMPLETE\n"
     ]
    }
   ],
   "source": [
    "gdf, stats = await run_complete_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rooftops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
