{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthophoto Download and Processing\n",
    " \n",
    "**Objective:** Download SITG orthophotos, generate tiles and visualizations.\n",
    " \n",
    "**Workflow:**\n",
    "1. URL discovery and validation for orthophoto archives\n",
    "2. Parallel download management\n",
    "3. Extraction and processing\n",
    "4. Tile generation and coverage analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from loguru import logger\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from asyncio import Queue\n",
    "from collections import Counter\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# Suppress non-critical warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_URL = False\n",
    "OUTPUT_URL_CSV_PATH = \"data/notebook_03/url\"\n",
    "OUTPUT_LOGS_PATH = \"data/notebook_03/logs\"\n",
    "\n",
    "GEOTIFF_PATH = \"data/SITG/ortho2019\"\n",
    "GEOTIFF_ZIP_PATH = \"/data/denis.iglesias/TM_rooftops/SITG/ortho_2019_zip_2024-11-10\"\n",
    "\n",
    "OUTPUT_QUADRILLAGE_PARQUET_PATH = \"data/notebook_03/parquet/03_quadrillage.parquet\"\n",
    "VIZ_OUTPUT_PATH = \"data/notebook_03/graphics\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Discovery for GeoTIFF Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging directory\n",
    "log_dir = Path(OUTPUT_LOGS_PATH)\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Configure structured logging\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    log_dir / \"url_checker_{time}.log\",\n",
    "    rotation=\"1 day\",\n",
    "    retention=\"7 days\",\n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\",\n",
    "    level=\"INFO\",\n",
    ")\n",
    "\n",
    "# Add console output for real-time monitoring\n",
    "logger.add(lambda msg: tqdm.write(msg), level=\"INFO\", format=\"{message}\")\n",
    "\n",
    "async def check_url(session, number, sem, queue):\n",
    "    \"\"\"\n",
    "    Check if a URL exists for a given orthophoto tile number.\n",
    "    \n",
    "    Uses HEAD requests for efficiency - only checks if resource exists\n",
    "    without downloading the actual content.\n",
    "    \n",
    "    Parameters:\n",
    "        session: aiohttp session for connection pooling\n",
    "        number: Tile number to check\n",
    "        sem: Semaphore for concurrency control\n",
    "        queue: Queue for collecting valid URLs\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if URL is valid (200 status)\n",
    "    \"\"\"\n",
    "    base_url = \"https://ge.ch/sitg/geodata/SITG/TELECHARGEMENT/ORTHO_2019\"\n",
    "    filename = f\"{number}.tif.zip\"\n",
    "    url = f\"{base_url}/{filename}\"\n",
    "\n",
    "    async with sem:  # Respect concurrency limits\n",
    "        try:\n",
    "            async with session.head(\n",
    "                url, timeout=aiohttp.ClientTimeout(total=3)\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    await queue.put(url)\n",
    "                    return True\n",
    "                return False\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "async def progress_reporter(queue, total_urls):\n",
    "    \"\"\"\n",
    "    Report progress statistics during URL discovery.\n",
    "    \n",
    "    Provides real-time feedback on discovery rate and estimated completion.\n",
    "    \"\"\"\n",
    "    valid_urls = []\n",
    "    start_time = time.time()\n",
    "    counter = Counter()\n",
    "\n",
    "    while True:\n",
    "        url = await queue.get()\n",
    "        if url == \"DONE\":\n",
    "            break\n",
    "\n",
    "        valid_urls.append(url)\n",
    "        counter[\"found\"] += 1\n",
    "\n",
    "        # Report progress at regular intervals\n",
    "        if counter[\"found\"] % 10 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            speed = counter[\"found\"] / elapsed_time if elapsed_time > 0 else 0\n",
    "            logger.info(\n",
    "                f\"Found {counter['found']} URLs | \"\n",
    "                f\"Speed: {speed:.2f} URLs/s | \"\n",
    "                f\"Progress: {(counter['found']/total_urls*100):.2f}%\"\n",
    "            )\n",
    "\n",
    "    return valid_urls\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main async function for URL discovery.\n",
    "    \n",
    "    Scans a range of potential tile numbers to find valid orthophoto URLs.\n",
    "    Uses high concurrency for efficiency while respecting server limits.\n",
    "    \"\"\"\n",
    "    # Define search range based on known tile numbering pattern\n",
    "    start_number = 24_800_000\n",
    "    end_number = 25_100_000\n",
    "    total_urls = end_number - start_number + 1\n",
    "\n",
    "    # Performance tuning parameters\n",
    "    max_concurrent = 1000  # Concurrent connections limit\n",
    "    chunk_size = 1500      # URLs to process per batch\n",
    "\n",
    "    sem = asyncio.Semaphore(max_concurrent)\n",
    "    queue = Queue()\n",
    "    valid_urls = []\n",
    "\n",
    "    # Start progress monitoring\n",
    "    reporter_task = asyncio.create_task(progress_reporter(queue, total_urls))\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Process URLs in chunks to avoid overwhelming memory\n",
    "        for chunk_start in tqdm(range(start_number, end_number + 1, chunk_size)):\n",
    "            chunk_end = min(chunk_start + chunk_size, end_number + 1)\n",
    "            chunk_numbers = range(chunk_start, chunk_end)\n",
    "\n",
    "            tasks = [check_url(session, num, sem, queue) for num in chunk_numbers]\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    # Signal completion and collect results\n",
    "    await queue.put(\"DONE\")\n",
    "    valid_urls = await reporter_task\n",
    "\n",
    "    # Save results with timestamp for tracking\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = f\"{OUTPUT_URL_CSV_PATH}/valid_urls_{timestamp}.txt\"\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for url in valid_urls:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "    logger.info(f\"Discovery complete: {len(valid_urls)} valid URLs found\")\n",
    "    logger.info(f\"Results saved to: {output_file}\")\n",
    "\n",
    "if UPDATE_URL:\n",
    "    # Execute URL discovery\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Downloaded Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_url_lists():\n",
    "    \"\"\"\n",
    "    Combine multiple URL discovery runs and check download status.\n",
    "    \n",
    "    Handles duplicate URLs across multiple discovery sessions and\n",
    "    identifies which files still need to be downloaded.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "\n",
    "    # Gather all URL lists except combined ones\n",
    "    txt_files = glob.glob(f\"{OUTPUT_URL_CSV_PATH}/*.txt\")\n",
    "    txt_files = [f for f in txt_files if \"combined\" not in f]\n",
    "\n",
    "    # Consolidate unique URLs\n",
    "    valid_urls = set()\n",
    "    for txt_file in txt_files:\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            urls = f.readlines()\n",
    "            valid_urls.update(urls)\n",
    "\n",
    "    # Save consolidated list\n",
    "    output_file = f\"{OUTPUT_URL_CSV_PATH}/00_valid_urls_combined.txt\"\n",
    "    print(f\"Saving {len(valid_urls)} unique URLs to {output_file}\")\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.writelines(valid_urls)\n",
    "\n",
    "    return valid_urls\n",
    "\n",
    "def check_download_status(valid_urls):\n",
    "    \"\"\"\n",
    "    Compare discovered URLs against downloaded files.\n",
    "    \n",
    "    Identifies which files have been successfully downloaded and\n",
    "    which are still pending.\n",
    "    \"\"\"\n",
    "    # Extract filenames from URLs\n",
    "    valid_filenames = [url.split(\"/\")[-1].strip() for url in valid_urls]\n",
    "    filename_to_url = {url.split(\"/\")[-1].strip(): url.strip() for url in valid_urls}\n",
    "\n",
    "    # Check existing downloads\n",
    "    zip_files = os.listdir(GEOTIFF_ZIP_PATH)\n",
    "    downloaded_files = set(valid_filenames).intersection(zip_files)\n",
    "\n",
    "    print(f\"Download status:\")\n",
    "    print(f\"  Already downloaded: {len(downloaded_files)} files\")\n",
    "    \n",
    "    # Identify missing files\n",
    "    missing_files = set(valid_filenames) - downloaded_files\n",
    "    print(f\"  Still needed: {len(missing_files)} files\")\n",
    "\n",
    "    if missing_files:\n",
    "        print(\"\\nMissing files URLs:\")\n",
    "        for filename in missing_files:\n",
    "            print(filename_to_url[filename])\n",
    "\n",
    "    return downloaded_files, missing_files\n",
    "\n",
    "# Execute consolidation and status check\n",
    "valid_urls = consolidate_url_lists()\n",
    "downloaded, missing = check_download_status(valid_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Orthophoto Tile Index\n",
    "### Tile Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfw(tif_path):\n",
    "    \"\"\"\n",
    "    Read TFW (World File) associated with a GeoTIFF.\n",
    "    \n",
    "    TFW files contain georeferencing information in a simple text format:\n",
    "    - Line 1: Pixel X size\n",
    "    - Line 2: Rotation about Y axis\n",
    "    - Line 3: Rotation about X axis\n",
    "    - Line 4: Pixel Y size (usually negative)\n",
    "    - Line 5: X coordinate of upper left pixel center\n",
    "    - Line 6: Y coordinate of upper left pixel center\n",
    "    \n",
    "    Parameters:\n",
    "        tif_path (Path): Path to the GeoTIFF file\n",
    "        \n",
    "    Returns:\n",
    "        dict: TFW parameters if found, None otherwise\n",
    "    \"\"\"\n",
    "    tif_path = Path(tif_path)\n",
    "    \n",
    "    # Check common TFW file extensions\n",
    "    tfw_paths = [\n",
    "        tif_path.with_suffix(\".tfw\"),\n",
    "        tif_path.with_suffix(\".tifw\"),\n",
    "        Path(str(tif_path.with_suffix(\"\")) + \".tfw\"),\n",
    "    ]\n",
    "\n",
    "    for tfw_path in tfw_paths:\n",
    "        if tfw_path.exists():\n",
    "            try:\n",
    "                with open(tfw_path, \"r\") as f:\n",
    "                    lines = [float(line.strip()) for line in f.readlines()]\n",
    "\n",
    "                if len(lines) != 6:\n",
    "                    print(\n",
    "                        f\"Warning: Invalid TFW format in {tfw_path} \"\n",
    "                        f\"(expected 6 lines, found {len(lines)})\"\n",
    "                    )\n",
    "                    return None\n",
    "\n",
    "                return {\n",
    "                    \"x_scale\": lines[0],\n",
    "                    \"y_rotation\": lines[1],\n",
    "                    \"x_rotation\": lines[2],\n",
    "                    \"y_scale\": lines[3],\n",
    "                    \"x_origin\": lines[4],\n",
    "                    \"y_origin\": lines[5],\n",
    "                    \"tfw_path\": str(tfw_path),\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error reading TFW file {tfw_path}: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def process_single_geotiff(tif_path):\n",
    "    \"\"\"\n",
    "    Extract comprehensive metadata from a single GeoTIFF file.\n",
    "    \n",
    "    Processes both embedded GeoTIFF metadata and external TFW files,\n",
    "    creating a unified spatial footprint for the image.\n",
    "    \n",
    "    Parameters:\n",
    "        tif_path (Path): Path to GeoTIFF file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Metadata including geometry, CRS, and file properties\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tif_path = Path(tif_path)\n",
    "        result = {\n",
    "            \"file_path\": str(tif_path),\n",
    "            \"file_name\": tif_path.name,\n",
    "            \"size_mb\": os.path.getsize(tif_path) / (1024 * 1024),\n",
    "            \"errors\": [],\n",
    "        }\n",
    "\n",
    "        # Prioritize TFW data as it's often more reliable for Swiss data\n",
    "        tfw_data = read_tfw(tif_path)\n",
    "        if tfw_data:\n",
    "            result.update(\n",
    "                {\n",
    "                    \"has_tfw\": True,\n",
    "                    \"tfw_path\": tfw_data[\"tfw_path\"],\n",
    "                    \"tfw_x_scale\": tfw_data[\"x_scale\"],\n",
    "                    \"tfw_y_scale\": tfw_data[\"y_scale\"],\n",
    "                    \"tfw_x_rotation\": tfw_data[\"x_rotation\"],\n",
    "                    \"tfw_y_rotation\": tfw_data[\"y_rotation\"],\n",
    "                    \"tfw_x_origin\": tfw_data[\"x_origin\"],\n",
    "                    \"tfw_y_origin\": tfw_data[\"y_origin\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Infer CRS from coordinate ranges (Swiss specific)\n",
    "            if (\n",
    "                2485000 <= tfw_data[\"x_origin\"] <= 2834000\n",
    "                and 1075000 <= tfw_data[\"y_origin\"] <= 1299000\n",
    "            ):\n",
    "                result[\"suggested_crs\"] = \"EPSG:2056\"  # CH1903+/LV95\n",
    "            elif (\n",
    "                485000 <= tfw_data[\"x_origin\"] <= 834000\n",
    "                and 75000 <= tfw_data[\"y_origin\"] <= 299000\n",
    "            ):\n",
    "                result[\"suggested_crs\"] = \"EPSG:21781\"  # CH1903/LV03\n",
    "            else:\n",
    "                result[\"suggested_crs\"] = None\n",
    "        else:\n",
    "            result[\"has_tfw\"] = False\n",
    "            result[\"suggested_crs\"] = None\n",
    "\n",
    "        # Read embedded GeoTIFF metadata\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            # Basic raster properties\n",
    "            result.update(\n",
    "                {\n",
    "                    \"width\": src.width,\n",
    "                    \"height\": src.height,\n",
    "                    \"band_count\": src.count,\n",
    "                    \"data_type\": str(src.profile.get(\"dtype\")),\n",
    "                    \"resolution_x\": src.res[0] if src.res else None,\n",
    "                    \"resolution_y\": src.res[1] if src.res else None,\n",
    "                    \"has_crs\": src.crs is not None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Handle CRS detection\n",
    "            if src.crs:\n",
    "                result[\"original_crs\"] = src.crs.to_string()\n",
    "            else:\n",
    "                result[\"original_crs\"] = result.get(\"suggested_crs\")\n",
    "                if result[\"original_crs\"]:\n",
    "                    result[\"errors\"].append(\n",
    "                        f\"No CRS in GeoTIFF, using suggested CRS from TFW: {result['original_crs']}\"\n",
    "                    )\n",
    "                else:\n",
    "                    result[\"errors\"].append(\n",
    "                        \"No CRS found and cannot determine from coordinates\"\n",
    "                    )\n",
    "\n",
    "            # Extract transform information\n",
    "            transform = src.transform\n",
    "            if transform and transform.is_identity is False:\n",
    "                result.update(\n",
    "                    {\n",
    "                        \"geotiff_x_scale\": transform.a,\n",
    "                        \"geotiff_y_scale\": transform.e,\n",
    "                        \"geotiff_x_rotation\": transform.b,\n",
    "                        \"geotiff_y_rotation\": transform.d,\n",
    "                        \"geotiff_x_origin\": transform.c,\n",
    "                        \"geotiff_y_origin\": transform.f,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Create spatial footprint - prefer TFW if available\n",
    "            if result.get(\"has_tfw\"):\n",
    "                # Calculate bounds from TFW parameters\n",
    "                left = result[\"tfw_x_origin\"]\n",
    "                top = result[\"tfw_y_origin\"]\n",
    "                right = left + (src.width * result[\"tfw_x_scale\"])\n",
    "                bottom = top + (src.height * result[\"tfw_y_scale\"])\n",
    "                footprint = box(left, bottom, right, top)\n",
    "                crs = result.get(\"suggested_crs\", \"EPSG:2056\")\n",
    "            else:\n",
    "                # Use GeoTIFF bounds\n",
    "                bounds = src.bounds\n",
    "                footprint = box(bounds.left, bounds.bottom, bounds.right, bounds.top)\n",
    "                crs = src.crs or \"EPSG:2056\"\n",
    "\n",
    "            # Create temporary GeoDataFrame for CRS transformation\n",
    "            temp_gdf = gpd.GeoDataFrame(geometry=[footprint], crs=crs)\n",
    "\n",
    "            # Ensure consistent CRS (Swiss LV95)\n",
    "            if temp_gdf.crs != \"EPSG:2056\":\n",
    "                try:\n",
    "                    temp_gdf = temp_gdf.to_crs(\"EPSG:2056\")\n",
    "                except Exception as e:\n",
    "                    result[\"errors\"].append(\n",
    "                        f\"CRS transformation failed: {str(e)}\"\n",
    "                    )\n",
    "                    return None\n",
    "\n",
    "            # Validate geometry\n",
    "            if not temp_gdf.geometry[0].is_valid:\n",
    "                temp_gdf.geometry = temp_gdf.geometry.buffer(0)\n",
    "                result[\"errors\"].append(\"Applied geometry correction\")\n",
    "\n",
    "            if np.any(np.isinf(temp_gdf.geometry[0].bounds)):\n",
    "                result[\"errors\"].append(\"Invalid bounds detected\")\n",
    "                return None\n",
    "\n",
    "            result[\"geometry\"] = temp_gdf.geometry[0]\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {tif_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_geotiff_index(input_folder, output_parquet, recursive=True):\n",
    "    \"\"\"\n",
    "    Create a comprehensive spatial index of all GeoTIFF files.\n",
    "    \n",
    "    Generates a GeoParquet file containing footprints and metadata\n",
    "    for efficient spatial queries and coverage analysis.\n",
    "    \n",
    "    Parameters:\n",
    "        input_folder (str): Directory containing GeoTIFF files\n",
    "        output_parquet (str): Output path for GeoParquet file\n",
    "        recursive (bool): Search subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame: Spatial index of all GeoTIFF files\n",
    "    \"\"\"\n",
    "    input_path = Path(input_folder)\n",
    "    if not input_path.exists():\n",
    "        raise ValueError(f\"Input folder does not exist: {input_folder}\")\n",
    "\n",
    "    # Locate all GeoTIFF files\n",
    "    if recursive:\n",
    "        tif_files = list(input_path.rglob(\"*.tif\")) + list(input_path.rglob(\"*.tiff\"))\n",
    "    else:\n",
    "        tif_files = list(input_path.glob(\"*.tif\")) + list(input_path.glob(\"*.tiff\"))\n",
    "\n",
    "    if not tif_files:\n",
    "        raise ValueError(f\"No GeoTIFF files found in {input_folder}\")\n",
    "\n",
    "    print(f\"Found {len(tif_files)} GeoTIFF files\")\n",
    "\n",
    "    # Process files with progress tracking\n",
    "    results = []\n",
    "    for tif_path in tqdm(tif_files, desc=\"Processing GeoTIFFs\"):\n",
    "        try:\n",
    "            result = process_single_geotiff(tif_path)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {tif_path}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if not results:\n",
    "        raise ValueError(\"No GeoTIFF files could be processed successfully\")\n",
    "\n",
    "    # Create spatial dataframe\n",
    "    gdf = gpd.GeoDataFrame(results, crs=\"EPSG:2056\")\n",
    "\n",
    "    # Add relative paths for portability\n",
    "    gdf[\"relative_path\"] = gdf[\"file_path\"].apply(\n",
    "        lambda x: str(Path(x).relative_to(input_path))\n",
    "    )\n",
    "\n",
    "    # Ensure geometry integrity\n",
    "    gdf.set_geometry('geometry', inplace=True)\n",
    "    \n",
    "    if gdf.crs is None:\n",
    "        gdf.set_crs(epsg=2056, inplace=True)\n",
    "\n",
    "    print(\"Validating geometries...\")\n",
    "    gdf['geometry'] = gdf['geometry'].buffer(0)\n",
    "\n",
    "    # Convert error lists to strings for Parquet compatibility\n",
    "    gdf['errors'] = gdf['errors'].apply(lambda x: str(x))\n",
    "\n",
    "    # Save in multiple formats for flexibility\n",
    "    gdf.to_parquet(output_parquet)\n",
    "    gdf.to_file(output_parquet.replace(\".parquet\", \".gpkg\"), driver=\"GPKG\")\n",
    "    print(\"Saved spatial index to:\")\n",
    "    print(f\"  - {output_parquet}\")\n",
    "    print(f\"  - {output_parquet.replace('.parquet', '.gpkg')}\")\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def analyze_tfw_geotiff(gdf):\n",
    "    \"\"\"\n",
    "    Analyze consistency between TFW and GeoTIFF metadata.\n",
    "    \n",
    "    Identifies discrepancies that might indicate data quality issues.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== METADATA ANALYSIS ===\")\n",
    "    print(f\"Total files processed: {len(gdf)}\")\n",
    "    print(f\"Files with TFW: {gdf['has_tfw'].sum()}\")\n",
    "    print(f\"Files with embedded CRS: {gdf['has_crs'].sum()}\")\n",
    "\n",
    "    # CRS distribution\n",
    "    print(\"\\nCRS Distribution:\")\n",
    "    print(gdf[\"original_crs\"].value_counts())\n",
    "\n",
    "    # Suggested CRS from coordinates\n",
    "    print(\"\\nInferred CRS from coordinates:\")\n",
    "    print(gdf[\"suggested_crs\"].value_counts())\n",
    "\n",
    "    # Resolution analysis for TFW files\n",
    "    tfw_files = gdf[gdf[\"has_tfw\"]]\n",
    "    if not tfw_files.empty:\n",
    "        print(\"\\nTFW Resolution Statistics:\")\n",
    "        print(f\"X resolution: {tfw_files['tfw_x_scale'].min():.3f} to {tfw_files['tfw_x_scale'].max():.3f}\")\n",
    "        print(f\"Y resolution: {tfw_files['tfw_y_scale'].min():.3f} to {tfw_files['tfw_y_scale'].max():.3f}\")\n",
    "\n",
    "        # Check for rotated images\n",
    "        rotated = tfw_files[\n",
    "            (tfw_files[\"tfw_x_rotation\"] != 0) | (tfw_files[\"tfw_y_rotation\"] != 0)\n",
    "        ]\n",
    "        if not rotated.empty:\n",
    "            print(f\"\\nFound {len(rotated)} rotated images\")\n",
    "\n",
    "    # Compare TFW vs GeoTIFF parameters\n",
    "    mask = gdf[\"has_tfw\"] & gdf[\"has_crs\"]\n",
    "    if mask.any():\n",
    "        comparison = gdf[mask].copy()\n",
    "        print(\"\\nTFW vs GeoTIFF Parameter Comparison:\")\n",
    "        \n",
    "        matches = 0\n",
    "        total = len(comparison)\n",
    "\n",
    "        for _, row in comparison.iterrows():\n",
    "            if all([\n",
    "                np.isclose(row[\"tfw_x_scale\"], row[\"geotiff_x_scale\"]),\n",
    "                np.isclose(row[\"tfw_y_scale\"], row[\"geotiff_y_scale\"]),\n",
    "                np.isclose(row[\"tfw_x_origin\"], row[\"geotiff_x_origin\"]),\n",
    "                np.isclose(row[\"tfw_y_origin\"], row[\"geotiff_y_origin\"]),\n",
    "            ]):\n",
    "                matches += 1\n",
    "\n",
    "        print(f\"Files with matching parameters: {matches}/{total}\")\n",
    "\n",
    "    # Report files with errors\n",
    "    error_mask = gdf[\"errors\"].apply(lambda x: len(x) > 0)\n",
    "    if error_mask.any():\n",
    "        print(f\"\\nFiles with processing notes: {error_mask.sum()}\")\n",
    "\n",
    "# Create the spatial index\n",
    "gdf_quadrillage_geotiff = create_geotiff_index(GEOTIFF_PATH, OUTPUT_QUADRILLAGE_PARQUET_PATH)\n",
    "\n",
    "# Analyze results\n",
    "analyze_tfw_geotiff(gdf_quadrillage_geotiff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract tile number from filename.\n",
    "    Example: '24851110.tif' -> '24851110'\n",
    "    \"\"\"\n",
    "    return filename.replace(\".tif\", \"\")\n",
    "\n",
    "def visualize_coverage(gdf, output_path):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of orthophoto coverage.\n",
    "    \n",
    "    Generates multiple views to assess data completeness and quality:\n",
    "    1. Overall coverage map\n",
    "    2. Tile grid with identifiers\n",
    "    3. File size distribution heatmap\n",
    "    \n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Spatial index of orthophotos\n",
    "        output_path (str): Directory for output visualizations\n",
    "        \n",
    "    Returns:\n",
    "        dict: Coverage statistics\n",
    "    \"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Plot 1: Coverage Overview\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "    # Define Swiss national extent\n",
    "    swiss_bounds = box(2485000, 1075000, 2834000, 1299000)\n",
    "    x, y = swiss_bounds.exterior.xy\n",
    "    ax.plot(x, y, \"r--\", label=\"Swiss National Extent\", linewidth=2)\n",
    "\n",
    "    # Plot all tile footprints\n",
    "    gdf.plot(ax=ax, alpha=0.5, edgecolor=\"blue\", facecolor=\"none\")\n",
    "    ax.set_title(\"Orthophoto Coverage Overview\", fontsize=16)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.savefig(output_path / \"01_coverage_overview.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Detailed Tile Grid\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "    # Color-code tiles by latitude for visual organization\n",
    "    y_coords = np.array([geom.bounds[1] for geom in gdf.geometry])\n",
    "    norm = Normalize(vmin=y_coords.min(), vmax=y_coords.max())\n",
    "\n",
    "    # Plot each tile with its identifier\n",
    "    for idx, row in gdf.iterrows():\n",
    "        color = plt.cm.viridis(norm(y_coords[idx]))\n",
    "        x, y = row.geometry.exterior.xy\n",
    "        ax.fill(x, y, alpha=0.5, facecolor=color, edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "        # Add tile number at centroid\n",
    "        centroid = row.geometry.centroid\n",
    "        ax.annotate(\n",
    "            clean_filename(row.file_name),\n",
    "            (centroid.x, centroid.y),\n",
    "            fontsize=3,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "            weight=\"light\",\n",
    "        )\n",
    "\n",
    "    ax.set_title(\"Tile Grid with Identifiers\", fontsize=16)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add colorbar for North-South gradient\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n",
    "    plt.colorbar(sm, ax=ax, label=\"Y-coordinate (North-South)\")\n",
    "\n",
    "    plt.savefig(output_path / \"02_tile_grid.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 3: File Size Distribution\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create scatter plot colored by file size\n",
    "    scatter = ax.scatter(\n",
    "        [geom.centroid.x for geom in gdf.geometry],\n",
    "        [geom.centroid.y for geom in gdf.geometry],\n",
    "        c=gdf[\"size_mb\"],\n",
    "        s=100,\n",
    "        alpha=0.6,\n",
    "        cmap=\"YlOrRd\",\n",
    "    )\n",
    "\n",
    "    plt.colorbar(scatter, ax=ax, label=\"File Size (MB)\")\n",
    "    ax.set_title(\"File Size Distribution Across Coverage Area\", fontsize=16)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.savefig(output_path / \"03_size_distribution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Calculate comprehensive statistics\n",
    "    bounds = gdf.total_bounds\n",
    "    x_coords = sorted(set([int(geom.bounds[0]) for geom in gdf.geometry]))\n",
    "    y_coords = sorted(set([int(geom.bounds[1]) for geom in gdf.geometry]))\n",
    "\n",
    "    # Determine tile dimensions\n",
    "    tile_width = abs(gdf.iloc[0].geometry.bounds[2] - gdf.iloc[0].geometry.bounds[0])\n",
    "    tile_height = abs(gdf.iloc[0].geometry.bounds[3] - gdf.iloc[0].geometry.bounds[1])\n",
    "\n",
    "    stats = {\n",
    "        \"total_area_km2\": gdf.geometry.area.sum() / 1_000_000,\n",
    "        \"tile_count\": len(gdf),\n",
    "        \"x_min\": bounds[0],\n",
    "        \"y_min\": bounds[1],\n",
    "        \"x_max\": bounds[2],\n",
    "        \"y_max\": bounds[3],\n",
    "        \"width_km\": (bounds[2] - bounds[0]) / 1000,\n",
    "        \"height_km\": (bounds[3] - bounds[1]) / 1000,\n",
    "        \"grid_width\": len(x_coords),\n",
    "        \"grid_height\": len(y_coords),\n",
    "        \"tile_width_m\": tile_width,\n",
    "        \"tile_height_m\": tile_height,\n",
    "        \"total_size_gb\": gdf[\"size_mb\"].sum() / 1024,\n",
    "        \"mean_size_mb\": gdf[\"size_mb\"].mean(),\n",
    "        \"min_size_mb\": gdf[\"size_mb\"].min(),\n",
    "        \"max_size_mb\": gdf[\"size_mb\"].max(),\n",
    "    }\n",
    "\n",
    "    # Generate and save comprehensive statistics report\n",
    "    with open(output_path / \"04_coverage_statistics.txt\", \"w\") as f:\n",
    "        print(\"\\n=== COVERAGE STATISTICS ===\", file=f)\n",
    "        print(\"\\nSpatial Coverage:\", file=f)\n",
    "        print(f\"  Total area: {stats['total_area_km2']:.2f} km²\", file=f)\n",
    "        print(f\"  Number of tiles: {stats['tile_count']}\", file=f)\n",
    "        print(\"\\nGeographic Extent:\", file=f)\n",
    "        print(f\"  X range: {stats['x_min']:.2f} to {stats['x_max']:.2f}\", file=f)\n",
    "        print(f\"  Y range: {stats['y_min']:.2f} to {stats['y_max']:.2f}\", file=f)\n",
    "        print(f\"  Width: {stats['width_km']:.2f} km\", file=f)\n",
    "        print(f\"  Height: {stats['height_km']:.2f} km\", file=f)\n",
    "        print(\"\\nGrid Structure:\", file=f)\n",
    "        print(f\"  Grid dimensions: {stats['grid_width']} × {stats['grid_height']} tiles\", file=f)\n",
    "        print(f\"  Tile size: {stats['tile_width_m']:.2f}m × {stats['tile_height_m']:.2f}m\", file=f)\n",
    "        print(\"\\nData Volume:\", file=f)\n",
    "        print(f\"  Total size: {stats['total_size_gb']:.2f} GB\", file=f)\n",
    "        print(f\"  Average file size: {stats['mean_size_mb']:.2f} MB\", file=f)\n",
    "        print(f\"  Size range: {stats['min_size_mb']:.2f} - {stats['max_size_mb']:.2f} MB\", file=f)\n",
    "\n",
    "    # Display statistics in console\n",
    "    with open(output_path / \"04_coverage_statistics.txt\", \"r\") as f:\n",
    "        print(f.read())\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Generate coverage analysis\n",
    "stats = visualize_coverage(gdf_quadrillage_geotiff, VIZ_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(gdf):\n",
    "    \"\"\"\n",
    "    Comprehensive duplicate detection and data quality analysis.\n",
    "    \n",
    "    Identifies various types of duplicates and inconsistencies:\n",
    "    - File system duplicates\n",
    "    - Coordinate duplicates\n",
    "    - Geometry overlaps\n",
    "    - Metadata inconsistencies\n",
    "    \n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Spatial index to analyze\n",
    "    \"\"\"\n",
    "    print(\"\\n=== COMPREHENSIVE DATA QUALITY ANALYSIS ===\")\n",
    "    print(f\"\\nAnalyzing {len(gdf)} files...\\n\")\n",
    "\n",
    "    print(\"1. FILE SYSTEM INTEGRITY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check for duplicate file paths\n",
    "    print(\"Checking for duplicate file references...\")\n",
    "    file_dups = gdf[gdf['file_path'].duplicated(keep='first')]\n",
    "    if len(file_dups) > 0:\n",
    "        print(f\"WARNING: Found {len(file_dups)} duplicate file paths\")\n",
    "        for idx, row in file_dups.iterrows():\n",
    "            print(f\"  - {row['file_path']}\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate file paths found\")\n",
    "\n",
    "    # Check for duplicate filenames\n",
    "    print(\"\\nChecking for duplicate filenames...\")\n",
    "    name_dups = gdf[gdf['file_name'].duplicated(keep='first')]\n",
    "    if len(name_dups) > 0:\n",
    "        print(f\"WARNING: Found {len(name_dups)} duplicate filenames\")\n",
    "        for idx, row in name_dups.iterrows():\n",
    "            print(f\"  - {row['file_name']} at {row['file_path']}\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate filenames found\")\n",
    "\n",
    "    print(\"\\n2. SPATIAL INTEGRITY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check TFW coordinate duplicates\n",
    "    print(\"Checking for duplicate TFW coordinates...\")\n",
    "    tfw_dups = gdf[gdf.duplicated(\n",
    "        subset=['tfw_x_origin', 'tfw_y_origin'], \n",
    "        keep='first'\n",
    "    )]\n",
    "    if len(tfw_dups) > 0:\n",
    "        print(f\"WARNING: Found {len(tfw_dups)} files with duplicate TFW origins\")\n",
    "        for idx, row in tfw_dups.iterrows():\n",
    "            print(f\"  - {row['file_name']} at ({row['tfw_x_origin']}, {row['tfw_y_origin']})\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate TFW origins found\")\n",
    "\n",
    "    # Check GeoTIFF coordinate duplicates\n",
    "    print(\"\\nChecking for duplicate GeoTIFF coordinates...\")\n",
    "    geotiff_dups = gdf[gdf.duplicated(\n",
    "        subset=['geotiff_x_origin', 'geotiff_y_origin'], \n",
    "        keep='first'\n",
    "    )]\n",
    "    if len(geotiff_dups) > 0:\n",
    "        print(f\"WARNING: Found {len(geotiff_dups)} files with duplicate GeoTIFF origins\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate GeoTIFF origins found\")\n",
    "\n",
    "    print(\"\\n3. GEOMETRY ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check for exact geometry duplicates\n",
    "    print(\"Checking for identical geometries...\")\n",
    "    geom_dups = gdf[gdf.geometry.apply(lambda x: x.wkb).duplicated(keep='first')]\n",
    "    if len(geom_dups) > 0:\n",
    "        print(f\"WARNING: Found {len(geom_dups)} duplicate geometries\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate geometries found\")\n",
    "\n",
    "    # Check for overlapping tiles\n",
    "    print(\"\\nAnalyzing tile overlaps...\")\n",
    "    overlaps = []\n",
    "    \n",
    "    # Use spatial index for efficient overlap detection\n",
    "    for idx1, row1 in gdf.iterrows():\n",
    "        possible_matches_idx = list(gdf.sindex.intersection(row1.geometry.bounds))\n",
    "        possible_matches = gdf.iloc[possible_matches_idx]\n",
    "\n",
    "        for idx2, row2 in possible_matches.iterrows():\n",
    "            if idx1 < idx2:  # Check each pair only once\n",
    "                if row1.geometry.intersects(row2.geometry):\n",
    "                    intersection = row1.geometry.intersection(row2.geometry)\n",
    "                    if intersection.area > 0:\n",
    "                        overlap_pct = (intersection.area / row1.geometry.area) * 100\n",
    "                        if overlap_pct > 1:  # Report significant overlaps only\n",
    "                            overlaps.append({\n",
    "                                'file1': row1['file_name'],\n",
    "                                'file2': row2['file_name'],\n",
    "                                'overlap_area_m2': intersection.area,\n",
    "                                'overlap_percentage': overlap_pct,\n",
    "                                'bounds': intersection.bounds\n",
    "                            })\n",
    "\n",
    "    if overlaps:\n",
    "        print(f\"WARNING: Found {len(overlaps)} overlapping tile pairs (>1% overlap)\")\n",
    "        # Show most significant overlaps\n",
    "        overlaps.sort(key=lambda x: x['overlap_percentage'], reverse=True)\n",
    "        print(\"\\nTop overlaps:\")\n",
    "        for overlap in overlaps[:5]:\n",
    "            print(f\"  - {overlap['file1']} × {overlap['file2']}: {overlap['overlap_percentage']:.2f}%\")\n",
    "        if len(overlaps) > 5:\n",
    "            print(f\"  ... and {len(overlaps) - 5} more overlaps\")\n",
    "    else:\n",
    "        print(\"✓ No significant overlaps found\")\n",
    "\n",
    "    print(\"\\n4. METADATA CONSISTENCY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check TFW vs GeoTIFF consistency\n",
    "    print(\"Checking TFW vs GeoTIFF parameter consistency...\")\n",
    "    has_both = gdf['has_tfw'] & gdf['has_crs']\n",
    "    files_with_both = gdf[has_both]\n",
    "\n",
    "    if len(files_with_both) == 0:\n",
    "        print(\"No files have both TFW and GeoTIFF metadata for comparison\")\n",
    "    else:\n",
    "        # Check for parameter mismatches\n",
    "        inconsistencies = files_with_both[\n",
    "            (files_with_both['tfw_x_scale'] != files_with_both['geotiff_x_scale']) |\n",
    "            (files_with_both['tfw_y_scale'] != files_with_both['geotiff_y_scale']) |\n",
    "            (files_with_both['tfw_x_origin'] != files_with_both['geotiff_x_origin']) |\n",
    "            (files_with_both['tfw_y_origin'] != files_with_both['geotiff_y_origin'])\n",
    "        ]\n",
    "\n",
    "        if len(inconsistencies) > 0:\n",
    "            print(f\"WARNING: Found {len(inconsistencies)} files with parameter inconsistencies\")\n",
    "            # Show first few examples\n",
    "            for idx, row in inconsistencies.head(3).iterrows():\n",
    "                print(f\"\\n  File: {row['file_name']}\")\n",
    "                if row['tfw_x_scale'] != row['geotiff_x_scale']:\n",
    "                    print(f\"    X scale: TFW={row['tfw_x_scale']}, GeoTIFF={row['geotiff_x_scale']}\")\n",
    "                if row['tfw_y_scale'] != row['geotiff_y_scale']:\n",
    "                    print(f\"    Y scale: TFW={row['tfw_y_scale']}, GeoTIFF={row['geotiff_y_scale']}\")\n",
    "        else:\n",
    "            print(\"✓ All TFW and GeoTIFF parameters are consistent\")\n",
    "\n",
    "    print(\"\\n=== ANALYSIS COMPLETE ===\\n\")\n",
    "\n",
    "# Execute comprehensive quality checks\n",
    "check_duplicates(gdf_quadrillage_geotiff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
