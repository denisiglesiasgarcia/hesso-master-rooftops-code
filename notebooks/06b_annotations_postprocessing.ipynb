{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import tempfile\n",
    "import traceback\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, mapping\n",
    "\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.windows import from_bounds, Window\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "DATASET_ANNOTATED_BRUT_PATH = \"datasets/supervisely/341575_free_space_rooftop_geneva_20250511_binary_mask\"\n",
    "DATASET_TILES_INFORMATION_CSV_PATH = \"data/notebook_06/dataset_20250405-193125/PNG_dataset_roboflow_20250405-193143/sampled_tiles.csv\"\n",
    "CORRECT_CRS = CRS.from_epsg(2056)\n",
    "EPSG_SUISSE = \"EPSG:2056\"\n",
    "\n",
    "CAD_BATIMENT_HORSOL_TOIT_MERGE_PARQUET_PATH = \"data/notebook_04/parquet/04_02_merged_rooftops_poly.parquet\"\n",
    "\n",
    "# Parquet files\n",
    "VERIFICATION_OUTPUT_PARQUET_PATH = \"data/notebook_06/parquet/06b_01_verification.parquet\"\n",
    "DATASET_OUTPUT_PARQUET_PATH = \"data/notebook_06/parquet/06b_02_dataset_processed.parquet\"\n",
    "DATASET_FINAL_OUTPUT_PARQUET_PATH = \"data/notebook_06/parquet/06b_03_dataset_final.parquet\"\n",
    "\n",
    "# dataset processed\n",
    "DATASET_PROCESSED_NAME = \"dataset_processed_\" + str(todays_date)\n",
    "DATASET_PROCESSED_PATH = \"datasets/supervisely/\" + DATASET_PROCESSED_NAME\n",
    "DATASET_OUTPUT_IMG_PATH = DATASET_PROCESSED_PATH + \"/images\"\n",
    "DATASET_OUTPUT_MASKS_PATH = DATASET_PROCESSED_PATH + \"/masks\"\n",
    "DATASET_OUTPUT_CHECKS_PATH = DATASET_PROCESSED_PATH + \"/check_dataset\"\n",
    "\n",
    "# buffer pour les chevauchements\n",
    "BUFFER_DISTANCE = 0 # en mètre\n",
    "OVERLAP_POSITIONS=['top', 'right', 'top-left', 'top-right']\n",
    "\n",
    "os.makedirs(DATASET_PROCESSED_PATH)\n",
    "os.makedirs(DATASET_OUTPUT_IMG_PATH)\n",
    "os.makedirs(DATASET_OUTPUT_MASKS_PATH)\n",
    "os.makedirs(DATASET_OUTPUT_CHECKS_PATH)\n",
    "\n",
    "#! Régénérer les tuiles dans tile_1024_split depuis les geotiff de 1.4Gb de SITG. Environ 1h\n",
    "#! Utiles si jamais les tuiles de 1024 sont corrompues ou effacées par erreur\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG = False\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_PROCESSES = 2\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_THREADS = 2\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG_COMBINED_METADATA_PARQUET = \"data/notebook_04/geotiff/tile_1024_split_old_20250519-120028/combined_metadata.parquet\"\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TILE_1024 = \"data/notebook_04/geotiff/tile_1024_split\"\n",
    "REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TEMP_1280 = 'data/notebook_04/geotiff/tile_1280_split'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régénérer tile_1024_split si problème\n",
    "\n",
    "optionnel, c'est dans le cas ou le tile_1024_split a été modifié par erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single tile\n",
    "def process_tile(row, output_dir, debug_dir):\n",
    "    try:\n",
    "        bounds_str = row['buffered_bounds']\n",
    "        if isinstance(bounds_str, str):\n",
    "            bounds_str = bounds_str.replace(' ', '')\n",
    "            bounds = tuple(float(x) for x in bounds_str.strip('()').split(','))\n",
    "        else:\n",
    "            bounds = bounds_str\n",
    "        min_x, min_y, max_x, max_y = bounds\n",
    "\n",
    "        tile_path = row['tile_path']\n",
    "        output_filename = os.path.basename(tile_path)\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "        with rasterio.open(row['geotiff_path']) as src:\n",
    "            window = from_bounds(min_x, min_y, max_x, max_y, src.transform)\n",
    "            window = rasterio.windows.Window(\n",
    "                col_off=int(round(window.col_off)),\n",
    "                row_off=int(round(window.row_off)),\n",
    "                width=int(round(window.width)),\n",
    "                height=int(round(window.height))\n",
    "            )\n",
    "            # Ensure window is within image bounds\n",
    "            if (window.col_off < 0 or window.row_off < 0 or \n",
    "                window.col_off + window.width > src.width or \n",
    "                window.row_off + window.height > src.height):\n",
    "                window = window.intersection(\n",
    "                    rasterio.windows.Window(0, 0, src.width, src.height)\n",
    "                )\n",
    "            data = src.read(window=window)\n",
    "            window_transform = rasterio.windows.transform(window, src.transform)\n",
    "            profile = src.profile.copy()\n",
    "            profile.update({\n",
    "                'height': window.height,\n",
    "                'width': window.width,\n",
    "                'transform': window_transform,\n",
    "                'crs': CORRECT_CRS,\n",
    "                'driver': 'GTiff',\n",
    "                'compress': None,\n",
    "                'predictor': 1,\n",
    "                'tiled': False,\n",
    "                'interleave': 'band',\n",
    "                'bigtiff': True,\n",
    "                'dtype': src.dtypes[0],\n",
    "            })\n",
    "            with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                dst.write(data)\n",
    "        return (row.name, True, None)\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        return (row.name, False, f\"Error: {str(e)}\\n{tb}\")\n",
    "\n",
    "# Copy a single file\n",
    "def copy_file(args):\n",
    "    src_file, dst_file = args\n",
    "    try:\n",
    "        shutil.copy2(src_file, dst_file)\n",
    "        return (True, src_file)\n",
    "    except Exception as e:\n",
    "        return (False, f\"Error copying {src_file} to {dst_file}: {str(e)}\")\n",
    "\n",
    "# Process a group of tiles\n",
    "def process_geotiffs(chunk_df, output_dir, debug_dir):\n",
    "    results = []\n",
    "    for idx, row in chunk_df.iterrows():\n",
    "        result = process_tile(row, output_dir, debug_dir)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "if REGENERATE_TILE_1024_SPLIT_FROM_SITG:\n",
    "    warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    df = pd.read_parquet(REGENERATE_TILE_1024_SPLIT_FROM_SITG_COMBINED_METADATA_PARQUET)\n",
    "    os.makedirs(REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TEMP_1280, exist_ok=True)\n",
    "    debug_dir = os.path.join(REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TEMP_1280, 'debug')\n",
    "    os.makedirs(debug_dir, exist_ok=True)\n",
    "\n",
    "    # Group by source file\n",
    "    grouped = df.groupby('geotiff_path')\n",
    "    group_dfs = [group for _, group in grouped]\n",
    "\n",
    "    print(f\"Processing {len(df)} tiles from {len(group_dfs)} source GeoTIFFs using {REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_PROCESSES} processes\")\n",
    "\n",
    "    # Parallel processing\n",
    "    with ProcessPoolExecutor(max_workers=REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_PROCESSES) as executor:\n",
    "        futures = [executor.submit(process_geotiffs, group_df, REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TEMP_1280, debug_dir) \n",
    "                  for group_df in group_dfs]\n",
    "        all_results = []\n",
    "        for future in tqdm(futures, total=len(futures), desc=\"Processing GeoTIFF groups\"):\n",
    "            results = future.result()\n",
    "            all_results.extend(results)\n",
    "\n",
    "    # Log results\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    for idx, success, error_msg in all_results:\n",
    "        if success:\n",
    "            success_count += 1\n",
    "        else:\n",
    "            error_count += 1\n",
    "            error_info_path = os.path.join(debug_dir, f\"error_row_{idx}.txt\")\n",
    "            with open(error_info_path, 'w') as f:\n",
    "                f.write(error_msg)\n",
    "    print(f\"Tile processing complete: {success_count} successful, {error_count} errors\")\n",
    "\n",
    "    src_dir = REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TILE_1024\n",
    "    dst_dir = REGENERATE_TILE_1024_SPLIT_FROM_SITG_OUTPUT_DIR_TEMP_1280\n",
    "\n",
    "    processed_tif_files = set(os.path.basename(row['tile_path']) for _, row in df.iterrows())\n",
    "\n",
    "    files_to_copy = []\n",
    "    print(\"\\nScanning directory structure...\")\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        rel_path = os.path.relpath(root, src_dir)\n",
    "        if rel_path != '.':\n",
    "            dst_root = os.path.join(dst_dir, rel_path)\n",
    "            os.makedirs(dst_root, exist_ok=True)\n",
    "            print(f\"Created directory: {rel_path}\")\n",
    "        for file in files:\n",
    "            if rel_path == '.' and file.endswith('.tif') and file in processed_tif_files:\n",
    "                continue\n",
    "            src_file = os.path.join(root, file)\n",
    "            dst_file = os.path.join(dst_dir, rel_path, file) if rel_path != '.' else os.path.join(dst_dir, file)\n",
    "            files_to_copy.append((src_file, dst_file))\n",
    "\n",
    "    # Parallel file copy\n",
    "    print(f\"\\nCopying {len(files_to_copy)} additional files using {REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_THREADS} threads...\")\n",
    "    with ThreadPoolExecutor(max_workers=REGENERATE_TILE_1024_SPLIT_FROM_SITG_NUM_THREADS) as executor:\n",
    "        results = list(tqdm(executor.map(copy_file, files_to_copy), total=len(files_to_copy), desc=\"Copying files\"))\n",
    "\n",
    "    # Check copy errors\n",
    "    copy_errors = [result for result in results if not result[0]]\n",
    "    if copy_errors:\n",
    "        print(f\"Warning: {len(copy_errors)} files failed to copy:\")\n",
    "        for _, error in copy_errors[:10]:\n",
    "            print(f\"  {error}\")\n",
    "        if len(copy_errors) > 10:\n",
    "            print(f\"  ... and {len(copy_errors) - 10} more errors\")\n",
    "\n",
    "    # File count verification\n",
    "    def count_files(directory):\n",
    "        count = 0\n",
    "        for root, _, files in os.walk(directory):\n",
    "            count += len(files)\n",
    "        return count\n",
    "\n",
    "    old_count = count_files(src_dir)\n",
    "    new_count = count_files(dst_dir)\n",
    "\n",
    "    print(f\"\\nTotal files in old directory (including subdirectories): {old_count}\")\n",
    "    print(f\"Total files in new directory (including subdirectories): {new_count}\")\n",
    "\n",
    "    expected_diff = len(processed_tif_files)\n",
    "    actual_diff = new_count - old_count + expected_diff\n",
    "\n",
    "    print(f\"Expected difference (replaced TIF files): {expected_diff}\")\n",
    "    print(f\"Actual difference: {actual_diff}\")\n",
    "\n",
    "    if actual_diff != 0:\n",
    "        print(\"WARNING: File count doesn't match expectations!\")\n",
    "        proceed = input(\"Do you want to proceed with the renaming? (y/n): \")\n",
    "        if proceed.lower() != 'y':\n",
    "            print(\"Operation aborted\")\n",
    "            exit()\n",
    "\n",
    "    # Rename old and new folders\n",
    "    todaysdate = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    old_folder_new_name = \"data/notebook_04/geotiff/tile_1024_split_old_\" + str(todaysdate)\n",
    "    print(f\"\\nRenaming old folder '{src_dir}' to '{old_folder_new_name}'\")\n",
    "    os.rename(src_dir, old_folder_new_name)\n",
    "    print(f\"Renaming new folder '{dst_dir}' to '{src_dir}'\")\n",
    "    os.rename(dst_dir, src_dir)\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset avant annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_dataset = gpd.read_file(DATASET_TILES_INFORMATION_CSV_PATH)\n",
    "if \"geometry\" in gdf_dataset.columns:\n",
    "    gdf_dataset = gdf_dataset.drop(columns=[\"geometry\"])\n",
    "if \"geometry_x\" in gdf_dataset.columns:\n",
    "    gdf_dataset = gdf_dataset.drop(columns=[\"geometry_x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gdf_dataset))\n",
    "gdf_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no duplicate in gdf_dataset[\"tile_id\"]\n",
    "assert(len(gdf_dataset[gdf_dataset.duplicated(subset=[\"tile_id\"])]) == 0), f\"gdf_dataset has duplicates in tile_id: {gdf_dataset[gdf_dataset.duplicated(subset=['tile_id'])]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset annoté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img\n",
    "dataset_img_path = os.path.join(DATASET_ANNOTATED_BRUT_PATH, [f for f in os.listdir(DATASET_ANNOTATED_BRUT_PATH) if f.startswith(\"dataset\")][0], \"img\")\n",
    "assert(os.path.exists(dataset_img_path)), f\"Path does not exist: {dataset_img_path}\"\n",
    "print(f\"dataset_masks_path: {dataset_img_path}\")\n",
    "print(f\"Number of files: {len(os.listdir(dataset_img_path))}\")\n",
    "\n",
    "\n",
    "# masks\n",
    "dataset_masks_path = os.path.join(DATASET_ANNOTATED_BRUT_PATH, [f for f in os.listdir(DATASET_ANNOTATED_BRUT_PATH) if f.startswith(\"dataset\")][0], \"masks_machine\")\n",
    "assert(os.path.exists(dataset_masks_path)), f\"Path does not exist: {dataset_masks_path}\"\n",
    "print(f\"dataset_masks_path: {dataset_masks_path}\")\n",
    "print(f\"Number of files: {len(os.listdir(dataset_masks_path))}\")\n",
    "\n",
    "# verif\n",
    "assert(len(os.listdir(dataset_img_path)) == len(os.listdir(dataset_masks_path))), f\"Number of files in {dataset_masks_path} is not equal to number of files in {dataset_masks_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lists of mask and image file paths\n",
    "dataset_original_masks_path_list = [os.path.join(dataset_masks_path, f) for f in os.listdir(dataset_masks_path)]\n",
    "dataset_original_img_path_list = [os.path.join(dataset_img_path, f) for f in os.listdir(dataset_img_path)]\n",
    "\n",
    "# Create annotation dataframe\n",
    "df_annotations = pd.DataFrame(\n",
    "    {\n",
    "        \"tile_id\": [os.path.basename(f).split(\".\")[0] for f in dataset_original_masks_path_list],\n",
    "        \"original_mask_path_png\": dataset_original_masks_path_list,\n",
    "        \"original_img_path_png\": dataset_original_img_path_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract tile_id suffix after second underscore\n",
    "df_annotations[\"tile_id\"] = df_annotations[\"tile_id\"].apply(lambda x: \"_\".join(x.split(\"_\")[2:]))\n",
    "\n",
    "# Check for duplicates\n",
    "assert(len(df_annotations[df_annotations.duplicated(subset=[\"tile_id\"])]) == 0), f\"gdf_dataset has duplicates in tile_id: {df_annotations[df_annotations.duplicated(subset=['tile_id'])]}\"\n",
    "assert(len(df_annotations[df_annotations.duplicated(subset=[\"original_mask_path_png\"])]) == 0), f\"gdf_dataset has duplicates in original_mask_path_png: {df_annotations[df_annotations.duplicated(subset=['original_mask_path_png'])]}\"\n",
    "assert(len(df_annotations[df_annotations.duplicated(subset=[\"original_img_path_png\"])]) == 0), f\"gdf_dataset has duplicates in original_img_path_png: {df_annotations[df_annotations.duplicated(subset=['original_img_path_png'])]}\"\n",
    "\n",
    "# Check for missing values\n",
    "assert(df_annotations[\"tile_id\"].notnull().all()), f\"df_annotations has null values in tile_id: {df_annotations[df_annotations['tile_id'].isnull()]}\"\n",
    "assert(df_annotations[\"original_mask_path_png\"].notnull().all()), f\"df_annotations has null values in original_mask_path_png: {df_annotations[df_annotations['original_mask_path_png'].isnull()]}\"\n",
    "assert(df_annotations[\"original_img_path_png\"].notnull().all()), f\"df_annotations has null values in original_img_path_png: {df_annotations[df_annotations['original_img_path_png'].isnull()]}\"\n",
    "\n",
    "# Check row count consistency\n",
    "assert(len(df_annotations[\"tile_id\"]) == len(gdf_dataset[\"tile_id\"])), f\"len(df_annotations['tile_id']) is not equal to len(gdf_dataset['tile_id']): {len(df_annotations['tile_id'])} != {len(gdf_dataset['tile_id'])}\"\n",
    "\n",
    "display(df_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_dataset = gdf_dataset.merge(\n",
    "    df_annotations,\n",
    "    how=\"left\",\n",
    "    left_on=\"tile_id\",\n",
    "    right_on=\"tile_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "assert(len(gdf_dataset[gdf_dataset.duplicated(subset=[\"tile_id\"])]) == 0), f\"gdf_dataset has duplicates in tile_id: {gdf_dataset[gdf_dataset.duplicated(subset=['tile_id'])]}\"\n",
    "assert(len(gdf_dataset[gdf_dataset.duplicated(subset=[\"original_img_path_png\"])]) == 0), f\"gdf_dataset has duplicates in img_path: {gdf_dataset[gdf_dataset.duplicated(subset=['original_img_path_png'])]}\"\n",
    "assert(len(gdf_dataset[gdf_dataset.duplicated(subset=[\"original_mask_path_png\"])]) == 0), f\"gdf_dataset has duplicates in original_mask_path_png: {gdf_dataset[gdf_dataset.duplicated(subset=['original_mask_path_png'])]}\"\n",
    "\n",
    "# Check for missing values\n",
    "assert(gdf_dataset[\"tile_id\"].notnull().all()), f\"gdf_dataset has null values in tile_id: {gdf_dataset[gdf_dataset['tile_id'].isnull()]}\"\n",
    "assert(gdf_dataset[\"original_img_path_png\"].notnull().all()), f\"gdf_dataset has null values in original_img_path_png: {gdf_dataset[gdf_dataset['original_img_path_png'].isnull()]}\"\n",
    "assert(gdf_dataset[\"original_mask_path_png\"].notnull().all()), f\"gdf_dataset has null values in original_mask_path_png: {gdf_dataset[gdf_dataset['original_mask_path_png'].isnull()]}\"\n",
    "\n",
    "# Check row count consistency\n",
    "assert(len(gdf_dataset[\"tile_id\"]) == len(df_annotations[\"tile_id\"])), f\"len(gdf_dataset['tile_id']) is not equal to len(df_annotations['tile_id']): {len(gdf_dataset['tile_id'])} != {len(df_annotations['tile_id'])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAD_BATIMENT_HORSOL_TOIT_MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_cad_batiment_horsol = gpd.read_parquet(CAD_BATIMENT_HORSOL_TOIT_MERGE_PARQUET_PATH)\n",
    "print(type(gdf_cad_batiment_horsol))\n",
    "gdf_cad_batiment_horsol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compléter données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract polygon geometry from a GeoTIFF file\n",
    "def get_geometry_from_tiff(tiff_path, crs=CORRECT_CRS):\n",
    "    try:\n",
    "        with rasterio.open(tiff_path) as src:\n",
    "            transform = src.transform\n",
    "            height, width = src.shape\n",
    "            crs_src = src.crs\n",
    "\n",
    "            if crs_src != crs:\n",
    "                print(f\"Warning: CRS mismatch in {tiff_path}. Found {crs_src}, expected {crs}\")\n",
    "\n",
    "            minx = transform[2]\n",
    "            maxy = transform[5]\n",
    "            miny = maxy + height * transform[4]\n",
    "            maxx = minx + width * transform[0]\n",
    "\n",
    "            polygon = Polygon([\n",
    "                (minx, miny), (maxx, miny), (maxx, maxy), (minx, maxy), (minx, miny)\n",
    "            ])\n",
    "\n",
    "            if not polygon.is_valid:\n",
    "                print(f\"Warning: Invalid polygon from {tiff_path}\")\n",
    "                polygon = polygon.buffer(0)\n",
    "\n",
    "            if polygon.area <= 0:\n",
    "                print(f\"Warning: Zero-area polygon from {tiff_path}\")\n",
    "                return None\n",
    "\n",
    "            return polygon\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {tiff_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get image width and height\n",
    "def get_image_dimensions(image_path):\n",
    "    try:\n",
    "        if os.path.exists(image_path):\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                return width, height\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {image_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def create_geodataframe_from_tiffs(df):\n",
    "    \"\"\"\n",
    "    Create a GeoDataFrame from a DataFrame of GeoTIFF paths.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 'tile_path' column\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame with geometry column\n",
    "    \"\"\"\n",
    "    geometries = []\n",
    "    indices = []\n",
    "    total_files = len(df)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        tiff_path = row['tile_path']\n",
    "        if os.path.exists(tiff_path):\n",
    "            geometry = get_geometry_from_tiff(tiff_path)\n",
    "            if geometry is not None:\n",
    "                geometries.append(geometry)\n",
    "                indices.append(idx)\n",
    "        else:\n",
    "            print(f\"File not found: {tiff_path}\")\n",
    "\n",
    "    print(f\"Processed {len(geometries)} of {total_files} files\")\n",
    "\n",
    "    df_processed = df.loc[indices].copy()\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df_processed,\n",
    "        geometry=geometries,\n",
    "        crs=EPSG_SUISSE\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "gdf_dataset = create_geodataframe_from_tiffs(gdf_dataset)\n",
    "\n",
    "# Add image dimensions columns\n",
    "image_dimensions = gdf_dataset['tile_path'].apply(get_image_dimensions)\n",
    "gdf_dataset['image_width'], gdf_dataset['image_height'] = zip(*image_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert there is no nan or null in gdf_dataset[\"image_width\"], gdf_dataset[\"image_height\"]\n",
    "assert(gdf_dataset[\"image_width\"].notnull().all()), f\"gdf_dataset has null values in image_width: {gdf_dataset[gdf_dataset['image_width'].isnull()]}\"\n",
    "assert(gdf_dataset[\"image_height\"].notnull().all()), f\"gdf_dataset has null values in image_height: {gdf_dataset[gdf_dataset['image_height'].isnull()]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gdf_dataset))\n",
    "gdf_dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing images dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Clip geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_geotiff_and_png_masks(gdf_dataset, gdf_buildings, output_img_dir, output_mask_dir, convert_masks_to_geotiff=True):\n",
    "    \"\"\"\n",
    "    Clips GeoTIFFs and corresponding PNG masks using building polygons.\n",
    "    Optionally converts PNG masks to GeoTIFF.\n",
    "    \n",
    "    Args:\n",
    "        gdf_dataset: GeoDataFrame with 'tile_path', 'original_mask_path_png', and 'tile_id'\n",
    "        gdf_buildings: GeoDataFrame with building polygons\n",
    "        output_img_dir: Output directory for clipped GeoTIFFs\n",
    "        output_mask_dir: Output directory for clipped masks\n",
    "        convert_masks_to_geotiff: Convert PNG masks to GeoTIFF if True\n",
    "        \n",
    "    Returns:\n",
    "        list: Processed tile_ids\n",
    "        dict: Skipped tile_ids with reasons\n",
    "    \"\"\"\n",
    "    successful_img_count = 0\n",
    "    successful_mask_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    processed_tile_ids = []\n",
    "    skipped_tile_ids = []\n",
    "    skipped_reasons = {}\n",
    "\n",
    "    for idx, row in tqdm(gdf_dataset.iterrows(), total=len(gdf_dataset), desc=\"Clipping files\"):\n",
    "        tiff_path = row['tile_path']\n",
    "        mask_path = row.get('original_mask_path_png')\n",
    "        tile_id = row['tile_id']\n",
    "\n",
    "        if pd.isna(tiff_path):\n",
    "            skipped_tile_ids.append(tile_id)\n",
    "            skipped_reasons[tile_id] = \"Missing tiff_path\"\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(tiff_path):\n",
    "            skipped_tile_ids.append(tile_id)\n",
    "            skipped_reasons[tile_id] = f\"TIFF file not found: {tiff_path}\"\n",
    "            continue\n",
    "\n",
    "        output_img_path = os.path.join(output_img_dir, os.path.basename(tiff_path))\n",
    "\n",
    "        if pd.isna(mask_path) or not os.path.exists(mask_path):\n",
    "            mask_path = None\n",
    "            output_mask_path = None\n",
    "            skipped_reasons[tile_id] = \"Missing or invalid mask_path\"\n",
    "        else:\n",
    "            if convert_masks_to_geotiff:\n",
    "                mask_basename = os.path.splitext(os.path.basename(mask_path))[0] + '.tif'\n",
    "                output_mask_path = os.path.join(output_mask_dir, mask_basename)\n",
    "            else:\n",
    "                output_mask_path = os.path.join(output_mask_dir, os.path.basename(mask_path))\n",
    "\n",
    "        try:\n",
    "            tile_geom = row.geometry\n",
    "            if tile_geom is None:\n",
    "                skipped_tile_ids.append(tile_id)\n",
    "                skipped_reasons[tile_id] = \"Missing geometry\"\n",
    "                continue\n",
    "\n",
    "            buildings_in_tile = gdf_buildings[gdf_buildings.intersects(tile_geom)]\n",
    "            if len(buildings_in_tile) == 0:\n",
    "                skipped_tile_ids.append(tile_id)\n",
    "                skipped_reasons[tile_id] = \"No intersecting buildings found\"\n",
    "                continue\n",
    "\n",
    "            with rasterio.open(tiff_path) as src:\n",
    "                src_meta = src.meta.copy()\n",
    "                original_height, original_width = src.height, src.width\n",
    "                shapes = [mapping(geom) for geom in buildings_in_tile.geometry]\n",
    "\n",
    "                masked_data, mask_transform = rasterio.mask.mask(\n",
    "                    src, \n",
    "                    shapes, \n",
    "                    crop=False, \n",
    "                    all_touched=True,\n",
    "                    invert=True,\n",
    "                    filled=True,\n",
    "                    nodata=0\n",
    "                )\n",
    "\n",
    "                binary_mask = (masked_data[0] == 0).astype(np.uint8)\n",
    "                original_img = src.read()\n",
    "                masked_img = original_img.copy()\n",
    "\n",
    "                for i in range(masked_img.shape[0]):\n",
    "                    masked_img[i][binary_mask == 0] = 0\n",
    "\n",
    "                out_meta = src_meta.copy()\n",
    "                with rasterio.open(output_img_path, 'w', **out_meta) as dest:\n",
    "                    dest.write(masked_img)\n",
    "\n",
    "                successful_img_count += 1\n",
    "\n",
    "                if mask_path is not None:\n",
    "                    try:\n",
    "                        with Image.open(mask_path) as mask_img:\n",
    "                            mask_array = np.array(mask_img)\n",
    "                            if mask_array.shape[:2] != (original_height, original_width):\n",
    "                                tiff_dims = f\"{original_width}x{original_height}\"\n",
    "                                mask_dims = f\"{mask_array.shape[1]}x{mask_array.shape[0]}\"\n",
    "                                print(f\"Warning: Mask dimensions don't match GeoTIFF for {os.path.basename(tiff_path)}\")\n",
    "                                print(f\"  - GeoTIFF dimensions: {tiff_dims}\")\n",
    "                                print(f\"  - Mask dimensions: {mask_dims}\")\n",
    "                                skipped_reasons[tile_id] = f\"Mask dimensions don't match GeoTIFF: GeoTIFF={tiff_dims}, Mask={mask_dims}\"\n",
    "                                continue\n",
    "\n",
    "                            if len(mask_array.shape) == 3:\n",
    "                                for i in range(mask_array.shape[2]):\n",
    "                                    mask_array[:, :, i] = mask_array[:, :, i] * binary_mask\n",
    "                            else:\n",
    "                                mask_array = mask_array * binary_mask\n",
    "\n",
    "                            if convert_masks_to_geotiff:\n",
    "                                mask_meta = src_meta.copy()\n",
    "                                if len(mask_array.shape) == 3:\n",
    "                                    mask_meta.update(\n",
    "                                        dtype=mask_array.dtype,\n",
    "                                        count=mask_array.shape[2],\n",
    "                                        nodata=0,\n",
    "                                    )\n",
    "                                else:\n",
    "                                    mask_meta.update(\n",
    "                                        dtype=mask_array.dtype,\n",
    "                                        count=1,\n",
    "                                        nodata=0,\n",
    "                                    )\n",
    "                                with rasterio.open(output_mask_path, 'w', **mask_meta) as dest:\n",
    "                                    if len(mask_array.shape) == 3:\n",
    "                                        for i in range(mask_array.shape[2]):\n",
    "                                            dest.write(mask_array[:, :, i], i+1)\n",
    "                                    else:\n",
    "                                        dest.write(mask_array, 1)\n",
    "                            else:\n",
    "                                Image.fromarray(mask_array).save(output_mask_path)\n",
    "\n",
    "                            successful_mask_count += 1\n",
    "                            processed_tile_ids.append(tile_id)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        error_count += 1\n",
    "                        skipped_tile_ids.append(tile_id)\n",
    "                        skipped_reasons[tile_id] = f\"Error processing mask: {str(e)}\"\n",
    "                        print(f\"Error processing mask {mask_path}: {e}\")\n",
    "                else:\n",
    "                    processed_tile_ids.append(tile_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            skipped_tile_ids.append(tile_id)\n",
    "            skipped_reasons[tile_id] = f\"Error: {str(e)}\"\n",
    "            print(f\"Error processing {tiff_path}: {e}\")\n",
    "\n",
    "    processed_tile_ids = list(set(processed_tile_ids))\n",
    "    skipped_tile_ids = list(set(skipped_tile_ids))\n",
    "    overlap = set(processed_tile_ids) & set(skipped_tile_ids)\n",
    "\n",
    "    print(f\"Processed {len(gdf_dataset)} files\")\n",
    "    print(f\"- {successful_img_count} GeoTIFFs\")\n",
    "    print(f\"- {successful_mask_count} masks\")\n",
    "    if convert_masks_to_geotiff:\n",
    "        print(f\"- {successful_mask_count} PNG masks converted to GeoTIFF\")\n",
    "    print(f\"- {error_count} errors\")\n",
    "    print(f\"- {len(processed_tile_ids)} tiles processed\")\n",
    "    print(f\"- {len(skipped_tile_ids)} tiles skipped\")\n",
    "\n",
    "    if overlap:\n",
    "        print(f\"Warning: {len(overlap)} tile_ids in both processed and skipped lists.\")\n",
    "\n",
    "    if skipped_tile_ids:\n",
    "        dimension_mismatches = [reason for tile_id, reason in skipped_reasons.items() \n",
    "                            if \"Mask dimensions don't match\" in reason]\n",
    "\n",
    "        print(\"\\nDimension mismatch summary:\")\n",
    "        print(f\"  - {len(dimension_mismatches)} files with dimension mismatches\")\n",
    "\n",
    "        if dimension_mismatches:\n",
    "            import re\n",
    "            geotiff_dims = []\n",
    "            mask_dims = []\n",
    "            pattern = r\"GeoTIFF=(\\d+x\\d+), Mask=(\\d+x\\d+)\"\n",
    "\n",
    "            for reason in dimension_mismatches:\n",
    "                match = re.search(pattern, reason)\n",
    "                if match:\n",
    "                    geotiff_dims.append(match.group(1))\n",
    "                    mask_dims.append(match.group(2))\n",
    "\n",
    "            from collections import Counter\n",
    "            geotiff_counter = Counter(geotiff_dims)\n",
    "            mask_counter = Counter(mask_dims)\n",
    "\n",
    "            print(\"\\nMost common GeoTIFF dimensions:\")\n",
    "            for dims, count in geotiff_counter.most_common(3):\n",
    "                print(f\"  - {dims}: {count} files\")\n",
    "\n",
    "            print(\"\\nMost common mask dimensions:\")\n",
    "            for dims, count in mask_counter.most_common(3):\n",
    "                print(f\"  - {dims}: {count} files\")\n",
    "\n",
    "        skipped_tiles = {tile_id: skipped_reasons[tile_id] for tile_id in skipped_tile_ids}\n",
    "        return processed_tile_ids, skipped_tiles\n",
    "\n",
    "processed_tile_ids, skipped_tiles = clip_geotiff_and_png_masks(\n",
    "    gdf_dataset, gdf_cad_batiment_horsol, DATASET_OUTPUT_IMG_PATH, DATASET_OUTPUT_MASKS_PATH, convert_masks_to_geotiff=True\n",
    ")\n",
    "\n",
    "print(f\"Processed tile_ids: {len(processed_tile_ids)}\")\n",
    "print(f\"Skipped tile_ids: {len(skipped_tiles)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajouter path au gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of files inside the paths\n",
    "dataset_processed_masks_path_list = [os.path.join(DATASET_OUTPUT_MASKS_PATH, f) for f in os.listdir(DATASET_OUTPUT_MASKS_PATH)]\n",
    "dataset_processed_img_path_list = [os.path.join(DATASET_OUTPUT_IMG_PATH, f) for f in os.listdir(DATASET_OUTPUT_IMG_PATH)]\n",
    "\n",
    "df_processed = pd.DataFrame(\n",
    "    {\n",
    "        \"tile_id\": [os.path.basename(f).split(\".\")[0] for f in dataset_processed_masks_path_list],\n",
    "        \"processed_mask_path_tif\": dataset_processed_masks_path_list,\n",
    "        \"processed_img_path_tif\": dataset_processed_img_path_list,\n",
    "    }\n",
    ")\n",
    "# split tile_id after the second underscore keep the right part\n",
    "df_processed[\"tile_id\"] = df_processed[\"tile_id\"].apply(lambda x: \"_\".join(x.split(\"_\")[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no duplicate in df_annotations[\"tile_id\"], no duplicate in df_annotations[\"tile_id\"], no duplicate in df_annotations[\"tile_id\"]\n",
    "assert(len(df_processed[df_processed.duplicated(subset=[\"tile_id\"])]) == 0), f\"df_processed has duplicates in tile_id: {df_processed[df_processed.duplicated(subset=['tile_id'])]}\"\n",
    "assert(len(df_processed[df_processed.duplicated(subset=[\"processed_img_path_tif\"])]) == 0), f\"df_processed has duplicates in processed_img_path_tif: {df_processed[df_processed.duplicated(subset=['processed_img_path_tif'])]}\"\n",
    "assert(len(df_processed[df_processed.duplicated(subset=[\"processed_mask_path_tif\"])]) == 0), f\"df_processed has duplicates in processed_mask_path_tif: {df_processed[df_processed.duplicated(subset=['processed_mask_path_tif'])]}\"\n",
    "\n",
    "# no nan or null in df_processed[\"tile_id\"], no nan or null in df_processed[\"processed_img_path_tif\"], no nan or null in df_processed[\"processed_mask_path_tif\"]\n",
    "assert(df_processed[\"tile_id\"].isnull().sum() == 0), f\"df_processed has null in tile_id: {df_processed[df_processed['tile_id'].isnull()]}\"\n",
    "assert(df_processed[\"processed_img_path_tif\"].isnull().sum() == 0), f\"df_processed has null in processed_img_path_tif: {df_processed[df_processed['processed_img_path_tif'].isnull()]}\"\n",
    "assert(df_processed[\"processed_mask_path_tif\"].isnull().sum() == 0), f\"df_processed has null in processed_mask_path_tif: {df_processed[df_processed['processed_mask_path_tif'].isnull()]}\"\n",
    "\n",
    "display(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_dataset = gdf_dataset.merge(\n",
    "    df_processed,\n",
    "    how=\"left\",\n",
    "    left_on=\"tile_id\",\n",
    "    right_on=\"tile_id\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gérér les chevauchement entre tuiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Déterminer coin chevaucehement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_relative_position(geom1, geom2, tolerance=0.5):\n",
    "    \"\"\"\n",
    "    Returns the relative position of geom1 to geom2.\n",
    "    Assumes Y increases northward.\n",
    "    \"\"\"\n",
    "    minx1, miny1, maxx1, maxy1 = geom1.bounds\n",
    "    minx2, miny2, maxx2, maxy2 = geom2.bounds\n",
    "\n",
    "    center_x1 = (minx1 + maxx1) / 2\n",
    "    center_y1 = (miny1 + maxy1) / 2\n",
    "    center_x2 = (minx2 + maxx2) / 2\n",
    "    center_y2 = (miny2 + maxy2) / 2\n",
    "\n",
    "    avg_width = ((maxx1 - minx1) + (maxx2 - minx2)) / 2\n",
    "    avg_height = ((maxy1 - miny1) + (maxy2 - miny2)) / 2\n",
    "\n",
    "    x_tolerance = tolerance * avg_width\n",
    "    y_tolerance = tolerance * avg_height\n",
    "\n",
    "    intersection = geom1.intersection(geom2)\n",
    "    intersection_area = intersection.area\n",
    "\n",
    "    vertical_position = None\n",
    "    horizontal_position = None\n",
    "\n",
    "    # Y increases northward\n",
    "    vertical_diff = center_y1 - center_y2\n",
    "    if abs(vertical_diff) <= y_tolerance:\n",
    "        vertical_position = None\n",
    "    elif vertical_diff > 0:\n",
    "        vertical_position = \"bottom\"\n",
    "    else:\n",
    "        vertical_position = \"top\"\n",
    "\n",
    "    # X increases eastward\n",
    "    horizontal_diff = center_x1 - center_x2\n",
    "    if abs(horizontal_diff) <= x_tolerance:\n",
    "        horizontal_position = None\n",
    "    elif horizontal_diff > 0:\n",
    "        horizontal_position = \"left\"\n",
    "    else:\n",
    "        horizontal_position = \"right\"\n",
    "\n",
    "    smaller_area = min(geom1.area, geom2.area)\n",
    "    overlap_percentage = (intersection_area / smaller_area) * 100 if smaller_area > 0 else 0\n",
    "\n",
    "    if vertical_position and horizontal_position:\n",
    "        position = f\"{vertical_position}-{horizontal_position}\"\n",
    "    elif vertical_position:\n",
    "        position = vertical_position\n",
    "    elif horizontal_position:\n",
    "        position = horizontal_position\n",
    "    else:\n",
    "        position = \"substantial-overlap\" if overlap_percentage > 90 else \"center\"\n",
    "\n",
    "    return position\n",
    "\n",
    "def get_opposite_position(position):\n",
    "    \"\"\"Returns the opposite relative position.\"\"\"\n",
    "    position_map = {\n",
    "        'top': 'bottom',\n",
    "        'bottom': 'top',\n",
    "        'left': 'right',\n",
    "        'right': 'left',\n",
    "        'top-left': 'bottom-right',\n",
    "        'top-right': 'bottom-left',\n",
    "        'bottom-left': 'top-right',\n",
    "        'bottom-right': 'top-left',\n",
    "        'center': 'center',\n",
    "        'substantial-overlap': 'substantial-overlap'\n",
    "    }\n",
    "    return position_map.get(position, position)\n",
    "\n",
    "def check_geotiffs_overlap(geom1, geom2, min_overlap_area=0.0):\n",
    "    \"\"\"Returns overlap info between two geometries.\"\"\"\n",
    "    result = {\n",
    "        'overlaps': False,\n",
    "        'overlap_area': 0.0,\n",
    "        'relative_position': None,\n",
    "        'overlap_percentage_1': 0.0,\n",
    "        'overlap_percentage_2': 0.0\n",
    "    }\n",
    "\n",
    "    if geom1.intersects(geom2):\n",
    "        intersection = geom1.intersection(geom2)\n",
    "        overlap_area = intersection.area\n",
    "\n",
    "        if overlap_area > min_overlap_area:\n",
    "            result['overlaps'] = True\n",
    "            result['overlap_area'] = overlap_area\n",
    "            result['relative_position'] = determine_relative_position(geom1, geom2)\n",
    "            result['overlap_percentage_1'] = (overlap_area / geom1.area) * 100\n",
    "            result['overlap_percentage_2'] = (overlap_area / geom2.area) * 100\n",
    "\n",
    "    return result\n",
    "\n",
    "def check_overlaps_in_dataframe(gdf, min_overlap_area=1.0, include_symmetric=False, buffer_distance=0.01):\n",
    "    \"\"\"\n",
    "    Checks for overlapping geometries in a GeoDataFrame.\n",
    "    \"\"\"\n",
    "    overlap_results = []\n",
    "    n = len(gdf)\n",
    "\n",
    "    try:\n",
    "        if not isinstance(gdf, gpd.GeoDataFrame):\n",
    "            raise TypeError(\"Input must be a GeoDataFrame\")\n",
    "\n",
    "        if n == 0:\n",
    "            raise ValueError(\"GeoDataFrame is empty\")\n",
    "\n",
    "        print(\"Creating spatial index...\")\n",
    "        sindex = gdf.sindex\n",
    "\n",
    "        print(f\"Checking overlaps among {n} geometries...\")\n",
    "        with tqdm(total=n, desc=\"Checking overlaps\") as pbar:\n",
    "            for i in range(n):\n",
    "                geom1 = gdf.iloc[i]['geometry']\n",
    "                tile_id1 = gdf.iloc[i]['tile_id']\n",
    "\n",
    "                if geom1 is None or not geom1.is_valid:\n",
    "                    print(f\"Warning: Skipping invalid geometry for {tile_id1}\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                bbox = geom1.bounds\n",
    "                potential_matches_idx = list(sindex.intersection(bbox))\n",
    "\n",
    "                if i in potential_matches_idx:\n",
    "                    potential_matches_idx.remove(i)\n",
    "\n",
    "                potential_matches_idx = [j for j in potential_matches_idx if j > i]\n",
    "\n",
    "                for j in potential_matches_idx:\n",
    "                    geom2 = gdf.iloc[j]['geometry']\n",
    "                    tile_id2 = gdf.iloc[j]['tile_id']\n",
    "\n",
    "                    if geom2 is None or not geom2.is_valid:\n",
    "                        print(f\"Warning: Skipping invalid geometry for {tile_id2}\")\n",
    "                        continue\n",
    "\n",
    "                    if buffer_distance > 0:\n",
    "                        buffered_geom1 = geom1.buffer(buffer_distance)\n",
    "                        buffered_geom2 = geom2.buffer(buffer_distance)\n",
    "                    else:\n",
    "                        buffered_geom1 = geom1\n",
    "                        buffered_geom2 = geom2\n",
    "\n",
    "                    if buffered_geom1.intersects(buffered_geom2):\n",
    "                        intersection = buffered_geom1.intersection(buffered_geom2)\n",
    "\n",
    "                        if not intersection.is_empty and intersection.area > min_overlap_area:\n",
    "                            result = check_geotiffs_overlap(buffered_geom1, buffered_geom2, min_overlap_area)\n",
    "\n",
    "                            if result['overlaps']:\n",
    "                                overlap_results.append({\n",
    "                                    'tile_id1': tile_id1,\n",
    "                                    'tile_id2': tile_id2,\n",
    "                                    'index1': i,\n",
    "                                    'index2': j,\n",
    "                                    'overlap_area': result['overlap_area'],\n",
    "                                    'relative_position': result['relative_position'],\n",
    "                                    'overlap_percentage_1': result['overlap_percentage_1'],\n",
    "                                    'overlap_percentage_2': result['overlap_percentage_2'],\n",
    "                                    'buffered': buffer_distance > 0\n",
    "                                })\n",
    "\n",
    "                                if include_symmetric:\n",
    "                                    opposite_position = get_opposite_position(result['relative_position'])\n",
    "\n",
    "                                    overlap_results.append({\n",
    "                                        'tile_id1': tile_id2,\n",
    "                                        'tile_id2': tile_id1,\n",
    "                                        'index1': j,\n",
    "                                        'index2': i,\n",
    "                                        'overlap_area': result['overlap_area'],\n",
    "                                        'relative_position': opposite_position,\n",
    "                                        'overlap_percentage_1': result['overlap_percentage_2'],\n",
    "                                        'overlap_percentage_2': result['overlap_percentage_1'],\n",
    "                                        'buffered': buffer_distance > 0\n",
    "                                    })\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during overlap check: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    if overlap_results:\n",
    "        overlap_df = pd.DataFrame(overlap_results)\n",
    "        print(f\"Found {len(overlap_df)} overlapping pairs\")\n",
    "\n",
    "        position_counts = overlap_df['relative_position'].value_counts()\n",
    "        print(\"\\nOverlap positions before filtering:\")\n",
    "        for pos, count in position_counts.items():\n",
    "            print(f\"  {pos}: {count}\")\n",
    "\n",
    "        if 'buffered' in overlap_df.columns:\n",
    "            buffered_count = overlap_df['buffered'].sum()\n",
    "            print(f\"\\nOverlaps using buffered geometries: {buffered_count} ({(buffered_count/len(overlap_df))*100:.1f}%)\")\n",
    "\n",
    "        return overlap_df\n",
    "    else:\n",
    "        print(\"No overlapping pairs found\")\n",
    "        return pd.DataFrame(columns=['tile_id1', 'tile_id2', 'index1', 'index2',\n",
    "                                    'overlap_area', 'relative_position',\n",
    "                                    'overlap_percentage_1', 'overlap_percentage_2',\n",
    "                                    'buffered'])\n",
    "\n",
    "\n",
    "# Run overlap check\n",
    "overlap_df = check_overlaps_in_dataframe(gdf_dataset, min_overlap_area=1.0, include_symmetric=True, buffer_distance=BUFFER_DISTANCE)\n",
    "\n",
    "# Show unique positions\n",
    "print(\"\\nUnique position types found:\")\n",
    "for pos in sorted(overlap_df['relative_position'].unique()):\n",
    "    print(f\"  {pos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mettre en background si chevauchement haut ou à droite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_overlap_in_geotiffs(overlap_df, gdf_dataset, overlap_positions=None, overwrite=True, buffer_distance=0.01):\n",
    "    \"\"\"\n",
    "    Removes overlapping regions in GeoTIFF files by setting pixel values to 0 in the specified overlap areas.\n",
    "    Handles both image and mask files. Overlap regions are determined by spatial intersection and relative position.\n",
    "\n",
    "    Args:\n",
    "        overlap_df: DataFrame with overlap information between tiles.\n",
    "        gdf_dataset: GeoDataFrame containing GeoTIFF paths and geometries.\n",
    "        overlap_positions: List of relative positions to process (default: ['right', 'top', 'top-right', 'bottom-right']).\n",
    "        overwrite: If True, modifies files in place. If False, writes to new files.\n",
    "        buffer_distance: Buffer distance for geometry intersection (should match overlap detection).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with details about processed files.\n",
    "    \"\"\"\n",
    "    # Set default positions if not provided\n",
    "    if overlap_positions is None:\n",
    "        overlap_positions = ['right', 'top', 'top-right', 'bottom-right']\n",
    "\n",
    "    processed_files = []\n",
    "    failed_files = []\n",
    "\n",
    "    # Exit if there are no overlaps to process\n",
    "    if len(overlap_df) == 0:\n",
    "        print(\"No overlaps to process\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Filter overlaps by specified positions\n",
    "    filtered_df = overlap_df.copy()\n",
    "    if overlap_positions:\n",
    "        position_filter = filtered_df['relative_position'].apply(\n",
    "            lambda pos: any(p in pos for p in overlap_positions)\n",
    "        )\n",
    "        filtered_df = filtered_df[position_filter]\n",
    "        print(f\"Processing {len(filtered_df)} out of {len(overlap_df)} overlaps that match position criteria\")\n",
    "\n",
    "    if len(filtered_df) == 0:\n",
    "        print(\"No overlaps match the specified positions\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Process each overlap entry\n",
    "    with tqdm(total=len(filtered_df), desc=\"Processing overlaps\") as pbar:\n",
    "        for idx, row in filtered_df.iterrows():\n",
    "            index1 = row['index1']\n",
    "            index2 = row['index2']\n",
    "            position = row['relative_position']\n",
    "\n",
    "            # Get file paths for images and masks\n",
    "            tiff_path1 = gdf_dataset.iloc[index1]['processed_img_path_tif']\n",
    "            tiff_path2 = gdf_dataset.iloc[index2]['processed_img_path_tif']\n",
    "            mask_path1 = gdf_dataset.iloc[index1]['processed_mask_path_tif']\n",
    "            mask_path2 = gdf_dataset.iloc[index2]['processed_mask_path_tif']\n",
    "\n",
    "            # Get geometries for both tiles\n",
    "            geom1 = gdf_dataset.iloc[index1]['geometry']\n",
    "            geom2 = gdf_dataset.iloc[index2]['geometry']\n",
    "\n",
    "            # Use buffered geometries if specified\n",
    "            use_buffer = row.get('buffered', True)\n",
    "            if use_buffer:\n",
    "                buffered_geom1 = geom1.buffer(buffer_distance)\n",
    "                buffered_geom2 = geom2.buffer(buffer_distance)\n",
    "            else:\n",
    "                buffered_geom1 = geom1\n",
    "                buffered_geom2 = geom2\n",
    "\n",
    "            # Calculate intersection area\n",
    "            intersection = buffered_geom1.intersection(buffered_geom2)\n",
    "\n",
    "            # Skip if intersection is empty or invalid\n",
    "            if intersection.is_empty or intersection.area <= 0:\n",
    "                failed_files.append({\n",
    "                    'file_path': f\"{tiff_path1} / {tiff_path2}\",\n",
    "                    'file_type': \"both\",\n",
    "                    'position': position,\n",
    "                    'error': \"Empty intersection\"\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            # Decide which file to modify based on overlap position\n",
    "            modify_idx1 = False\n",
    "            if 'right' in position and 'left' not in position:\n",
    "                modify_idx1 = True\n",
    "            elif 'left' in position and 'right' not in position:\n",
    "                modify_idx1 = False\n",
    "            elif 'top' in position and 'bottom' not in position:\n",
    "                modify_idx1 = False\n",
    "            elif 'bottom' in position and 'top' not in position:\n",
    "                modify_idx1 = True\n",
    "            elif 'center' in position or 'substantial' in position:\n",
    "                modify_idx1 = geom1.area <= geom2.area\n",
    "            else:\n",
    "                modify_idx1 = row['overlap_percentage_1'] <= row['overlap_percentage_2']\n",
    "\n",
    "            # Select file paths to modify\n",
    "            if modify_idx1:\n",
    "                img_to_modify = tiff_path1\n",
    "                mask_to_modify = mask_path1\n",
    "                overlap_with_img = tiff_path2\n",
    "                overlap_with_mask = mask_path2\n",
    "                tile_id = gdf_dataset.iloc[index1]['tile_id']\n",
    "            else:\n",
    "                img_to_modify = tiff_path2\n",
    "                mask_to_modify = mask_path2\n",
    "                overlap_with_img = tiff_path1\n",
    "                overlap_with_mask = mask_path1\n",
    "                tile_id = gdf_dataset.iloc[index2]['tile_id']\n",
    "\n",
    "            # Process both image and mask files\n",
    "            for file_type, file_to_modify in [(\"image\", img_to_modify), (\"mask\", mask_to_modify)]:\n",
    "                if not os.path.exists(file_to_modify):\n",
    "                    failed_files.append({\n",
    "                        'file_path': file_to_modify,\n",
    "                        'file_type': file_type,\n",
    "                        'position': position,\n",
    "                        'error': \"File does not exist\"\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                # Determine output file path\n",
    "                if overwrite:\n",
    "                    output_file = file_to_modify\n",
    "                else:\n",
    "                    output_dir = os.path.dirname(file_to_modify)\n",
    "                    base_name = os.path.basename(file_to_modify)\n",
    "                    output_file = os.path.join(output_dir, f\"overlap_fixed_{base_name}\")\n",
    "\n",
    "                try:\n",
    "                    temp_file = None\n",
    "                    if overwrite:\n",
    "                        temp_dir = os.path.dirname(file_to_modify)\n",
    "                        temp_file = os.path.join(temp_dir, f\"temp_{os.path.basename(file_to_modify)}\")\n",
    "\n",
    "                    with rasterio.open(file_to_modify) as src:\n",
    "                        data = src.read()\n",
    "                        minx, miny, maxx, maxy = intersection.bounds\n",
    "                        window = from_bounds(minx, miny, maxx, maxy, src.transform)\n",
    "\n",
    "                        # Validate window coordinates\n",
    "                        if (np.isnan(window.col_off) or np.isnan(window.row_off) or \n",
    "                            np.isnan(window.width) or np.isnan(window.height)):\n",
    "                            failed_files.append({\n",
    "                                'file_path': file_to_modify,\n",
    "                                'file_type': file_type,\n",
    "                                'position': position,\n",
    "                                'error': \"Invalid window coordinates\"\n",
    "                            })\n",
    "                            continue\n",
    "\n",
    "                        # Convert window coordinates to integers and check bounds\n",
    "                        col_off = max(0, int(window.col_off))\n",
    "                        row_off = max(0, int(window.row_off))\n",
    "                        width = min(int(np.ceil(window.width)), src.width - col_off)\n",
    "                        height = min(int(np.ceil(window.height)), src.height - row_off)\n",
    "\n",
    "                        if width <= 0 or height <= 0:\n",
    "                            failed_files.append({\n",
    "                                'file_path': file_to_modify,\n",
    "                                'file_type': file_type,\n",
    "                                'position': position,\n",
    "                                'error': \"Invalid window dimensions\"\n",
    "                            })\n",
    "                            continue\n",
    "\n",
    "                        # Set overlapping region pixels to 0\n",
    "                        for band in range(data.shape[0]):\n",
    "                            data[band, row_off:row_off+height, col_off:col_off+width] = 0\n",
    "\n",
    "                        profile = src.profile\n",
    "\n",
    "                    write_path = temp_file if overwrite else output_file\n",
    "\n",
    "                    # Write modified data to file\n",
    "                    with rasterio.open(write_path, 'w', **profile) as dst:\n",
    "                        dst.write(data)\n",
    "\n",
    "                    # Replace original file if overwriting\n",
    "                    if overwrite and temp_file:\n",
    "                        if os.path.exists(file_to_modify):\n",
    "                            os.remove(file_to_modify)\n",
    "                        shutil.move(temp_file, file_to_modify)\n",
    "\n",
    "                    processed_files.append({\n",
    "                        'file_path': file_to_modify,\n",
    "                        'file_type': file_type,\n",
    "                        'position': position,\n",
    "                        'overlap_with': overlap_with_img if file_type == 'image' else overlap_with_mask,\n",
    "                        'overlap_area': row['overlap_area'],\n",
    "                        'modified_pixels': width * height,\n",
    "                        'tile_id': tile_id\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    error_msg = str(e)\n",
    "                    print(f\"Error processing {file_type} file {file_to_modify}: {error_msg}\")\n",
    "                    if overwrite and temp_file and os.path.exists(temp_file):\n",
    "                        os.remove(temp_file)\n",
    "                    failed_files.append({\n",
    "                        'file_path': file_to_modify,\n",
    "                        'file_type': file_type,\n",
    "                        'position': position,\n",
    "                        'error': error_msg\n",
    "                    })\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Summarize results\n",
    "    if processed_files:\n",
    "        results_df = pd.DataFrame(processed_files)\n",
    "        print(f\"Successfully processed {len(results_df)} files\")\n",
    "        if failed_files:\n",
    "            failed_df = pd.DataFrame(failed_files)\n",
    "            print(f\"Failed to process {len(failed_df)} files\")\n",
    "            print(\"First few failures:\")\n",
    "            print(failed_df.head())\n",
    "        return results_df\n",
    "    else:\n",
    "        if failed_files:\n",
    "            failed_df = pd.DataFrame(failed_files)\n",
    "            print(f\"Failed to process all {len(failed_df)} files\")\n",
    "            print(\"First few failures:\")\n",
    "            print(failed_df.head())\n",
    "        print(\"No files were processed successfully\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage\n",
    "results = remove_overlap_in_geotiffs(overlap_df, gdf_dataset, overlap_positions=OVERLAP_POSITIONS, buffer_distance=BUFFER_DISTANCE)\n",
    "\n",
    "# Display summary of results\n",
    "if len(results) > 0:\n",
    "    print(\"\\nSummary of processed files:\")\n",
    "    print(f\"Total modified files: {len(results)}\")\n",
    "    file_type_counts = results['file_type'].value_counts()\n",
    "    print(\"\\nFiles by type:\")\n",
    "    print(file_type_counts)\n",
    "    print(\"\\nSample of processed files:\")\n",
    "    display(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vérification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_overlap_corrections(overlap_df, gdf_dataset, buffer_distance=0.01):\n",
    "    \"\"\"\n",
    "    Checks if overlapping regions in GeoTIFF files have been corrected by verifying\n",
    "    if one or both tiles contain background values (0) in the overlapping area.\n",
    "    Handles buffered geometries as needed.\n",
    "\n",
    "    Args:\n",
    "        overlap_df: DataFrame with overlap information.\n",
    "        gdf_dataset: GeoDataFrame with GeoTIFF paths and geometries.\n",
    "        buffer_distance: Buffer distance for geometry, should match the value used in overlap detection.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with verification results for each overlapping pair.\n",
    "    \"\"\"\n",
    "\n",
    "    verification_results = []\n",
    "    skipped_pairs = 0\n",
    "\n",
    "    if len(overlap_df) == 0:\n",
    "        print(\"No overlaps to verify\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Verifying {len(overlap_df)} overlapping pairs...\")\n",
    "    with tqdm(total=len(overlap_df), desc=\"Verifying overlaps\") as pbar:\n",
    "        for idx, row in overlap_df.iterrows():\n",
    "            index1 = row['index1']\n",
    "            index2 = row['index2']\n",
    "            position = row['relative_position']\n",
    "\n",
    "            tiff_path1 = gdf_dataset.iloc[index1]['processed_img_path_tif']\n",
    "            tiff_path2 = gdf_dataset.iloc[index2]['processed_img_path_tif']\n",
    "\n",
    "            geom1 = gdf_dataset.iloc[index1]['geometry']\n",
    "            geom2 = gdf_dataset.iloc[index2]['geometry']\n",
    "\n",
    "            result = {\n",
    "                'tile_id1': row['tile_id1'],\n",
    "                'tile_id2': row['tile_id2'],\n",
    "                'position': position,\n",
    "                'overlap_area': row['overlap_area'],\n",
    "                'file1_has_zeros': False,\n",
    "                'file2_has_zeros': False,\n",
    "                'file1_zero_percentage': 0.0,\n",
    "                'file2_zero_percentage': 0.0,\n",
    "                'both_have_zeros': False,\n",
    "                'either_has_zeros': False,\n",
    "                'avg_zero_percentage': 0.0,\n",
    "                'status': 'unchecked'\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                # Validate geometries\n",
    "                if geom1 is None or not geom1.is_valid or geom2 is None or not geom2.is_valid:\n",
    "                    result['status'] = 'invalid_geometry'\n",
    "                    verification_results.append(result)\n",
    "                    pbar.update(1)\n",
    "                    skipped_pairs += 1\n",
    "                    continue\n",
    "\n",
    "                # Apply buffer if required\n",
    "                use_buffer = row.get('buffered', True)\n",
    "                if use_buffer:\n",
    "                    buffered_geom1 = geom1.buffer(buffer_distance)\n",
    "                    buffered_geom2 = geom2.buffer(buffer_distance)\n",
    "                else:\n",
    "                    buffered_geom1 = geom1\n",
    "                    buffered_geom2 = geom2\n",
    "\n",
    "                # Compute intersection\n",
    "                intersection = buffered_geom1.intersection(buffered_geom2)\n",
    "\n",
    "                # Skip if intersection is empty or has no area\n",
    "                if intersection.is_empty or intersection.area <= 0:\n",
    "                    result['status'] = 'empty_intersection'\n",
    "                    verification_results.append(result)\n",
    "                    pbar.update(1)\n",
    "                    skipped_pairs += 1\n",
    "                    continue\n",
    "\n",
    "                # Check first file for zeros in overlap\n",
    "                with rasterio.open(tiff_path1) as src1:\n",
    "                    minx, miny, maxx, maxy = intersection.bounds\n",
    "                    window1 = from_bounds(minx, miny, maxx, maxy, src1.transform)\n",
    "\n",
    "                    # Validate window coordinates\n",
    "                    if (np.isnan(window1.col_off) or np.isnan(window1.row_off) or \n",
    "                        np.isnan(window1.width) or np.isnan(window1.height)):\n",
    "                        result['status'] = 'invalid_window_file1'\n",
    "                        verification_results.append(result)\n",
    "                        pbar.update(1)\n",
    "                        skipped_pairs += 1\n",
    "                        continue\n",
    "\n",
    "                    # Ensure window is within image bounds\n",
    "                    col_off1 = max(0, int(window1.col_off))\n",
    "                    row_off1 = max(0, int(window1.row_off))\n",
    "                    width1 = min(int(np.ceil(window1.width)), src1.width - col_off1)\n",
    "                    height1 = min(int(np.ceil(window1.height)), src1.height - row_off1)\n",
    "\n",
    "                    if width1 <= 0 or height1 <= 0:\n",
    "                        result['status'] = 'invalid_dimensions_file1'\n",
    "                        verification_results.append(result)\n",
    "                        pbar.update(1)\n",
    "                        skipped_pairs += 1\n",
    "                        continue\n",
    "\n",
    "                    data1 = src1.read(1, window=((row_off1, row_off1+height1), (col_off1, col_off1+width1)))\n",
    "                    zero_count1 = np.sum(data1 == 0)\n",
    "                    total_pixels1 = data1.size\n",
    "                    zero_percentage1 = (zero_count1 / total_pixels1) * 100\n",
    "\n",
    "                    result['file1_has_zeros'] = zero_count1 > 0\n",
    "                    result['file1_zero_percentage'] = zero_percentage1\n",
    "\n",
    "                # Check second file for zeros in overlap\n",
    "                with rasterio.open(tiff_path2) as src2:\n",
    "                    minx, miny, maxx, maxy = intersection.bounds\n",
    "                    window2 = from_bounds(minx, miny, maxx, maxy, src2.transform)\n",
    "\n",
    "                    if (np.isnan(window2.col_off) or np.isnan(window2.row_off) or \n",
    "                        np.isnan(window2.width) or np.isnan(window2.height)):\n",
    "                        result['status'] = 'invalid_window_file2' if result['status'] == 'unchecked' else 'invalid_windows_both'\n",
    "                        verification_results.append(result)\n",
    "                        pbar.update(1)\n",
    "                        skipped_pairs += 1\n",
    "                        continue\n",
    "\n",
    "                    col_off2 = max(0, int(window2.col_off))\n",
    "                    row_off2 = max(0, int(window2.row_off))\n",
    "                    width2 = min(int(np.ceil(window2.width)), src2.width - col_off2)\n",
    "                    height2 = min(int(np.ceil(window2.height)), src2.height - row_off2)\n",
    "\n",
    "                    if width2 <= 0 or height2 <= 0:\n",
    "                        result['status'] = 'invalid_dimensions_file2' if result['status'] == 'unchecked' else 'invalid_dimensions_both'\n",
    "                        verification_results.append(result)\n",
    "                        pbar.update(1)\n",
    "                        skipped_pairs += 1\n",
    "                        continue\n",
    "\n",
    "                    data2 = src2.read(1, window=((row_off2, row_off2+height2), (col_off2, col_off2+width2)))\n",
    "                    zero_count2 = np.sum(data2 == 0)\n",
    "                    total_pixels2 = data2.size\n",
    "                    zero_percentage2 = (zero_count2 / total_pixels2) * 100\n",
    "\n",
    "                    result['file2_has_zeros'] = zero_count2 > 0\n",
    "                    result['file2_zero_percentage'] = zero_percentage2\n",
    "\n",
    "                # Compute summary metrics for overlap\n",
    "                if result['status'] == 'unchecked':\n",
    "                    result['both_have_zeros'] = result['file1_has_zeros'] and result['file2_has_zeros']\n",
    "                    result['either_has_zeros'] = result['file1_has_zeros'] or result['file2_has_zeros']\n",
    "                    result['avg_zero_percentage'] = (result['file1_zero_percentage'] + result['file2_zero_percentage']) / 2\n",
    "\n",
    "                    if result['both_have_zeros']:\n",
    "                        result['status'] = 'both_have_zeros'\n",
    "                    elif result['either_has_zeros']:\n",
    "                        result['status'] = 'one_has_zeros'\n",
    "                    else:\n",
    "                        result['status'] = 'no_zeros'\n",
    "\n",
    "            except Exception as e:\n",
    "                result['status'] = f\"error: {str(e)}\"\n",
    "                skipped_pairs += 1\n",
    "\n",
    "            verification_results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    if verification_results:\n",
    "        df_verification = pd.DataFrame(verification_results)\n",
    "\n",
    "        # Print summary statistics\n",
    "        status_counts = df_verification['status'].value_counts()\n",
    "        print(\"\\nVerification results:\")\n",
    "        for status, count in status_counts.items():\n",
    "            print(f\"  {status}: {count} pairs ({count/len(df_verification)*100:.1f}%)\")\n",
    "\n",
    "        # Show statistics for valid results\n",
    "        valid_df = df_verification[df_verification['status'].isin(['both_have_zeros', 'one_has_zeros', 'no_zeros'])]\n",
    "\n",
    "        if len(valid_df) > 0:\n",
    "            both_zeros_count = valid_df['both_have_zeros'].sum()\n",
    "            either_zeros_count = valid_df['either_has_zeros'].sum()\n",
    "\n",
    "            print(f\"\\n  Pairs where both tiles have zeros in overlap: {both_zeros_count} ({both_zeros_count/len(valid_df)*100:.1f}%)\")\n",
    "            print(f\"  Pairs where at least one tile has zeros in overlap: {either_zeros_count} ({either_zeros_count/len(valid_df)*100:.1f}%)\")\n",
    "\n",
    "            avg_zero_pct = valid_df['avg_zero_percentage'].mean()\n",
    "            print(f\"  Average percentage of zeros in overlap areas: {avg_zero_pct:.1f}%\")\n",
    "\n",
    "            # Warn if any pairs have no background pixels in overlap\n",
    "            failed_verification = valid_df[valid_df['status'] == 'no_zeros']\n",
    "            if len(failed_verification) > 0:\n",
    "                print(f\"\\nWARNING: {len(failed_verification)} pairs have no background pixels in overlap regions!\")\n",
    "                print(\"\\nSample of problematic pairs:\")\n",
    "                display(failed_verification.head(5))\n",
    "\n",
    "        print(f\"\\nSkipped {skipped_pairs} pairs due to geometry or window issues\")\n",
    "\n",
    "        return df_verification\n",
    "    else:\n",
    "        print(\"No verification results\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run verification to check overlap corrections in GeoTIFF files\n",
    "df_verification = verify_overlap_corrections(overlap_df, gdf_dataset, buffer_distance=BUFFER_DISTANCE)\n",
    "\n",
    "if len(df_verification) > 0:\n",
    "    # Display pairs where the average percentage of zeros in the overlap is low (<10%)\n",
    "    low_zeros = df_verification[df_verification['avg_zero_percentage'] < 10]\n",
    "    if len(low_zeros) > 0:\n",
    "        print(\"\\nPairs with low zero percentage (<10%):\")\n",
    "        display(low_zeros[['tile_id1', 'tile_id2', 'position', 'file1_zero_percentage', 'file2_zero_percentage', 'status']])\n",
    "    \n",
    "    # Display pairs where both files have a high percentage of zeros in the overlap (>90%)\n",
    "    high_zeros = df_verification[\n",
    "        (df_verification['file1_zero_percentage'] > 90) & \n",
    "        (df_verification['file2_zero_percentage'] > 90)\n",
    "    ]\n",
    "    if len(high_zeros) > 0:\n",
    "        print(\"\\nPairs where both files have high zero percentage (>90%):\")\n",
    "        display(high_zeros[['tile_id1', 'tile_id2', 'position', 'file1_zero_percentage', 'file2_zero_percentage', 'status']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_overlap_corrections(overlap_df, df_verification, gdf_dataset, dataset_output_checks_path, zero_threshold=99.9):\n",
    "    \"\"\"\n",
    "    Visualize overlap corrections between GeoTIFF files, including mask overlays.\n",
    "\n",
    "    Args:\n",
    "        overlap_df: DataFrame with overlap information.\n",
    "        df_verification: DataFrame with verification results.\n",
    "        gdf_dataset: GeoDataFrame with GeoTIFF paths and geometries.\n",
    "        dataset_output_checks_path: Directory to save visualizations.\n",
    "        zero_threshold: Threshold (%) to consider a region as background (default 99.9).\n",
    "\n",
    "    Returns:\n",
    "        dict: Statistics about the visualizations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create output directory with timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_dir = os.path.join(dataset_output_checks_path, f\"overlap_check_{timestamp}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Counters for results\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    mask_issues = 0\n",
    "\n",
    "    def get_safe_window_data(src, intersection_bounds):\n",
    "        \"\"\"\n",
    "        Extract window data from raster, ensuring dimensions are valid.\n",
    "        \"\"\"\n",
    "        minx, miny, maxx, maxy = intersection_bounds\n",
    "        window = from_bounds(minx, miny, maxx, maxy, src.transform)\n",
    "        col_off = max(0, min(int(round(window.col_off)), src.width - 1))\n",
    "        row_off = max(0, min(int(round(window.row_off)), src.height - 1))\n",
    "        width = max(1, min(int(round(window.width)), src.width - col_off))\n",
    "        height = max(1, min(int(round(window.height)), src.height - row_off))\n",
    "        safe_window = Window(col_off, row_off, width, height)\n",
    "        data = src.read(1, window=safe_window)\n",
    "        return data, safe_window\n",
    "\n",
    "    def visualize_pair(row, output_path):\n",
    "        try:\n",
    "            tile_id1 = row['tile_id1']\n",
    "            tile_id2 = row['tile_id2']\n",
    "\n",
    "            # Find indices for the tiles\n",
    "            idx1 = gdf_dataset[gdf_dataset['tile_id'] == tile_id1].index[0]\n",
    "            idx2 = gdf_dataset[gdf_dataset['tile_id'] == tile_id2].index[0]\n",
    "\n",
    "            # Get file paths\n",
    "            tiff_path1 = gdf_dataset.loc[idx1, 'processed_img_path_tif']\n",
    "            tiff_path2 = gdf_dataset.loc[idx2, 'processed_img_path_tif']\n",
    "\n",
    "            # Check file existence\n",
    "            if not os.path.exists(tiff_path1) or not os.path.exists(tiff_path2):\n",
    "                print(f\"Files not found for {tile_id1} and {tile_id2}\")\n",
    "                return False\n",
    "\n",
    "            # Check for mask files\n",
    "            has_masks = False\n",
    "            if 'processed_mask_path_tif' in gdf_dataset.columns:\n",
    "                mask_path1 = gdf_dataset.loc[idx1, 'processed_mask_path_tif']\n",
    "                mask_path2 = gdf_dataset.loc[idx2, 'processed_mask_path_tif']\n",
    "                has_masks = (os.path.exists(mask_path1) and os.path.exists(mask_path2))\n",
    "                if not has_masks:\n",
    "                    print(f\"Warning: Mask files not found for {tile_id1} and/or {tile_id2}\")\n",
    "            else:\n",
    "                print(\"Warning: 'processed_mask_path_tif' column not found in dataset, masks will not be visualized\")\n",
    "\n",
    "            # Open raster files and analyze data\n",
    "            with rasterio.open(tiff_path1) as src1, rasterio.open(tiff_path2) as src2:\n",
    "                bounds1 = src1.bounds\n",
    "                bounds2 = src2.bounds\n",
    "\n",
    "                # Calculate intersection of bounds\n",
    "                intersection = (\n",
    "                    max(bounds1.left, bounds2.left),\n",
    "                    max(bounds1.bottom, bounds2.bottom),\n",
    "                    min(bounds1.right, bounds2.right),\n",
    "                    min(bounds1.top, bounds2.top)\n",
    "                )\n",
    "\n",
    "                # Check for valid intersection\n",
    "                if intersection[2] <= intersection[0] or intersection[3] <= intersection[1]:\n",
    "                    print(f\"No valid intersection for {tile_id1} and {tile_id2}\")\n",
    "                    return False\n",
    "\n",
    "                # Extract overlap data using safe window extraction\n",
    "                data1, window1 = get_safe_window_data(src1, intersection)\n",
    "                data2, window2 = get_safe_window_data(src2, intersection)\n",
    "\n",
    "                # Read full images for background\n",
    "                full_data1 = src1.read(1)\n",
    "                full_data2 = src2.read(1)\n",
    "\n",
    "                # Create masks for overlap regions\n",
    "                overlap_mask1 = np.zeros_like(full_data1, dtype=bool)\n",
    "                overlap_mask1[window1.row_off:window1.row_off+window1.height, \n",
    "                             window1.col_off:window1.col_off+window1.width] = True\n",
    "\n",
    "                overlap_mask2 = np.zeros_like(full_data2, dtype=bool)\n",
    "                overlap_mask2[window2.row_off:window2.row_off+window2.height, \n",
    "                             window2.col_off:window2.col_off+window2.width] = True\n",
    "\n",
    "                # Initialize mask variables\n",
    "                has_mask_conflict = False\n",
    "                mask_conflict_percentage = 0\n",
    "                mask_data1 = None\n",
    "                mask_data2 = None\n",
    "                mask_overlap1 = None\n",
    "                mask_overlap2 = None\n",
    "\n",
    "                if has_masks:\n",
    "                    try:\n",
    "                        with rasterio.open(mask_path1) as mask_src1, rasterio.open(mask_path2) as mask_src2:\n",
    "                            mask_data1 = mask_src1.read(1)\n",
    "                            mask_data2 = mask_src2.read(1)\n",
    "                            mask_overlap1 = mask_src1.read(1, window=window1)\n",
    "                            mask_overlap2 = mask_src2.read(1, window=window2)\n",
    "\n",
    "                            # Handle shape mismatches\n",
    "                            if mask_overlap1.shape != mask_overlap2.shape:\n",
    "                                print(f\"Mask shape mismatch for {tile_id1} and {tile_id2}: {mask_overlap1.shape} vs {mask_overlap2.shape}\")\n",
    "                                min_height = min(mask_overlap1.shape[0], mask_overlap2.shape[0])\n",
    "                                min_width = min(mask_overlap1.shape[1], mask_overlap2.shape[1])\n",
    "                                mask_overlap1 = mask_overlap1[:min_height, :min_width]\n",
    "                                mask_overlap2 = mask_overlap2[:min_height, :min_width]\n",
    "                                data1 = data1[:min_height, :min_width]\n",
    "                                data2 = data2[:min_height, :min_width]\n",
    "\n",
    "                            # Check for mask conflicts in overlap\n",
    "                            if mask_overlap1.shape == mask_overlap2.shape and mask_overlap1.size > 0:\n",
    "                                mask_conflict = np.logical_and(mask_overlap1 > 0, mask_overlap2 > 0)\n",
    "                                has_mask_conflict = np.any(mask_conflict)\n",
    "                                mask_conflict_percentage = np.sum(mask_conflict) / mask_conflict.size * 100\n",
    "                            else:\n",
    "                                has_mask_conflict = False\n",
    "                                mask_conflict_percentage = 0\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading mask files for {tile_id1} and {tile_id2}: {str(e)}\")\n",
    "                        has_masks = False\n",
    "                        has_mask_conflict = False\n",
    "                        mask_conflict_percentage = 0\n",
    "\n",
    "                # Ensure image data has matching dimensions\n",
    "                if data1.shape != data2.shape:\n",
    "                    min_height = min(data1.shape[0], data2.shape[0])\n",
    "                    min_width = min(data1.shape[1], data2.shape[1])\n",
    "                    data1 = data1[:min_height, :min_width]\n",
    "                    data2 = data2[:min_height, :min_width]\n",
    "\n",
    "                # Create visualization figure\n",
    "                fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "                # Row 1: Image analysis\n",
    "\n",
    "                # Plot first tile with overlap highlighted\n",
    "                axs[0, 0].imshow(full_data1, cmap='gray')\n",
    "                highlighted1 = np.zeros((*full_data1.shape, 4))\n",
    "                highlighted1[..., 0] = 1  # Red\n",
    "                highlighted1[..., 3] = np.where(overlap_mask1, 0.4, 0)\n",
    "                axs[0, 0].imshow(highlighted1)\n",
    "                axs[0, 0].set_title(f\"Tile {tile_id1}\\nZero %: {row.get('file1_zero_percentage', 'N/A'):.1f}%\")\n",
    "                axs[0, 0].axis('off')\n",
    "\n",
    "                # Plot second tile with overlap highlighted\n",
    "                axs[0, 1].imshow(full_data2, cmap='gray')\n",
    "                highlighted2 = np.zeros((*full_data2.shape, 4))\n",
    "                highlighted2[..., 2] = 1  # Blue\n",
    "                highlighted2[..., 3] = np.where(overlap_mask2, 0.4, 0)\n",
    "                axs[0, 1].imshow(highlighted2)\n",
    "                axs[0, 1].set_title(f\"Tile {tile_id2}\\nZero %: {row.get('file2_zero_percentage', 'N/A'):.1f}%\")\n",
    "                axs[0, 1].axis('off')\n",
    "\n",
    "                # Composite image of overlap regions\n",
    "                if data1.size > 0 and data2.size > 0:\n",
    "                    composite = np.zeros((data1.shape[0], data1.shape[1] * 2))\n",
    "                    composite[:, :data1.shape[1]] = data1\n",
    "                    composite[:, data1.shape[1]:] = data2\n",
    "                    axs[0, 2].imshow(composite, cmap='gray')\n",
    "                    axs[0, 2].axvline(x=data1.shape[1], color='r', linestyle='--')\n",
    "\n",
    "                    # Calculate zero percentage in overlap\n",
    "                    zeros1 = np.sum(data1 == 0) / data1.size * 100\n",
    "                    zeros2 = np.sum(data2 == 0) / data2.size * 100\n",
    "\n",
    "                    # Determine content status\n",
    "                    if zeros1 >= zero_threshold and zeros2 >= zero_threshold:\n",
    "                        content_status = \"both_background\"\n",
    "                    else:\n",
    "                        content_status = \"partial_image\"\n",
    "\n",
    "                    axs[0, 2].set_title(f\"Overlap Comparison\\nPosition: {row.get('position', 'N/A')}, Status: {content_status}\")\n",
    "                    axs[0, 2].text(data1.shape[1] * 0.5, data1.shape[0] * 0.9, \n",
    "                                  f\"{zeros1:.1f}% zeros\", ha='center', color='white',\n",
    "                                  bbox=dict(facecolor='red', alpha=0.7))\n",
    "                    axs[0, 2].text(data1.shape[1] * 1.5, data1.shape[0] * 0.9, \n",
    "                                  f\"{zeros2:.1f}% zeros\", ha='center', color='white',\n",
    "                                  bbox=dict(facecolor='blue', alpha=0.7))\n",
    "                else:\n",
    "                    axs[0, 2].text(0.5, 0.5, \"No overlap data available\", \n",
    "                                 ha='center', va='center', fontsize=12)\n",
    "                    content_status = \"no_data\"\n",
    "                    zeros1 = zeros2 = 0\n",
    "\n",
    "                axs[0, 2].axis('off')\n",
    "\n",
    "                # Row 2: Mask analysis\n",
    "\n",
    "                if has_masks and mask_data1 is not None and mask_data2 is not None:\n",
    "                    # Plot first tile with mask overlay\n",
    "                    axs[1, 0].imshow(full_data1, cmap='gray')\n",
    "                    mask_overlay1 = np.zeros((*full_data1.shape, 4))\n",
    "                    mask_overlay1[..., 0] = 1  # Red\n",
    "                    mask_overlay1[..., 3] = np.where(mask_data1 > 0, 0.5, 0)\n",
    "                    axs[1, 0].imshow(mask_overlay1)\n",
    "                    axs[1, 0].set_title(f\"Tile {tile_id1} with mask overlay\")\n",
    "                    axs[1, 0].axis('off')\n",
    "\n",
    "                    # Plot second tile with mask overlay\n",
    "                    axs[1, 1].imshow(full_data2, cmap='gray')\n",
    "                    mask_overlay2 = np.zeros((*full_data2.shape, 4))\n",
    "                    mask_overlay2[..., 2] = 1  # Blue\n",
    "                    mask_overlay2[..., 3] = np.where(mask_data2 > 0, 0.5, 0)\n",
    "                    axs[1, 1].imshow(mask_overlay2)\n",
    "                    axs[1, 1].set_title(f\"Tile {tile_id2} with mask overlay\")\n",
    "                    axs[1, 1].axis('off')\n",
    "\n",
    "                    # Composite of overlap region with masks\n",
    "                    if (mask_overlap1 is not None and mask_overlap2 is not None and \n",
    "                        data1.size > 0 and data2.size > 0):\n",
    "\n",
    "                        mask_composite = np.zeros((data1.shape[0], data1.shape[1] * 2, 4))\n",
    "                        for c in range(3):\n",
    "                            if np.max(data1) > 0:\n",
    "                                mask_composite[:, :data1.shape[1], c] = data1 / np.max(data1)\n",
    "                            if np.max(data2) > 0:\n",
    "                                mask_composite[:, data1.shape[1]:, c] = data2 / np.max(data2)\n",
    "                        mask_composite[..., 3] = 1.0\n",
    "\n",
    "                        if mask_overlap1.shape == data1.shape and mask_overlap2.shape == data2.shape:\n",
    "                            mask_overlay_left = np.zeros((data1.shape[0], data1.shape[1], 4))\n",
    "                            mask_overlay_left[..., 0] = 1.0\n",
    "                            mask_overlay_left[..., 3] = np.where(mask_overlap1 > 0, 0.5, 0)\n",
    "                            mask_overlay_right = np.zeros((data2.shape[0], data2.shape[1], 4))\n",
    "                            mask_overlay_right[..., 2] = 1.0\n",
    "                            mask_overlay_right[..., 3] = np.where(mask_overlap2 > 0, 0.5, 0)\n",
    "                            axs[1, 2].imshow(mask_composite)\n",
    "                            axs[1, 2].imshow(np.pad(mask_overlay_left, ((0,0), (0,data1.shape[1]), (0,0)), 'constant'))\n",
    "                            axs[1, 2].imshow(np.pad(mask_overlay_right, ((0,0), (data1.shape[1],0), (0,0)), 'constant'))\n",
    "                            axs[1, 2].axvline(x=data1.shape[1], color='yellow', linestyle='--')\n",
    "                            if has_mask_conflict:\n",
    "                                title = f\"Mask Overlap Comparison\\nWarning: {mask_conflict_percentage:.1f}% mask conflict!\"\n",
    "                            else:\n",
    "                                title = \"Mask Overlap Comparison\\nNo mask conflicts\"\n",
    "                            axs[1, 2].set_title(title)\n",
    "                            axs[1, 2].axis('off')\n",
    "                        else:\n",
    "                            axs[1, 2].text(0.5, 0.5, \"Mask-image dimension mismatch\", \n",
    "                                         ha='center', va='center', fontsize=12)\n",
    "                            axs[1, 2].axis('off')\n",
    "                    else:\n",
    "                        axs[1, 2].text(0.5, 0.5, \"Unable to process mask overlaps\", \n",
    "                                     ha='center', va='center', fontsize=12)\n",
    "                        axs[1, 2].axis('off')\n",
    "\n",
    "                else:\n",
    "                    # No masks available\n",
    "                    for i in range(3):\n",
    "                        axs[1, i].text(0.5, 0.5, \"No mask files found\", \n",
    "                                     ha='center', va='center', fontsize=12)\n",
    "                        axs[1, i].axis('off')\n",
    "\n",
    "                # Add main title\n",
    "                plt.suptitle(f\"Overlap Analysis: {tile_id1} and {tile_id2}\", fontsize=16, y=0.98)\n",
    "                plt.tight_layout()\n",
    "                plt.subplots_adjust(top=0.92)\n",
    "                plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "\n",
    "                # Update row with analysis info\n",
    "                row['content_status'] = content_status\n",
    "                row['zeros1'] = zeros1\n",
    "                row['zeros2'] = zeros2\n",
    "\n",
    "                if has_masks:\n",
    "                    row['has_masks'] = True\n",
    "                    row['has_mask_conflict'] = has_mask_conflict\n",
    "                    row['mask_conflict_percentage'] = mask_conflict_percentage\n",
    "                else:\n",
    "                    row['has_masks'] = False\n",
    "\n",
    "                return True, has_masks and has_mask_conflict\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing pair {tile_id1} and {tile_id2}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return False, False\n",
    "\n",
    "    # Process each verified pair\n",
    "    if len(df_verification) > 0:\n",
    "        print(f\"Processing {len(df_verification)} verified pairs...\")\n",
    "        results_df = pd.DataFrame()\n",
    "        for idx, row in tqdm(df_verification.iterrows(), total=len(df_verification)):\n",
    "            tile_id1 = row['tile_id1']\n",
    "            tile_id2 = row['tile_id2']\n",
    "            position = row.get('position', 'unknown')\n",
    "            row_copy = row.copy()\n",
    "            filename = f\"{tile_id1}_{tile_id2}_{position}.png\"\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            success, has_mask_issue = visualize_pair(row_copy, output_path)\n",
    "            if success:\n",
    "                successful += 1\n",
    "                if has_mask_issue:\n",
    "                    mask_issues += 1\n",
    "                results_df = pd.concat([results_df, pd.DataFrame([row_copy])], ignore_index=True)\n",
    "            else:\n",
    "                failed += 1\n",
    "\n",
    "        # Save results to CSV\n",
    "        results_path = os.path.join(output_dir, \"overlap_analysis_results.csv\")\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"Saved results to {results_path}\")\n",
    "\n",
    "        # Save mask issues to separate CSV if any\n",
    "        if mask_issues > 0:\n",
    "            mask_issues_df = results_df[results_df.get('has_mask_conflict', False) == True]\n",
    "            mask_issues_path = os.path.join(output_dir, \"mask_issues.csv\")\n",
    "            mask_issues_df.to_csv(mask_issues_path, index=False)\n",
    "            print(f\"Found {mask_issues} tile pairs with mask issues. Saved to {mask_issues_path}\")\n",
    "\n",
    "    print(f\"Visualization complete. Created {successful} visualizations in {output_dir}\")\n",
    "    print(f\"- Successful visualizations: {successful}\")\n",
    "    print(f\"- Failed visualizations: {failed}\")\n",
    "    print(f\"- Pairs with mask issues: {mask_issues}\")\n",
    "\n",
    "    return {\n",
    "        \"successful\": successful,\n",
    "        \"failed\": failed,\n",
    "        \"mask_issues\": mask_issues,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"results_path\": results_path if len(df_verification) > 0 else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overlap corrections between GeoTIFF files and save the results.\n",
    "visualization_results = visualize_overlap_corrections(\n",
    "    overlap_df=overlap_df,\n",
    "    df_verification=df_verification,\n",
    "    gdf_dataset=gdf_dataset,\n",
    "    dataset_output_checks_path=DATASET_OUTPUT_CHECKS_PATH,\n",
    ")\n",
    "\n",
    "# Display summary of visualization results\n",
    "print(f\"Results saved to directory: {visualization_results['output_dir']}\")\n",
    "print(f\"Successful visualizations: {visualization_results['successful']}\")\n",
    "print(f\"Failed visualizations: {visualization_results['failed']}\")\n",
    "print(f\"Tiles with mask issues: {visualization_results['mask_issues']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding images pour 1280x1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_image_dimensions(img_dir, mask_dir, target_size=(1280, 1280), overwrite=True):\n",
    "    \"\"\"\n",
    "    Pads images and masks to a target size with identical padding for each pair.\n",
    "    Overwrites original files or creates new ones, depending on the 'overwrite' flag.\n",
    "\n",
    "    Args:\n",
    "        img_dir (str): Directory containing GeoTIFF images.\n",
    "        mask_dir (str): Directory containing GeoTIFF masks.\n",
    "        target_size (tuple): Desired (width, height) for output images and masks.\n",
    "        overwrite (bool): If True, overwrite original files. If False, create new files with '_padded' suffix.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries for each modified file pair:\n",
    "            {\n",
    "                'img_file': path to modified image file,\n",
    "                'mask_file': path to modified mask file,\n",
    "                'from_size': original (width, height),\n",
    "                'to_size': new (width, height),\n",
    "                'padding': (start_x, start_y, pad_width, pad_height)\n",
    "            }\n",
    "    \"\"\"\n",
    "    # List image and mask files\n",
    "    img_files = [f for f in os.listdir(img_dir) if f.endswith(('.tif', '.tiff'))]\n",
    "    mask_files = [f for f in os.listdir(mask_dir) if f.endswith(('.tif', '.tiff'))]\n",
    "\n",
    "    # Map mask base names to file names\n",
    "    mask_map = {}\n",
    "    for mask_file in mask_files:\n",
    "        mask_basename = os.path.splitext(mask_file)[0]\n",
    "        mask_map[mask_basename] = mask_file\n",
    "\n",
    "    total_images = len(img_files)\n",
    "    resized_pairs = 0\n",
    "    errors = 0\n",
    "    skipped = 0\n",
    "    modified_files = []\n",
    "\n",
    "    print(f\"Processing {total_images} images to ensure {target_size[0]}x{target_size[1]} dimensions...\")\n",
    "\n",
    "    for img_filename in tqdm(img_files, desc=\"Standardizing images\"):\n",
    "        try:\n",
    "            img_path = os.path.join(img_dir, img_filename)\n",
    "            img_basename = os.path.splitext(img_filename)[0]\n",
    "            mask_filename = mask_map.get(img_basename)\n",
    "\n",
    "            if mask_filename:\n",
    "                mask_path = os.path.join(mask_dir, mask_filename)\n",
    "                if not os.path.exists(mask_path):\n",
    "                    print(f\"Warning: Mask file {mask_path} not found. Skipping pair.\")\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Warning: No matching mask found for {img_filename}. Skipping.\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Open image to get dimensions\n",
    "            with rasterio.open(img_path) as src:\n",
    "                height, width = src.height, src.width\n",
    "\n",
    "                # Skip if already at target size\n",
    "                if (width, height) == target_size:\n",
    "                    continue\n",
    "\n",
    "                pad_width = max(0, target_size[0] - width)\n",
    "                pad_height = max(0, target_size[1] - height)\n",
    "                start_x = pad_width // 2\n",
    "                start_y = pad_height // 2\n",
    "\n",
    "                # Skip if image is larger than target\n",
    "                if pad_width < 0 or pad_height < 0:\n",
    "                    print(f\"Warning: {img_filename} is larger than target size. Skipping pair.\")\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "\n",
    "                # Pad image\n",
    "                with rasterio.open(img_path) as src:\n",
    "                    img_data = src.read()\n",
    "                    bands = img_data.shape[0]\n",
    "                    padded_data = np.zeros((bands, target_size[1], target_size[0]), dtype=img_data.dtype)\n",
    "                    for b in range(bands):\n",
    "                        padded_data[b, start_y:start_y+height, start_x:start_x+width] = img_data[b]\n",
    "\n",
    "                    # Adjust georeferencing\n",
    "                    transform = src.transform\n",
    "                    xoff = transform.c - start_x * transform.a\n",
    "                    yoff = transform.f - start_y * transform.e\n",
    "                    new_transform = rasterio.Affine(transform.a, transform.b, xoff,\n",
    "                                                    transform.d, transform.e, yoff)\n",
    "\n",
    "                    meta = src.meta.copy()\n",
    "                    meta.update({\n",
    "                        'height': target_size[1],\n",
    "                        'width': target_size[0],\n",
    "                        'transform': new_transform\n",
    "                    })\n",
    "\n",
    "                    # Write padded image to temp file\n",
    "                    with tempfile.NamedTemporaryFile(suffix='.tif', delete=False) as tmp:\n",
    "                        tmp_path = tmp.name\n",
    "                    with rasterio.open(tmp_path, 'w', **meta) as dst:\n",
    "                        dst.write(padded_data)\n",
    "                    shutil.move(tmp_path, img_path)\n",
    "\n",
    "                # Pad mask\n",
    "                if mask_filename.lower().endswith('.png'):\n",
    "                    mask_img = Image.open(mask_path)\n",
    "                    mask_width, mask_height = mask_img.size\n",
    "                    if (mask_width, mask_height) != (width, height):\n",
    "                        print(f\"Warning: Dimensions mismatch between {img_filename} and {mask_filename}. Using GeoTIFF dimensions.\")\n",
    "\n",
    "                    new_mask = Image.new(mask_img.mode, target_size, 0)\n",
    "                    new_mask.paste(mask_img, (start_x, start_y))\n",
    "                    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n",
    "                        tmp_path = tmp.name\n",
    "                    new_mask.save(tmp_path)\n",
    "                    shutil.move(tmp_path, mask_path)\n",
    "\n",
    "                elif mask_filename.lower().endswith(('.tif', '.tiff')):\n",
    "                    with rasterio.open(mask_path) as mask_src:\n",
    "                        mask_height, mask_width = mask_src.height, mask_src.width\n",
    "                        if (mask_width, mask_height) != (width, height):\n",
    "                            print(f\"Warning: Dimensions mismatch between {img_filename} and {mask_filename}. Using image dimensions.\")\n",
    "\n",
    "                        mask_data = mask_src.read()\n",
    "                        mask_bands = mask_data.shape[0]\n",
    "                        padded_mask_data = np.zeros((mask_bands, target_size[1], target_size[0]), dtype=mask_data.dtype)\n",
    "                        for b in range(mask_bands):\n",
    "                            padded_mask_data[b, start_y:start_y+mask_height, start_x:start_x+mask_width] = mask_data[b]\n",
    "\n",
    "                        mask_meta = mask_src.meta.copy()\n",
    "                        mask_meta.update({\n",
    "                            'height': target_size[1],\n",
    "                            'width': target_size[0],\n",
    "                            'transform': new_transform\n",
    "                        })\n",
    "\n",
    "                        with tempfile.NamedTemporaryFile(suffix='.tif', delete=False) as tmp:\n",
    "                            tmp_path = tmp.name\n",
    "                        with rasterio.open(tmp_path, 'w', **mask_meta) as dst:\n",
    "                            dst.write(padded_mask_data)\n",
    "                        shutil.move(tmp_path, mask_path)\n",
    "\n",
    "                modified_files.append({\n",
    "                    'img_file': img_path,\n",
    "                    'mask_file': mask_path,\n",
    "                    'from_size': (width, height),\n",
    "                    'to_size': target_size,\n",
    "                    'padding': (start_x, start_y, pad_width, pad_height)\n",
    "                })\n",
    "                resized_pairs += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            print(f\"Error processing {img_filename}: {e}\")\n",
    "\n",
    "    print(\"Standardization complete:\")\n",
    "    print(f\"- Total images processed: {total_images}\")\n",
    "    print(f\"- Image/mask pairs resized and overwritten: {resized_pairs}\")\n",
    "    print(f\"- Pairs skipped: {skipped}\")\n",
    "    print(f\"- Errors encountered: {errors}\")\n",
    "    print(f\"- Images already at target size: {total_images - resized_pairs - skipped - errors}\")\n",
    "\n",
    "    return modified_files\n",
    "\n",
    "# Run the function and print summary\n",
    "modified_files = standardize_image_dimensions(\n",
    "    img_dir=DATASET_OUTPUT_IMG_PATH,\n",
    "    mask_dir=DATASET_OUTPUT_MASKS_PATH,\n",
    "    target_size=(1280, 1280)\n",
    ")\n",
    "\n",
    "if modified_files:\n",
    "    print(f\"\\nModified {len(modified_files)} file pairs. First 5 examples:\")\n",
    "    for i, file_info in enumerate(modified_files[:5]):\n",
    "        print(f\"{i+1}. {os.path.basename(file_info['img_file'])}: {file_info['from_size']} -> {file_info['to_size']}\")\n",
    "else:\n",
    "    print(\"\\nNo files were modified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_padding(processed_img_dir, processed_mask_dir, output_dir, show_images=False, modified_files=None, \n",
    "                   modified_sample_count=5, unmodified_sample_count=5):\n",
    "    \"\"\"\n",
    "    Verifies padding consistency for all GeoTIFF and mask pairs.\n",
    "    Provides visualizations showing padding information and formatting.\n",
    "    Saves summary and sample images to the specified output directory.\n",
    "\n",
    "    Args:\n",
    "        processed_img_dir (str): Directory containing processed GeoTIFF images.\n",
    "        processed_mask_dir (str): Directory containing processed mask files.\n",
    "        output_dir (str): Directory to save verification results and visualizations.\n",
    "        show_images (bool): If True, displays images inline. Otherwise, saves and closes them.\n",
    "        modified_files (list): List of dictionaries with information about modified files.\n",
    "        modified_sample_count (int): Number of modified samples to visualize.\n",
    "        unmodified_sample_count (int): Number of unmodified samples to visualize.\n",
    "\n",
    "    Returns:\n",
    "        dict: Summary of verification results.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # List all GeoTIFF files in the image directory\n",
    "    all_tiff_files = [f for f in os.listdir(processed_img_dir) if f.endswith(('.tif', '.tiff'))]\n",
    "    \n",
    "    if not all_tiff_files:\n",
    "        print(\"No GeoTIFF files found for verification.\")\n",
    "        return\n",
    "    \n",
    "    # Build a lookup for modified files and their padding info\n",
    "    modified_info = {}\n",
    "    if modified_files:\n",
    "        for info in modified_files:\n",
    "            filename = os.path.basename(info['img_file'])\n",
    "            modified_info[filename] = info\n",
    "    \n",
    "    modified_paths = set(modified_info.keys())\n",
    "    \n",
    "    # Separate files into modified and unmodified groups\n",
    "    modified_tiff_files = [f for f in all_tiff_files if f in modified_paths]\n",
    "    unmodified_tiff_files = [f for f in all_tiff_files if f not in modified_paths]\n",
    "    \n",
    "    print(f\"Found {len(modified_tiff_files)} modified files and {len(unmodified_tiff_files)} unmodified files\")\n",
    "    \n",
    "    # Initialize counters for verification results\n",
    "    total_files = len(all_tiff_files)\n",
    "    verified_files = 0\n",
    "    dimension_mismatches = 0\n",
    "    missing_masks = 0\n",
    "    \n",
    "    print(f\"Verifying all {total_files} image-mask pairs...\")\n",
    "    \n",
    "    # Check dimensions for all image-mask pairs\n",
    "    for img_filename in tqdm(all_tiff_files, desc=\"Verifying files\"):\n",
    "        img_path = os.path.join(processed_img_dir, img_filename)\n",
    "        img_basename = os.path.splitext(img_filename)[0]\n",
    "        \n",
    "        # Find the corresponding mask file\n",
    "        mask_filename = None\n",
    "        for ext in ['.tif', '.tiff', '.png', '.PNG']:\n",
    "            candidate_mask = img_basename + ext\n",
    "            if os.path.exists(os.path.join(processed_mask_dir, candidate_mask)):\n",
    "                mask_filename = candidate_mask\n",
    "                break\n",
    "        \n",
    "        if not mask_filename:\n",
    "            print(f\"No matching mask found for {img_filename}.\")\n",
    "            missing_masks += 1\n",
    "            continue\n",
    "        \n",
    "        mask_path = os.path.join(processed_mask_dir, mask_filename)\n",
    "        \n",
    "        try:\n",
    "            # Read GeoTIFF image dimensions\n",
    "            with rasterio.open(img_path) as src:\n",
    "                geotiff_height, geotiff_width = src.height, src.width\n",
    "            \n",
    "            # Read mask dimensions\n",
    "            if mask_filename.lower().endswith(('.tif', '.tiff')):\n",
    "                with rasterio.open(mask_path) as mask_src:\n",
    "                    mask_height, mask_width = mask_src.height, mask_src.width\n",
    "            else:\n",
    "                with Image.open(mask_path) as mask_img:\n",
    "                    mask_width, mask_height = mask_img.size\n",
    "            \n",
    "            # Compare dimensions\n",
    "            if (geotiff_height, geotiff_width) != (mask_height, mask_width):\n",
    "                print(f\"Dimension mismatch for {img_basename}: GeoTIFF {geotiff_width}x{geotiff_height}, \"\n",
    "                      f\"Mask {mask_width}x{mask_height}\")\n",
    "                dimension_mismatches += 1\n",
    "            else:\n",
    "                verified_files += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error verifying {img_filename}: {e}\")\n",
    "    \n",
    "    # Print summary of verification\n",
    "    print(\"\\nVerification Summary:\")\n",
    "    print(f\"- Total files checked: {total_files}\")\n",
    "    print(f\"- Successfully verified pairs: {verified_files}\")\n",
    "    print(f\"- Dimension mismatches: {dimension_mismatches}\")\n",
    "    print(f\"- Missing masks: {missing_masks}\")\n",
    "    \n",
    "    def visualize_sample(img_filename, sample_type):\n",
    "        \"\"\"\n",
    "        Visualizes a single image-mask pair, showing the image, mask, and overlay.\n",
    "        For modified files, highlights the original image area.\n",
    "        Saves the visualization to the output directory.\n",
    "\n",
    "        Args:\n",
    "            img_filename (str): Filename of the image to visualize.\n",
    "            sample_type (str): Label for the sample type (\"Modified\" or \"Unmodified\").\n",
    "\n",
    "        Returns:\n",
    "            bool: True if visualization was successful, False otherwise.\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(processed_img_dir, img_filename)\n",
    "        img_basename = os.path.splitext(img_filename)[0]\n",
    "        \n",
    "        # Find the corresponding mask file\n",
    "        mask_filename = None\n",
    "        for ext in ['.tif', '.tiff', '.png', '.PNG']:\n",
    "            candidate_mask = img_basename + ext\n",
    "            if os.path.exists(os.path.join(processed_mask_dir, candidate_mask)):\n",
    "                mask_filename = candidate_mask\n",
    "                break\n",
    "        \n",
    "        if not mask_filename:\n",
    "            print(f\"No mask found for {img_filename}\")\n",
    "            return False\n",
    "        \n",
    "        mask_path = os.path.join(processed_mask_dir, mask_filename)\n",
    "        \n",
    "        try:\n",
    "            # Read GeoTIFF image data\n",
    "            with rasterio.open(img_path) as src:\n",
    "                geotiff_data = src.read(1)\n",
    "            \n",
    "            # Read mask data\n",
    "            if mask_filename.lower().endswith(('.tif', '.tiff')):\n",
    "                with rasterio.open(mask_path) as mask_src:\n",
    "                    mask_data = mask_src.read(1)\n",
    "            else:\n",
    "                mask_data = np.array(Image.open(mask_path))\n",
    "                if len(mask_data.shape) == 3:\n",
    "                    mask_data = mask_data[:, :, 0]\n",
    "            \n",
    "            # Prepare dimension information for display\n",
    "            is_modified = img_filename in modified_paths\n",
    "            if is_modified:\n",
    "                padding_info = modified_info[img_filename]\n",
    "                original_size = padding_info['from_size']\n",
    "                dimension_text = f\"Original: {original_size[0]}×{original_size[1]} → Current: 1280×1280\"\n",
    "            else:\n",
    "                dimension_text = \"Original: 1280×1280 (no change needed)\"\n",
    "            \n",
    "            # Create visualization with three subplots\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "            \n",
    "            # Show GeoTIFF image\n",
    "            axes[0].imshow(geotiff_data, cmap='gray')\n",
    "            axes[0].set_title(f\"GeoTIFF: {img_filename}\")\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # Show mask image\n",
    "            axes[1].imshow(mask_data, cmap='gray')\n",
    "            axes[1].set_title(f\"Mask: {mask_filename}\")\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            # Prepare overlay for alignment check\n",
    "            if geotiff_data.max() > geotiff_data.min():\n",
    "                normalized_geotiff = (geotiff_data - geotiff_data.min()) / (geotiff_data.max() - geotiff_data.min())\n",
    "            else:\n",
    "                normalized_geotiff = np.zeros_like(geotiff_data)\n",
    "            \n",
    "            if mask_data.max() > mask_data.min():\n",
    "                normalized_mask = (mask_data - mask_data.min()) / (mask_data.max() - mask_data.min())\n",
    "            else:\n",
    "                normalized_mask = np.zeros_like(mask_data)\n",
    "            \n",
    "            overlay = np.zeros((geotiff_data.shape[0], geotiff_data.shape[1], 3))\n",
    "            overlay[:, :, 0] = normalized_geotiff  # Red channel for GeoTIFF\n",
    "            overlay[:, :, 2] = normalized_mask     # Blue channel for Mask\n",
    "            \n",
    "            axes[2].imshow(overlay)\n",
    "            axes[2].set_title(\"Overlay (purple shows alignment)\")\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            # Draw rectangle for original image area if modified\n",
    "            if is_modified:\n",
    "                padding_info = modified_info[img_filename]\n",
    "                padding = padding_info['padding']  # (start_x, start_y, pad_width, pad_height)\n",
    "                original_size = padding_info['from_size']\n",
    "                \n",
    "                start_x, start_y = padding[0], padding[1]\n",
    "                width, height = original_size\n",
    "                \n",
    "                from matplotlib.patches import Rectangle\n",
    "                rect_style = dict(linewidth=2, edgecolor='yellow', facecolor='none', linestyle='--')\n",
    "                \n",
    "                axes[0].add_patch(Rectangle((start_x, start_y), width, height, **rect_style))\n",
    "                axes[1].add_patch(Rectangle((start_x, start_y), width, height, **rect_style))\n",
    "                axes[2].add_patch(Rectangle((start_x, start_y), width, height, **rect_style))\n",
    "            \n",
    "            plt.suptitle(f\"{sample_type} Sample: {img_basename}\\n{dimension_text}\", fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            save_path = os.path.join(output_dir, f\"{sample_type.lower()}_sample_{img_basename}.png\")\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            \n",
    "            if show_images:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "            \n",
    "            print(f\"Visualization for {img_basename} ({sample_type}):\")\n",
    "            print(f\"- GeoTIFF dimensions: {geotiff_data.shape}\")\n",
    "            print(f\"- Mask dimensions: {mask_data.shape}\")\n",
    "            print(f\"- {dimension_text}\")\n",
    "            print(f\"- Verification image saved to: {save_path}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing {img_filename}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Visualize a sample of modified files\n",
    "    if modified_sample_count > 0 and modified_tiff_files:\n",
    "        print(f\"\\nGenerating visualizations for {min(modified_sample_count, len(modified_tiff_files))} modified samples...\")\n",
    "        \n",
    "        modified_samples = random.sample(modified_tiff_files, min(modified_sample_count, len(modified_tiff_files)))\n",
    "        \n",
    "        successful_visualizations = 0\n",
    "        for img_filename in modified_samples:\n",
    "            if visualize_sample(img_filename, \"Modified\"):\n",
    "                successful_visualizations += 1\n",
    "        \n",
    "        print(f\"Successfully created {successful_visualizations} modified sample visualizations\")\n",
    "    \n",
    "    # Visualize a sample of unmodified files\n",
    "    if unmodified_sample_count > 0 and unmodified_tiff_files:\n",
    "        print(f\"\\nGenerating visualizations for {min(unmodified_sample_count, len(unmodified_tiff_files))} unmodified samples...\")\n",
    "        \n",
    "        unmodified_samples = random.sample(unmodified_tiff_files, min(unmodified_sample_count, len(unmodified_tiff_files)))\n",
    "        \n",
    "        successful_visualizations = 0\n",
    "        for img_filename in unmodified_samples:\n",
    "            if visualize_sample(img_filename, \"Unmodified\"):\n",
    "                successful_visualizations += 1\n",
    "        \n",
    "        print(f\"Successfully created {successful_visualizations} unmodified sample visualizations\")\n",
    "    \n",
    "    return {\n",
    "        'total_files': total_files,\n",
    "        'verified_files': verified_files, \n",
    "        'dimension_mismatches': dimension_mismatches,\n",
    "        'missing_masks': missing_masks,\n",
    "        'modified_files_count': len(modified_tiff_files),\n",
    "        'unmodified_files_count': len(unmodified_tiff_files)\n",
    "    }\n",
    "\n",
    "# Example usage with improved graphics and summary output\n",
    "verification_results = verify_padding(\n",
    "    processed_img_dir=DATASET_OUTPUT_IMG_PATH,\n",
    "    processed_mask_dir=DATASET_OUTPUT_MASKS_PATH,\n",
    "    output_dir=DATASET_OUTPUT_CHECKS_PATH + \"/padding_verification\",\n",
    "    modified_files=modified_files,\n",
    "    modified_sample_count=5,\n",
    "    unmodified_sample_count=3,\n",
    "    show_images=False\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Verification Results:\")\n",
    "for key, value in verification_results.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove files that are 100% zeros and update the dataset accordingly\n",
    "print(\"PRE-VERIFICATION CLEANUP: REMOVING 100% ZERO FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def remove_zero_files_and_records(gdf_dataset, dataset_output_img_path, dataset_output_masks_path):\n",
    "    \"\"\"\n",
    "    Remove image and mask files that are 100% zeros, and remove corresponding dataframe records.\n",
    "    This is done before the final verification to clean up the dataset.\n",
    "    \n",
    "    Args:\n",
    "        gdf_dataset: The GeoDataFrame with file paths.\n",
    "        dataset_output_img_path: Path to processed images directory.\n",
    "        dataset_output_masks_path: Path to processed masks directory.\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned GeoDataFrame with zero-content records removed.\n",
    "    \"\"\"\n",
    "    print(f\"Starting with {len(gdf_dataset)} records\")\n",
    "    gdf_cleaned = gdf_dataset.copy()\n",
    "    zero_files_to_remove = []\n",
    "    files_deleted = []\n",
    "    records_to_remove = []\n",
    "    print(\"\\nScanning for 100% zero files...\")\n",
    "\n",
    "    for idx, row in gdf_cleaned.iterrows():\n",
    "        img_path = row.get('processed_img_path_tif')\n",
    "        mask_path = row.get('processed_mask_path_tif')\n",
    "        tile_id = row['tile_id']\n",
    "        img_all_zero = False\n",
    "        mask_all_zero = False\n",
    "        should_remove = False\n",
    "\n",
    "        # Check if image is all zeros\n",
    "        if pd.notna(img_path) and os.path.exists(img_path):\n",
    "            try:\n",
    "                with rasterio.open(img_path) as src:\n",
    "                    img_data = src.read(1)\n",
    "                    if np.all(img_data == 0):\n",
    "                        img_all_zero = True\n",
    "                        print(f\"   Image is 100% zeros: {tile_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Error reading image {tile_id}: {e}\")\n",
    "                should_remove = True\n",
    "\n",
    "        # Check if mask is all zeros\n",
    "        if pd.notna(mask_path) and os.path.exists(mask_path):\n",
    "            try:\n",
    "                with rasterio.open(mask_path) as src:\n",
    "                    mask_data = src.read(1)\n",
    "                    if np.all(mask_data == 0):\n",
    "                        mask_all_zero = True\n",
    "                        print(f\"   Mask is 100% zeros: {tile_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Error reading mask {tile_id}: {e}\")\n",
    "                should_remove = True\n",
    "\n",
    "        # Remove if image is all zeros or if there was a read error\n",
    "        if img_all_zero or should_remove:\n",
    "            zero_files_to_remove.append(tile_id)\n",
    "            records_to_remove.append(idx)\n",
    "            if img_all_zero and mask_all_zero:\n",
    "                print(f\"   Marked for removal: {tile_id} (both image and mask are 100% zeros)\")\n",
    "            elif img_all_zero and not mask_all_zero:\n",
    "                print(f\"   Marked for removal: {tile_id} (image is 100% zeros, removing both)\")\n",
    "            elif should_remove:\n",
    "                print(f\"   Marked for removal: {tile_id} (file read errors)\")\n",
    "            if pd.notna(img_path) and os.path.exists(img_path):\n",
    "                files_deleted.append(img_path)\n",
    "            if pd.notna(mask_path) and os.path.exists(mask_path):\n",
    "                files_deleted.append(mask_path)\n",
    "        else:\n",
    "            if not img_all_zero and mask_all_zero:\n",
    "                print(f\"   Keeping: {tile_id} (image has content, mask is 100% zeros - acceptable)\")\n",
    "            elif not img_all_zero and not mask_all_zero:\n",
    "                print(f\"   Keeping: {tile_id} (both image and mask have content)\")\n",
    "\n",
    "    print(f\"\\nFound {len(zero_files_to_remove)} records where image is 100% zeros\")\n",
    "    if zero_files_to_remove:\n",
    "        print(f\"   Records to remove: {zero_files_to_remove}\")\n",
    "        print(\"   Removing pairs only when image is 100% zeros (mask state doesn't matter)\")\n",
    "\n",
    "    # Delete files from disk\n",
    "    print(f\"\\nDeleting {len(files_deleted)} files from disk...\")\n",
    "    deleted_count = 0\n",
    "    delete_errors = 0\n",
    "    for file_path in files_deleted:\n",
    "        try:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "                deleted_count += 1\n",
    "                print(f\"   Deleted: {os.path.basename(file_path)}\")\n",
    "            else:\n",
    "                print(f\"   File already missing: {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error deleting {os.path.basename(file_path)}: {e}\")\n",
    "            delete_errors += 1\n",
    "\n",
    "    print(f\"   Successfully deleted {deleted_count} files\")\n",
    "    if delete_errors > 0:\n",
    "        print(f\"   Failed to delete {delete_errors} files\")\n",
    "\n",
    "    # Remove records from dataframe\n",
    "    if records_to_remove:\n",
    "        print(f\"\\nRemoving {len(records_to_remove)} records from dataframe...\")\n",
    "        gdf_cleaned = gdf_cleaned.drop(records_to_remove)\n",
    "        gdf_cleaned = gdf_cleaned.reset_index(drop=True)\n",
    "        print(f\"   Dataframe now has {len(gdf_cleaned)} records\")\n",
    "    else:\n",
    "        print(\"\\nNo records to remove from dataframe\")\n",
    "\n",
    "    # Cleanup summary\n",
    "    print(\"\\nCleanup summary:\")\n",
    "    print(f\"   Original records: {len(gdf_dataset)}\")\n",
    "    print(f\"   Records removed: {len(records_to_remove)}\")\n",
    "    print(f\"   Final records: {len(gdf_cleaned)}\")\n",
    "    print(f\"   Files deleted: {deleted_count}\")\n",
    "\n",
    "    return gdf_cleaned\n",
    "\n",
    "# Filter out records with missing files\n",
    "print(\"Filtering out records with missing files\")\n",
    "gdf_dataset_filtered = gdf_dataset.copy()\n",
    "missing_file_records = []\n",
    "for idx, row in gdf_dataset_filtered.iterrows():\n",
    "    img_path = row.get('processed_img_path_tif')\n",
    "    mask_path = row.get('processed_mask_path_tif')\n",
    "    img_exists = pd.notna(img_path) and os.path.exists(img_path)\n",
    "    mask_exists = pd.notna(mask_path) and os.path.exists(mask_path)\n",
    "    if not (img_exists and mask_exists):\n",
    "        missing_file_records.append(idx)\n",
    "\n",
    "if missing_file_records:\n",
    "    gdf_dataset_filtered = gdf_dataset_filtered.drop(missing_file_records)\n",
    "    print(f\"Removed {len(missing_file_records)} records with missing files\")\n",
    "else:\n",
    "    print(\"No records with missing files found\")\n",
    "\n",
    "print(f\"After filtering missing files: {len(gdf_dataset_filtered)} records\")\n",
    "\n",
    "# Remove 100% zero files and their records\n",
    "print(\"\\nRemoving 100% zero files and records\")\n",
    "gdf_dataset_cleaned = remove_zero_files_and_records(\n",
    "    gdf_dataset=gdf_dataset_filtered,\n",
    "    dataset_output_img_path=DATASET_OUTPUT_IMG_PATH,\n",
    "    dataset_output_masks_path=DATASET_OUTPUT_MASKS_PATH\n",
    ")\n",
    "\n",
    "# Run final verification on cleaned dataset\n",
    "print(\"\\nRunning final verification on cleaned dataset\")\n",
    "print(f\"Records going into verification: {len(gdf_dataset_cleaned)}\")\n",
    "\n",
    "def final_verification_checks_no_zeros(gdf_dataset, dataset_output_img_path, dataset_output_masks_path):\n",
    "    \"\"\"\n",
    "    Perform final verification checks on the dataset.\n",
    "    Zero-content checking is skipped as it was already done in preprocessing.\n",
    "    \"\"\"\n",
    "    print(\"RUNNING FINAL VERIFICATION CHECKS (NO ZERO CHECK)\")\n",
    "    print(\"=\" * 50)\n",
    "    gdf_dataset = gdf_dataset.copy()\n",
    "    gdf_dataset['validation_processing'] = 'ok'\n",
    "    print(f\"\\nInitialized validation_processing column for {len(gdf_dataset)} records\")\n",
    "\n",
    "    # Verify all referenced files exist\n",
    "    print(\"\\nVerifying all referenced files exist...\")\n",
    "    missing_images = []\n",
    "    missing_masks = []\n",
    "    for idx, row in gdf_dataset.iterrows():\n",
    "        img_path = row.get('processed_img_path_tif')\n",
    "        if pd.notna(img_path) and not os.path.exists(img_path):\n",
    "            missing_images.append(row['tile_id'])\n",
    "            gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "        mask_path = row.get('processed_mask_path_tif')\n",
    "        if pd.notna(mask_path) and not os.path.exists(mask_path):\n",
    "            missing_masks.append(row['tile_id'])\n",
    "            gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "    assert len(missing_images) == 0, f\"Missing image files for tiles: {missing_images[:10]}\"\n",
    "    assert len(missing_masks) == 0, f\"Missing mask files for tiles: {missing_masks[:10]}\"\n",
    "    print(f\"   All {len(gdf_dataset)} image and mask files exist\")\n",
    "\n",
    "    # Check file dimensions\n",
    "    print(\"\\nVerifying file dimensions are 1280x1280...\")\n",
    "    target_size = (1280, 1280)\n",
    "    dimension_errors = []\n",
    "    sample_size = min(10, len(gdf_dataset))\n",
    "    sample_indices = np.random.choice(len(gdf_dataset), sample_size, replace=False)\n",
    "    for idx in sample_indices:\n",
    "        row = gdf_dataset.iloc[idx]\n",
    "        tile_id = row['tile_id']\n",
    "        img_path = row.get('processed_img_path_tif')\n",
    "        if pd.notna(img_path):\n",
    "            with rasterio.open(img_path) as src:\n",
    "                if (src.width, src.height) != target_size:\n",
    "                    dimension_errors.append(f\"Image {tile_id}: {src.width}x{src.height}\")\n",
    "                    gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "        mask_path = row.get('processed_mask_path_tif')\n",
    "        if pd.notna(mask_path):\n",
    "            with rasterio.open(mask_path) as mask_src:\n",
    "                if (mask_src.width, mask_src.height) != target_size:\n",
    "                    dimension_errors.append(f\"Mask {tile_id}: {mask_src.width}x{mask_src.height}\")\n",
    "                    gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "    assert len(dimension_errors) == 0, f\"Dimension errors found: {dimension_errors}\"\n",
    "    print(f\"   Sample check: All {sample_size} files have correct 1280x1280 dimensions\")\n",
    "\n",
    "    # Validate critical columns\n",
    "    print(\"\\nValidating critical columns...\")\n",
    "    required_columns = ['tile_id', 'processed_img_path_tif', 'processed_mask_path_tif']\n",
    "    for col in required_columns:\n",
    "        assert col in gdf_dataset.columns, f\"Required column missing: {col}\"\n",
    "        null_count = gdf_dataset[col].isnull().sum()\n",
    "        assert null_count == 0, f\"Found {null_count} null values in required column: {col}\"\n",
    "    print(\"   All required columns present with no null values\")\n",
    "\n",
    "    # Validate geometries\n",
    "    print(\"\\nValidating geometries...\")\n",
    "    if 'geometry' in gdf_dataset.columns:\n",
    "        null_geoms = gdf_dataset['geometry'].isnull().sum()\n",
    "        assert null_geoms == 0, f\"Found {null_geoms} null geometries\"\n",
    "        invalid_geoms = []\n",
    "        for idx, row in gdf_dataset.iterrows():\n",
    "            if not row['geometry'].is_valid:\n",
    "                invalid_geoms.append(row['tile_id'])\n",
    "                gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "        assert len(invalid_geoms) == 0, f\"Invalid geometries found for tiles: {invalid_geoms[:10]}\"\n",
    "        print(f\"   All {len(gdf_dataset)} geometries are valid\")\n",
    "\n",
    "    # Verify directory structure\n",
    "    print(\"\\nVerifying directory structure...\")\n",
    "    assert os.path.exists(dataset_output_img_path), f\"Image directory does not exist: {dataset_output_img_path}\"\n",
    "    assert os.path.exists(dataset_output_masks_path), f\"Mask directory does not exist: {dataset_output_masks_path}\"\n",
    "    expected_img_files = set()\n",
    "    expected_mask_files = set()\n",
    "    for idx, row in gdf_dataset.iterrows():\n",
    "        img_path = row.get('processed_img_path_tif')\n",
    "        mask_path = row.get('processed_mask_path_tif')\n",
    "        if pd.notna(img_path):\n",
    "            expected_img_files.add(os.path.basename(img_path))\n",
    "        if pd.notna(mask_path):\n",
    "            expected_mask_files.add(os.path.basename(mask_path))\n",
    "    actual_img_files = set([f for f in os.listdir(dataset_output_img_path) if f.endswith(('.tif', '.tiff'))])\n",
    "    actual_mask_files = set([f for f in os.listdir(dataset_output_masks_path) if f.endswith(('.tif', '.tiff'))])\n",
    "    missing_expected_imgs = expected_img_files - actual_img_files\n",
    "    missing_expected_masks = expected_mask_files - actual_mask_files\n",
    "    assert len(missing_expected_imgs) == 0, f\"Expected image files missing: {list(missing_expected_imgs)[:5]}\"\n",
    "    assert len(missing_expected_masks) == 0, f\"Expected mask files missing: {list(missing_expected_masks)[:5]}\"\n",
    "    print(f\"   Directory structure correct: {len(expected_img_files)} expected images, {len(expected_mask_files)} expected masks\")\n",
    "\n",
    "    # Check basic data integrity\n",
    "    print(\"\\nChecking basic data integrity...\")\n",
    "    empty_rows = gdf_dataset.isnull().all(axis=1).sum()\n",
    "    assert empty_rows == 0, f\"Found {empty_rows} completely empty rows\"\n",
    "    assert len(gdf_dataset) > 0, \"Dataset is empty\"\n",
    "    memory_mb = gdf_dataset.memory_usage(deep=True).sum() / 1024**2\n",
    "    assert memory_mb < 1000, f\"Dataset unusually large: {memory_mb:.1f} MB\"\n",
    "    print(f\"   Dataset integrity OK: {len(gdf_dataset)} records, {memory_mb:.1f} MB\")\n",
    "\n",
    "    # Zero content check skipped (already done)\n",
    "    print(\"\\nZero content check: SKIPPED (already done in preprocessing)\")\n",
    "\n",
    "    # Check for duplicate images\n",
    "    print(\"\\nChecking for duplicate images...\")\n",
    "    image_hashes = {}\n",
    "    duplicate_groups = []\n",
    "    for idx, row in gdf_dataset.iterrows():\n",
    "        img_path = row.get('processed_img_path_tif')\n",
    "        tile_id = row['tile_id']\n",
    "        if pd.notna(img_path) and os.path.exists(img_path):\n",
    "            try:\n",
    "                with rasterio.open(img_path) as src:\n",
    "                    img_data = src.read()\n",
    "                    img_hash = hash(img_data.tobytes())\n",
    "                    if img_hash in image_hashes:\n",
    "                        if len(image_hashes[img_hash]) == 1:\n",
    "                            duplicate_groups.append(image_hashes[img_hash] + [tile_id])\n",
    "                        else:\n",
    "                            for group in duplicate_groups:\n",
    "                                if image_hashes[img_hash][0] in group:\n",
    "                                    group.append(tile_id)\n",
    "                                    break\n",
    "                        image_hashes[img_hash].append(tile_id)\n",
    "                    else:\n",
    "                        image_hashes[img_hash] = [tile_id]\n",
    "            except Exception as e:\n",
    "                print(f\"   Error processing image {tile_id}: {e}\")\n",
    "                gdf_dataset.loc[idx, 'validation_processing'] = 'ko'\n",
    "    duplicate_count = 0\n",
    "    for group in duplicate_groups:\n",
    "        for tile_id in group:\n",
    "            tile_idx = gdf_dataset[gdf_dataset['tile_id'] == tile_id].index[0]\n",
    "            gdf_dataset.loc[tile_idx, 'validation_processing'] = 'ko'\n",
    "            duplicate_count += 1\n",
    "    print(f\"   Found {len(duplicate_groups)} duplicate groups affecting {duplicate_count} files\")\n",
    "    if duplicate_groups:\n",
    "        print(f\"   Example duplicate group: {duplicate_groups[0]}\")\n",
    "\n",
    "    # Validation summary\n",
    "    print(\"\\nValidation summary...\")\n",
    "    validation_counts = gdf_dataset['validation_processing'].value_counts()\n",
    "    ok_count = validation_counts.get('ok', 0)\n",
    "    ko_count = validation_counts.get('ko', 0)\n",
    "    print(f\"   Records marked 'ok': {ok_count}\")\n",
    "    print(f\"   Records marked 'ko': {ko_count}\")\n",
    "    print(f\"   Success rate: {ok_count/len(gdf_dataset)*100:.1f}%\")\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ALL VERIFICATION CHECKS PASSED\")\n",
    "    print(f\"Dataset validated: {ok_count} OK, {ko_count} KO\")\n",
    "    print(\"Dataset is ready for saving\")\n",
    "    print(\"=\" * 50)\n",
    "    return gdf_dataset\n",
    "\n",
    "# Run the verification on the cleaned dataset\n",
    "gdf_dataset_final = final_verification_checks_no_zeros(\n",
    "    gdf_dataset=gdf_dataset_cleaned,\n",
    "    dataset_output_img_path=DATASET_OUTPUT_IMG_PATH,\n",
    "    dataset_output_masks_path=DATASET_OUTPUT_MASKS_PATH\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL RESULTS AFTER CLEANUP AND VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Final validation counts:\")\n",
    "print(gdf_dataset_final['validation_processing'].value_counts())\n",
    "print(f\"\\nDataset ready for saving: {len(gdf_dataset_final)} total records\")\n",
    "print(f\"High-quality records: {(gdf_dataset_final['validation_processing'] == 'ok').sum()}\")\n",
    "\n",
    "assert (gdf_dataset_final['validation_processing'] == 'ko').sum() == 0, \"There are still 'ko' records in the dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save verification and dataset results to parquet files for future use\n",
    "df_verification.to_parquet(VERIFICATION_OUTPUT_PARQUET_PATH, index=False)\n",
    "gdf_dataset.to_parquet(DATASET_OUTPUT_PARQUET_PATH, index=False)\n",
    "gdf_dataset_final.to_parquet(DATASET_FINAL_OUTPUT_PARQUET_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
