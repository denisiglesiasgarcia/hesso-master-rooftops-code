{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60fee028",
   "metadata": {},
   "source": [
    "# Generate slurm sbatch files for CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bfe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from training_configs import CONFIGS\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3765f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "todays_date_no_hours = time.strftime(\"%Y%m%d\")\n",
    "\n",
    "OUTPUT_DIR = \"configs_bamboo/\" + todays_date\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9727fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_slurm_file(config_name, config_dict, fold, partition, time_batch, gpu, batch_size, size, output_training_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Create a SLURM batch file for a given configuration.\n",
    "    \n",
    "    Args:\n",
    "        config_name: Name of the configuration (key from CONFIGS)\n",
    "        config_dict: Configuration dictionary\n",
    "        time_batch: Time batch for SLURM job (in days)\n",
    "        gpu: GPU specification for SLURM\n",
    "        output_dir: Base directory to save the SLURM files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create subdirectories based on architecture and backbone_size\n",
    "    arch_dir = os.path.join(output_dir, config_dict['architecture'])\n",
    "    final_dir = os.path.join(arch_dir, config_dict['backbone_size'])\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    \n",
    "    # Add fold to the configuration name\n",
    "    job_name = f\"{size}_{config_name}_fold_{fold}\"\n",
    "\n",
    "    # SLURM file template\n",
    "    slurm_template = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --partition={partition}\n",
    "#SBATCH --time={time_batch}\n",
    "#SBATCH {gpu}\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem=64G\n",
    "#SBATCH --output=%x-%J.out\n",
    "#SBATCH --error=%x-%J.err\n",
    "#SBATCH --requeue\n",
    "\n",
    "# ============================================================================\n",
    "# SLURM Job with Auto-Resume Functionality\n",
    "# Config: {config_name} | Fold: {fold} | Size: {size}\n",
    "# ============================================================================\n",
    "\n",
    "# Configuration variables\n",
    "CONFIG_NAME=\"{config_name}\"\n",
    "CONFIG_NAME_BACKBONE=\"{config_dict['architecture']}_{config_dict['backbone']}_{config_dict['encoder_weights']}\"\n",
    "FOLD={fold}\n",
    "OUTPUT_DIR=\"{output_training_dir}\"\n",
    "BATCH_SIZE={batch_size}\n",
    "SIZE=\"{size}\"\n",
    "\n",
    "# Set strict error handling\n",
    "set -e\n",
    "set -u\n",
    "\n",
    "# Function to handle early termination signals\n",
    "cleanup() {{\n",
    "    echo \"$(date): Job received termination signal\"\n",
    "    echo \"Attempting graceful shutdown...\"\n",
    "    if [[ ! -z \"${{TRAIN_PID:-}}\" ]]; then\n",
    "        kill -TERM $TRAIN_PID 2>/dev/null || true\n",
    "        wait $TRAIN_PID 2>/dev/null || true\n",
    "    fi\n",
    "    exit 0\n",
    "}}\n",
    "\n",
    "trap cleanup SIGUSR1 SIGTERM SIGINT\n",
    "\n",
    "# Load Anaconda module\n",
    "source /opt/ebsofts/Anaconda3/2024.02-1/etc/profile.d/conda.sh\n",
    "\n",
    "# Activate conda environment\n",
    "conda activate myenv\n",
    "\n",
    "# Look for existing checkpoints\n",
    "echo \"Checking for Existing Checkpoints...\"\n",
    "CHECKPOINT_PATTERN=\"${{OUTPUT_DIR}}/${{CONFIG_NAME_BACKBONE}}/single_fold_${{FOLD}}_*/01_model_output/checkpoint_epoch_*_fold_${{FOLD}}.pth\"\n",
    "RESUME_ARG=\"\"\n",
    "\n",
    "if ls $CHECKPOINT_PATTERN 2>/dev/null | head -1 >/dev/null; then\n",
    "    LATEST_CHECKPOINT=$(ls -t $CHECKPOINT_PATTERN 2>/dev/null | head -1)\n",
    "    if [[ -f \"$LATEST_CHECKPOINT\" ]]; then\n",
    "        EPOCH_NUM=$(basename \"$LATEST_CHECKPOINT\" | sed 's/checkpoint_epoch_\\\\([0-9]*\\\\)_fold_.*/\\\\1/')\n",
    "        echo \"✓ Found checkpoint at epoch $EPOCH_NUM: $LATEST_CHECKPOINT\"\n",
    "        RESUME_ARG=\"--resume $LATEST_CHECKPOINT\"\n",
    "    fi\n",
    "else\n",
    "    echo \"✓ No checkpoints found - starting fresh training\"\n",
    "fi\n",
    "\n",
    "# Create output directory\n",
    "mkdir -p \"$OUTPUT_DIR\"\n",
    "\n",
    "# Start training\n",
    "python3 02_training_v21.py \\\\\n",
    "    --config \"$CONFIG_NAME\" \\\\\n",
    "    --fold \"$FOLD\" \\\\\n",
    "    --gpu 0 \\\\\n",
    "    --output_dir \"$OUTPUT_DIR\" \\\\\n",
    "    --batch_size \"$BATCH_SIZE\" \\\\\n",
    "    --no_graphics_every_epoch \\\\\n",
    "    --verbose_logging \\\\\n",
    "    $RESUME_ARG \\\\\n",
    "    2>&1 | tee \"{job_name}-${{SLURM_JOB_ID}}.log\" &\n",
    "\n",
    "TRAIN_PID=$!\n",
    "wait $TRAIN_PID\n",
    "TRAIN_EXIT_CODE=$?\n",
    "\n",
    "echo \"============================================================================\"\n",
    "echo \"Training Completed: $(date) | Exit Code: $TRAIN_EXIT_CODE\"\n",
    "if [[ $TRAIN_EXIT_CODE -eq 0 ]]; then\n",
    "    echo \"Status: ✓ SUCCESS\"\n",
    "else\n",
    "    echo \"Status: ✗ FAILED - Check log: {job_name}-${{SLURM_JOB_ID}}.log\"\n",
    "fi\n",
    "echo \"============================================================================\"\n",
    "\n",
    "exit $TRAIN_EXIT_CODE\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Write to file\n",
    "    filename = os.path.join(final_dir, f\"{config_name}_fold_{fold}.sh\")\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(slurm_template)\n",
    "    \n",
    "    print(f\"Created: {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of configurations in CONFIGS\n",
    "print(f\"Total configurations: {len(CONFIGS)}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Number of configurations per architecture\n",
    "for arch in set(config['architecture'] for config in CONFIGS.values()):\n",
    "    count = sum(1 for config in CONFIGS.values() if config['architecture'] == arch)\n",
    "    print(f\"Architecture {arch}: {count} configurations\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the backbones per size\n",
    "backbone_sizes = {\n",
    "    # 0-25M PARAMETER RANGE - Lightweight Champions\n",
    "    'small': [\n",
    "        'timm-efficientnet-b3',\n",
    "        'tu-efficientvit_b2.r224_in1k',\n",
    "        'tu-fastvit_t8.apple_in1k',\n",
    "        'tu-repvit_m1.dist_in1k',\n",
    "        'tu-regnety_032.ra_in1k'\n",
    "    ],\n",
    "    \n",
    "    # 25M-50M PARAMETER RANGE - Sweet Spot Performance\n",
    "    'medium': [\n",
    "        'tu-mambaout_small',\n",
    "        'tu-efficientnetv2_rw_s.ra2_in1k',\n",
    "        'tu-regnety_080.ra3_in1k',\n",
    "        'timm-res2net101_26w_4s',\n",
    "        'resnext50_32x4d'\n",
    "    ],\n",
    "    \n",
    "    # 50M-100M PARAMETER RANGE - High Performance\n",
    "    'large': [\n",
    "        'tu-mambaout_base',\n",
    "        'tu-efficientnetv2_rw_m.agc_in1k',\n",
    "        'timm-resnest200e',\n",
    "        'resnext101_32x8d',\n",
    "        'timm-efficientnet-b5'\n",
    "    ],\n",
    "    \n",
    "    # 100M+ PARAMETER RANGE - Foundation Model Territory\n",
    "    'huge': [\n",
    "        'tu-tf_efficientnetv2_xl.in21k_ft_in1k'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# add backbone_sizes to CONFIGS\n",
    "for config in CONFIGS.values():\n",
    "    config['backbone_size'] = 'unknown'\n",
    "    for size, backbones in backbone_sizes.items():\n",
    "        if config['backbone'] in backbones:\n",
    "            config['backbone_size'] = size\n",
    "            break\n",
    "\n",
    "# assert that all backbones are classified\n",
    "for config in CONFIGS.values():\n",
    "    assert config['backbone_size'] != 'unknown', f\"Backbone {config['backbone']} not classified!\"\n",
    "\n",
    "# Show me the backbone per architecture and backbone_size\n",
    "print(\"\\nBackbones per architecture and backbone_size:\")\n",
    "for arch in set(config['architecture'] for config in CONFIGS.values()):\n",
    "    print(f\"\\nArchitecture: {arch}\")\n",
    "    for size in ['small', 'medium', 'large', 'huge']:\n",
    "        backbones = set(config['backbone'] for config in CONFIGS.values() \n",
    "                       if config['architecture'] == arch and config['backbone_size'] == size)\n",
    "        if backbones:\n",
    "            print(f\"  {size.capitalize()}:\")\n",
    "            for backbone in sorted(backbones):\n",
    "                print(f\"    {backbone}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f07a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SLURM files for each configuration\n",
    "print(f\"Using output directory: {OUTPUT_DIR}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "created_files = []\n",
    "\n",
    "for config_name, config_dict in CONFIGS.items():\n",
    "    # Determine time and GPU requirements based on backbone size\n",
    "    if config_dict['architecture'] == 'deeplabv3':\n",
    "        partition = \"shared-gpu\"\n",
    "        time_batch = \"12:00:00\"\n",
    "        gpu = \"--gres=gpu:1,VramPerGpu:30G\"\n",
    "        batch_size = int(config_dict['batch_size'] * 2)\n",
    "        size = config_dict['backbone_size']\n",
    "        architecture = config_dict['architecture']\n",
    "        output_training_dir = f\"01_training_{size}_{architecture}_\" + todays_date_no_hours\n",
    "    elif config_dict['backbone_size'] in ['small']:\n",
    "        partition = \"shared-gpu\"\n",
    "        time_batch = \"12:00:00\"\n",
    "        gpu = \"--gres=gpu:1,VramPerGpu:24G\"\n",
    "        batch_size = config_dict['batch_size']\n",
    "        size = \"small\"\n",
    "        architecture = config_dict['architecture']\n",
    "        output_training_dir = f\"01_training_{size}_{architecture}_\" + todays_date_no_hours\n",
    "    elif config_dict['backbone_size'] in ['medium']:\n",
    "        partition = \"shared-gpu\"\n",
    "        time_batch = \"12:00:00\"\n",
    "        gpu = \"--gres=gpu:1,VramPerGpu:24G\"\n",
    "        batch_size = config_dict['batch_size']\n",
    "        size = \"medium\"\n",
    "        architecture = config_dict['architecture']\n",
    "        output_training_dir = f\"01_training_{size}_{architecture}_\" + todays_date_no_hours\n",
    "    elif config_dict['backbone_size'] in [\"large\"]:\n",
    "        partition = \"shared-gpu\"\n",
    "        time_batch = \"12:00:00\"\n",
    "        gpu = \"--gres=gpu:1,VramPerGpu:24G\"\n",
    "        batch_size = config_dict['batch_size']\n",
    "        size = \"large\"\n",
    "        architecture = config_dict['architecture']\n",
    "        output_training_dir = f\"01_training_{size}_{architecture}_\" + todays_date_no_hours\n",
    "    elif config_dict['backbone_size'] in ['huge']:\n",
    "        partition = \"public-gpu\"\n",
    "        time_batch = \"2-00:00:00\"\n",
    "        gpu = \"--gpus=nvidia_a100_80gb_pcie:1\"\n",
    "        batch_size = config_dict['batch_size']\n",
    "        size = \"huge\"\n",
    "        output_training_dir = f\"01_training_{size}_{architecture}_\" + todays_date_no_hours\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backbone size: {config_dict['backbone_size']} for config {config_name}\")\n",
    "    \n",
    "    # Create SLURM files for each fold (0-4)\n",
    "    for fold in range(5):\n",
    "        filename = create_slurm_file(\n",
    "            config_name=config_name, \n",
    "            config_dict=config_dict,\n",
    "            fold=fold,\n",
    "            partition=partition, \n",
    "            time_batch=time_batch, \n",
    "            gpu=gpu,\n",
    "            batch_size=batch_size,\n",
    "            size=size,\n",
    "            output_training_dir=output_training_dir,\n",
    "            output_dir=OUTPUT_DIR\n",
    "        )\n",
    "        created_files.append(filename)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Verify all SLURM files were created correctly\n",
    "verified_configs = set()\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith('.sh') and 'submit_' not in file:\n",
    "            # Extract config name from filename (remove _fold_X.sh suffix)\n",
    "            # Example: \"config_name_fold_0.sh\" -> \"config_name\"\n",
    "            config_name = file.replace('.sh', '').rsplit('_fold_', 1)[0]\n",
    "            verified_configs.add(config_name)\n",
    "            # Print the full path for verification\n",
    "            full_path = os.path.join(root, file)\n",
    "            print(f\"Verified: {full_path}\")\n",
    "\n",
    "# Convert to set for comparison\n",
    "expected_configs = set(CONFIGS.keys())\n",
    "\n",
    "assert verified_configs == expected_configs, f\"Mismatch in created files. Missing: {expected_configs - verified_configs}, Extra: {verified_configs - expected_configs}\"\n",
    "print(f\"\\nSuccessfully created {len(created_files)} SLURM files for all configurations!\")\n",
    "print(f\"All files are organized under: {OUTPUT_DIR}/architecture/backbone_size/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create submission scripts for each category\n",
    "def create_category_submission_scripts():\n",
    "    \"\"\"Create submission scripts for each backbone size category with fold-by-fold delays.\"\"\"\n",
    "    \n",
    "    categories = ['small', 'medium', 'large', 'huge']\n",
    "    \n",
    "    # Get the directory name for relative paths\n",
    "    output_dir_name = os.path.basename(OUTPUT_DIR)\n",
    "    \n",
    "    for category in categories:\n",
    "        category_configs = [name for name, config in CONFIGS.items() \n",
    "                          if config['backbone_size'] == category]\n",
    "        \n",
    "        if not category_configs:\n",
    "            continue\n",
    "        \n",
    "        total_jobs = len(category_configs) * 5  # 5 folds per config\n",
    "        \n",
    "        script_content = f\"\"\"#!/bin/bash\n",
    "# Submit all {category} models with fold-by-fold delays\n",
    "\n",
    "echo \"Submitting {total_jobs} {category} model jobs ({len(category_configs)} configs × 5 folds)...\"\n",
    "echo \"Strategy: Submit all fold 0, wait 1min, submit all fold 1, etc.\"\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Submit fold by fold with delays\n",
    "        for fold in range(5):\n",
    "            script_content += f\"echo 'Submitting all FOLD {fold} jobs for {category} models...'\\n\"\n",
    "            n = 30\n",
    "            # Submit all configs for this fold\n",
    "            if fold == 0:\n",
    "                for config_name in category_configs[:n]:\n",
    "                    config = CONFIGS[config_name]\n",
    "                    arch = config['architecture']\n",
    "                    script_path = f\"configs_bamboo/{output_dir_name}/{arch}/{category}/{config_name}_fold_{fold}.sh\"\n",
    "                    script_content += f\"sbatch {script_path}\\n\"\n",
    "                    script_content += \"sleep 30\\n\"\n",
    "                for config_name in category_configs[n:]:\n",
    "                    config = CONFIGS[config_name]\n",
    "                    arch = config['architecture']\n",
    "                    script_path = f\"configs_bamboo/{output_dir_name}/{arch}/{category}/{config_name}_fold_{fold}.sh\"\n",
    "                    script_content += f\"sbatch {script_path}\\n\"\n",
    "            else:\n",
    "                for config_name in category_configs:\n",
    "                    config = CONFIGS[config_name]\n",
    "                    arch = config['architecture']\n",
    "                    script_path = f\"configs_bamboo/{output_dir_name}/{arch}/{category}/{config_name}_fold_{fold}.sh\"\n",
    "                    script_content += f\"sbatch {script_path}\\n\"\n",
    "            \n",
    "            script_content += f\"echo 'Submitted {len(category_configs)} jobs for fold {fold}'\\n\"\n",
    "            \n",
    "            # Add delay except after the last fold\n",
    "            if fold < 4:\n",
    "                script_content += f\"echo 'Waiting 1 minute before submitting fold {fold + 1}...'\\n\"\n",
    "                script_content += \"sleep 60\\n\"\n",
    "            \n",
    "            script_content += \"\\n\"\n",
    "        \n",
    "        script_content += f'echo \"All {category} model jobs submitted! ({total_jobs} jobs total)\"'\n",
    "        \n",
    "        script_filename = os.path.join(OUTPUT_DIR, f\"submit_{category}_models_all_folds.sh\")\n",
    "        with open(script_filename, 'w') as f:\n",
    "            f.write(script_content)\n",
    "        \n",
    "        # Make it executable\n",
    "        os.chmod(script_filename, 0o755)\n",
    "        print(f\"Created: {script_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create architecture-size specific submission scripts\n",
    "def create_arch_size_submission_scripts():\n",
    "    \"\"\"Create submission scripts for each architecture-size combination with fold-by-fold delays.\"\"\"\n",
    "    \n",
    "    # Get all unique architecture-size combinations\n",
    "    arch_size_combinations = {}\n",
    "    for config_name, config in CONFIGS.items():\n",
    "        arch = config['architecture']\n",
    "        size = config['backbone_size']\n",
    "        key = f\"{arch}_{size}\"\n",
    "        \n",
    "        if key not in arch_size_combinations:\n",
    "            arch_size_combinations[key] = []\n",
    "        arch_size_combinations[key].append(config_name)\n",
    "    \n",
    "    print(f\"\\nCreating {len(arch_size_combinations)} architecture-size specific submission scripts...\")\n",
    "    \n",
    "    # Get the directory name for relative paths\n",
    "    output_dir_name = os.path.basename(OUTPUT_DIR)\n",
    "    \n",
    "    for combo_key, config_names in arch_size_combinations.items():\n",
    "        arch, size = combo_key.split('_')\n",
    "        \n",
    "        total_jobs = len(config_names) * 5  # 5 folds per config\n",
    "        \n",
    "        script_content = f\"\"\"#!/bin/bash\n",
    "# Submit all {size} {arch} models with fold-by-fold delays\n",
    "\n",
    "echo \"Submitting {total_jobs} {size} {arch} model jobs ({len(config_names)} configs × 5 folds)...\"\n",
    "echo \"Strategy: Submit all fold 0, wait 1min, submit all fold 1, etc.\"\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Submit fold by fold with delays\n",
    "        for fold in range(5):\n",
    "            script_content += f\"echo 'Submitting all FOLD {fold} jobs for {size} {arch} models...'\\n\"\n",
    "            \n",
    "            # Submit all configs for this fold\n",
    "            for config_name in sorted(config_names):\n",
    "                script_path = f\"configs_bamboo/{output_dir_name}/{arch}/{size}/{config_name}_fold_{fold}.sh\"\n",
    "                script_content += f\"sbatch {script_path}\\n\"\n",
    "            \n",
    "            script_content += f\"echo 'Submitted {len(config_names)} jobs for fold {fold}'\\n\"\n",
    "            \n",
    "            # Add delay except after the last fold\n",
    "            if fold < 4:\n",
    "                script_content += f\"echo 'Waiting 1 minute before submitting fold {fold + 1}...'\\n\"\n",
    "                script_content += \"sleep 60\\n\"\n",
    "            \n",
    "            script_content += \"\\n\"\n",
    "        \n",
    "        script_content += f'echo \"All {size} {arch} model jobs submitted! ({total_jobs} jobs total)\"'\n",
    "        \n",
    "        script_filename = os.path.join(OUTPUT_DIR, f\"submit_{size}_{arch}_models.sh\")\n",
    "        with open(script_filename, 'w') as f:\n",
    "            f.write(script_content)\n",
    "        \n",
    "        # Make it executable\n",
    "        os.chmod(script_filename, 0o755)\n",
    "        print(f\"Created: {script_filename} ({len(config_names)} configs, {total_jobs} jobs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a04eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRe-creating submission scripts with fold-by-fold delays...\")\n",
    "create_category_submission_scripts()\n",
    "create_arch_size_submission_scripts()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPDATED SUBMISSION SCRIPTS WITH FOLD-BY-FOLD DELAYS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNow each script will:\")\n",
    "print(\"1. Submit all fold 0 jobs\")\n",
    "print(\"2. Wait 1 minute\") \n",
    "print(\"3. Submit all fold 1 jobs\")\n",
    "print(\"4. Wait 1 minute\")\n",
    "print(\"5. Continue for folds 2, 3, 4\")\n",
    "print(\"\\nThis spreads the computational load more evenly over time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all SLURM files organized by architecture and size\n",
    "slurm_files_by_arch_size = {}\n",
    "\n",
    "# Walk through the directory structure\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith('.sh') and 'submit_' not in file:  # Exclude submission scripts\n",
    "            # Extract architecture and size from path\n",
    "            path_parts = root.replace(OUTPUT_DIR, '').strip('/').split('/')\n",
    "            if len(path_parts) >= 2:\n",
    "                arch = path_parts[0]\n",
    "                size = path_parts[1]\n",
    "                key = f\"{arch}_{size}\"\n",
    "                \n",
    "                if key not in slurm_files_by_arch_size:\n",
    "                    slurm_files_by_arch_size[key] = []\n",
    "                \n",
    "                slurm_files_by_arch_size[key].append(os.path.join(root, file))\n",
    "\n",
    "# Print sbatch commands for each architecture and size combination\n",
    "print(\"\\nSbatch commands per architecture and size:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for key, files in slurm_files_by_arch_size.items():\n",
    "    arch, size = key.split('_')\n",
    "    print(f\"\\n{arch.upper()} - {size.upper()}:\")\n",
    "    for file in sorted(files):  # Sort for better readability\n",
    "        print(f\"sbatch {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary of created submission scripts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION SCRIPTS CREATED:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. BY SIZE ONLY (submit all architectures of a given size):\")\n",
    "for size in ['small', 'medium', 'large', 'huge']:\n",
    "    script_path = os.path.join(OUTPUT_DIR, f\"submit_{size}_models_all_folds.sh\")\n",
    "    if os.path.exists(script_path):\n",
    "        count = sum(1 for name, config in CONFIGS.items() if config['backbone_size'] == size)\n",
    "        print(f\"   bash submit_{size}_models_all_folds.sh  # {count} configs × 5 folds = {count*5} jobs\")\n",
    "\n",
    "print(\"\\n2. BY ARCHITECTURE + SIZE (submit specific combinations):\")\n",
    "arch_size_scripts = []\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    for file in files:\n",
    "        if file.startswith('submit_') and file.endswith('_models.sh') and 'all_folds' not in file:\n",
    "            arch_size_scripts.append(file)\n",
    "\n",
    "for script in sorted(arch_size_scripts):\n",
    "    # Extract size and arch from filename like \"submit_huge_unet_models.sh\"\n",
    "    parts = script.replace('submit_', '').replace('_models.sh', '').split('_')\n",
    "    if len(parts) >= 2:\n",
    "        size, arch = parts[0], '_'.join(parts[1:])\n",
    "        count = sum(1 for name, config in CONFIGS.items() \n",
    "                   if config['backbone_size'] == size and config['architecture'] == arch)\n",
    "        print(f\"   bash {script}  # {count} configs × 5 folds = {count*5} jobs\")\n",
    "\n",
    "print(f\"\\nAll scripts are located in: {OUTPUT_DIR}/\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"   cd \" + OUTPUT_DIR)\n",
    "print(\"   bash submit_huge_unet_models.sh      # Submit only huge UNet models\")\n",
    "print(\"   bash submit_small_segformer_models.sh # Submit only small Segformer models\") \n",
    "print(\"   bash submit_large_models_all_folds.sh # Submit ALL large models (any architecture)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_monitoring_script(output_dir):\n",
    "    \"\"\"Create a comprehensive job monitoring script.\"\"\"\n",
    "    script_content = \"\"\"#!/bin/bash\n",
    "# Clean terminal-friendly job monitoring script\n",
    "\n",
    "clear\n",
    "echo \"==================================================\"\n",
    "echo \"           TRAINING MONITOR $(uname -n)\"\n",
    "echo \"==================================================\"\n",
    "echo \"Date: $(date '+%Y-%m-%d %H:%M:%S')\"\n",
    "echo \"User: $USER\"\n",
    "echo \"\"\n",
    "\n",
    "# Current jobs\n",
    "echo \"CURRENT JOBS:\"\n",
    "echo \"--------------------------------------------------\"\n",
    "RUNNING=$(squeue -u $USER -t RUNNING -h | wc -l)\n",
    "PENDING=$(squeue -u $USER -t PENDING -h | wc -l)\n",
    "\n",
    "if [ \"$RUNNING\" -eq 0 ] && [ \"$PENDING\" -eq 0 ]; then\n",
    "    echo \"  No jobs in queue\"\n",
    "else\n",
    "    if [ \"$RUNNING\" -gt 0 ]; then\n",
    "        echo \"  Running: $RUNNING jobs\"\n",
    "        echo \"\"\n",
    "        # Fixed formatting without justification flags\n",
    "        printf \"  %-25s %-8s %-10s %-10s\\\\n\" \"JOB NAME\" \"STATE\" \"TIME\" \"TIME LEFT\"\n",
    "        echo \"  --------------------------------------------------------------\"\n",
    "        squeue -u $USER -t RUNNING --format=\"%25j|%8T|%10M|%10L\" --noheader | head -8 | awk -F '|' '{\n",
    "            gsub(/^[ \\t]+|[ \\t]+$/, \"\", $1);\n",
    "            gsub(/^[ \\t]+|[ \\t]+$/, \"\", $2);\n",
    "            gsub(/^[ \\t]+|[ \\t]+$/, \"\", $3);\n",
    "            gsub(/^[ \\t]+|[ \\t]+$/, \"\", $4);\n",
    "            printf \"  %-25s %-8s %-10s %-10s\\\\n\", $1, $2, $3, $4;\n",
    "        }'\n",
    "        if [ \"$RUNNING\" -gt 8 ]; then\n",
    "            echo \"  ... and $((RUNNING - 8)) more running jobs\"\n",
    "        fi\n",
    "        echo \"\"\n",
    "    fi\n",
    "    \n",
    "    if [ \"$PENDING\" -gt 0 ]; then\n",
    "        echo \"  Pending: $PENDING jobs\"\n",
    "        # Fixed formatting without justification flags\n",
    "        squeue -u $USER -t PENDING --format=\"%25j|%8T|%15R\" --noheader | head -5 | awk -F '|' '{\n",
    "            gsub(/^[ \\t]+|[ \\t]+$/, \"\", $1);\n",
    "            gsub(/^[ \\t]+|[ \\t]+$/, \"\", $2);\n",
    "            gsub(/^[ \\t]+|[ \\t]+$/, \"\", $3);\n",
    "            printf \"  %-25s %-8s %-15s\\\\n\", $1, $2, $3;\n",
    "        }'\n",
    "        if [ \"$PENDING\" -gt 5 ]; then\n",
    "            echo \"  ... and $((PENDING - 5)) more pending jobs\"\n",
    "        fi\n",
    "        echo \"\"\n",
    "    fi\n",
    "fi\n",
    "\n",
    "# Recent failures\n",
    "echo \"RECENT ISSUES:\"\n",
    "echo \"--------------------------------------------------\"\n",
    "TODAY=$(date '+%Y-%m-%d')\n",
    "\n",
    "# Get failed jobs with timing information\n",
    "TEMP_FAILED_FILE=\"/tmp/monitor_failed_$.txt\"\n",
    "sacct -u $USER --starttime=$TODAY --format=JobID,JobName%35,State,Start,End --noheader 2>/dev/null | \\\n",
    "    grep -v batch | grep -v extern | \\\n",
    "    grep -E \"FAILED|CANCELLED|TIMEOUT\" > \"$TEMP_FAILED_FILE\"\n",
    "\n",
    "FAILED_TODAY=$(wc -l < \"$TEMP_FAILED_FILE\" 2>/dev/null || echo \"0\")\n",
    "\n",
    "if [ \"$FAILED_TODAY\" -eq 0 ]; then\n",
    "    echo \"  No failed jobs today\"\n",
    "else\n",
    "    echo \"  $FAILED_TODAY failed jobs today (showing most recent 5):\"\n",
    "    echo \"\"\n",
    "    printf \"    %-30s %-10s %-8s %-8s\\\\n\" \"JOB NAME\" \"STATE\" \"START\" \"END\"\n",
    "    echo \"    ----------------------------------------------------------------\"\n",
    "    # Sort by job ID to get most recent first, then take last 5\n",
    "    sort -n \"$TEMP_FAILED_FILE\" | tail -5 | while read line; do\n",
    "        job_id=$(echo \"$line\" | awk '{print $1}')\n",
    "        job_name=$(echo \"$line\" | awk '{print $2}' | cut -c1-30)\n",
    "        job_state=$(echo \"$line\" | awk '{print $3}')\n",
    "        job_start=$(echo \"$line\" | awk '{print $4}')\n",
    "        job_end=$(echo \"$line\" | awk '{print $5}')\n",
    "        \n",
    "        # Format times to show just time (HH:MM) if today, or date if older\n",
    "        if [ -n \"$job_start\" ] && [ \"$job_start\" != \"Unknown\" ]; then\n",
    "            start_time=$(date -d \"$job_start\" '+%H:%M' 2>/dev/null || echo \"$job_start\" | cut -c12-16)\n",
    "        else\n",
    "            start_time=\"--:--\"\n",
    "        fi\n",
    "        \n",
    "        if [ -n \"$job_end\" ] && [ \"$job_end\" != \"Unknown\" ]; then\n",
    "            end_time=$(date -d \"$job_end\" '+%H:%M' 2>/dev/null || echo \"$job_end\" | cut -c12-16)\n",
    "        else\n",
    "            end_time=\"--:--\"\n",
    "        fi\n",
    "        \n",
    "        printf \"    %-30s %-10s %-8s %-8s\\\\n\" \"$job_name\" \"$job_state\" \"$start_time\" \"$end_time\"\n",
    "    done\n",
    "    \n",
    "    # Show summary of failure types\n",
    "    echo \"\"\n",
    "    echo \"  Failure breakdown:\"\n",
    "    FAILED_COUNT=$(grep \"FAILED\" \"$TEMP_FAILED_FILE\" | wc -l)\n",
    "    TIMEOUT_COUNT=$(grep \"TIMEOUT\" \"$TEMP_FAILED_FILE\" | wc -l)\n",
    "    CANCELLED_COUNT=$(grep \"CANCELLED\" \"$TEMP_FAILED_FILE\" | wc -l)\n",
    "    \n",
    "    if [ \"$FAILED_COUNT\" -gt 0 ]; then\n",
    "        echo \"    FAILED: $FAILED_COUNT jobs\"\n",
    "    fi\n",
    "    if [ \"$TIMEOUT_COUNT\" -gt 0 ]; then\n",
    "        echo \"    TIMEOUT: $TIMEOUT_COUNT jobs\"\n",
    "    fi\n",
    "    if [ \"$CANCELLED_COUNT\" -gt 0 ]; then\n",
    "        echo \"    CANCELLED: $CANCELLED_COUNT jobs\"\n",
    "    fi\n",
    "fi\n",
    "echo \"\"\n",
    "\n",
    "# Clean up temp file\n",
    "rm -f \"$TEMP_FAILED_FILE\"\n",
    "\n",
    "# Training progress\n",
    "echo \"TRAINING PROGRESS:\"\n",
    "echo \"--------------------------------------------------\"\n",
    "\n",
    "# Count recent checkpoints and logs\n",
    "RECENT_CHECKPOINTS=$(find . -name \"checkpoint_epoch_*.pth\" -mtime -1 2>/dev/null | wc -l)\n",
    "ACTIVE_LOGS=$(find . -name \"training_*.log\" -mtime -0.1 2>/dev/null | wc -l)\n",
    "\n",
    "echo \"  Recent checkpoints (24h): $RECENT_CHECKPOINTS\"\n",
    "echo \"  Active training logs:     $ACTIVE_LOGS\"\n",
    "echo \"\"\n",
    "\n",
    "# Disk usage\n",
    "echo \"DISK USAGE:\"\n",
    "echo \"--------------------------------------------------\"\n",
    "if ls 01_training_*/ >/dev/null 2>&1; then\n",
    "    echo \"  Training directories:\"\n",
    "    du -sh 01_training_*/ 2>/dev/null | sort -hr | head -5 | while read size dir; do\n",
    "        printf \"    %-8s %s\\\\n\" \"$size\" \"$dir\"\n",
    "    done\n",
    "    \n",
    "    total_size=$(du -sh 01_training_*/ 2>/dev/null | awk '{sum+=$1} END {printf \"%.0fG\", sum}')\n",
    "    echo \"  Total: $total_size\"\n",
    "else\n",
    "    echo \"  No training directories found\"\n",
    "fi\n",
    "\n",
    "# Available space\n",
    "avail_space=$(df -h . 2>/dev/null | tail -1 | awk '{print $4}')\n",
    "used_percent=$(df -h . 2>/dev/null | tail -1 | awk '{print $5}')\n",
    "echo \"  Available space: $avail_space ($used_percent used)\"\n",
    "echo \"\"\n",
    "\"\"\"\n",
    "    \n",
    "    script_path = os.path.join(output_dir, \"monitor_training.sh\")\n",
    "    with open(script_path, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    os.chmod(script_path, 0o755)\n",
    "    print(f\"Created monitoring script: {script_path}\")\n",
    "    return script_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dcccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_restart_failed_jobs_script(output_dir):\n",
    "    \"\"\"Create a script to restart failed jobs.\"\"\"\n",
    "    script_content = \"\"\"#!/bin/bash\n",
    "# Resume/restart failed jobs script with time range option\n",
    "\n",
    "# Default to 24 hours if no argument provided\n",
    "HOURS_BACK=${1:-24}\n",
    "\n",
    "# Get the directory where this script is located\n",
    "SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n",
    "echo \"Script directory: $SCRIPT_DIR\"\n",
    "\n",
    "clear\n",
    "echo \"==================================================\"\n",
    "echo \"         RESTART FAILED JOBS\"\n",
    "echo \"==================================================\"\n",
    "echo \"Looking back: $HOURS_BACK hours\"\n",
    "echo \"\"\n",
    "\n",
    "# Calculate start time based on hours back\n",
    "if command -v date >/dev/null 2>&1; then\n",
    "    if date -d \"1 hour ago\" >/dev/null 2>&1; then\n",
    "        # GNU date (Linux)\n",
    "        START_TIME=$(date -d \"$HOURS_BACK hours ago\" '+%Y-%m-%dT%H:%M:%S')\n",
    "        START_DATE=$(date -d \"$HOURS_BACK hours ago\" '+%Y-%m-%d')\n",
    "        START_EPOCH=$(date -d \"$HOURS_BACK hours ago\" '+%s')\n",
    "    else\n",
    "        # BSD date (macOS)\n",
    "        START_TIME=$(date -v-${HOURS_BACK}H '+%Y-%m-%dT%H:%M:%S')\n",
    "        START_DATE=$(date -v-${HOURS_BACK}H '+%Y-%m-%d')\n",
    "        START_EPOCH=$(date -v-${HOURS_BACK}H '+%s')\n",
    "    fi\n",
    "else\n",
    "    echo \"Error: date command not available\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"Searching for failed jobs since: $START_TIME\"\n",
    "echo \"\"\n",
    "\n",
    "# Get failed jobs from the specified time range\n",
    "echo \"Scanning job database...\"\n",
    "TEMP_FILE=\"/tmp/failed_jobs_$.txt\"\n",
    "\n",
    "# Use the start time with sacct (try full timestamp first, fall back to date)\n",
    "sacct -u $USER --starttime=\"$START_TIME\" --format=JobID,JobName%50,State,Start,End --noheader 2>/dev/null | \\\n",
    "    grep -v batch | grep -v extern | \\\n",
    "    grep -E \"FAILED|CANCELLED|TIMEOUT\" > \"$TEMP_FILE\"\n",
    "\n",
    "# If that didn't work, try with just the date and filter manually\n",
    "if [ ! -s \"$TEMP_FILE\" ]; then\n",
    "    sacct -u $USER --starttime=\"$START_DATE\" --format=JobID,JobName%50,State,Start,End --noheader 2>/dev/null | \\\n",
    "        grep -v batch | grep -v extern | \\\n",
    "        grep -E \"FAILED|CANCELLED|TIMEOUT\" > \"$TEMP_FILE\"\n",
    "fi\n",
    "\n",
    "# Filter results to only include jobs started after our time threshold\n",
    "FILTERED_FILE=\"/tmp/filtered_jobs_$.txt\"\n",
    "> \"$FILTERED_FILE\"  # Create empty file\n",
    "\n",
    "while IFS= read -r line; do\n",
    "    if [ -n \"$line\" ]; then\n",
    "        job_start=$(echo \"$line\" | awk '{print $4}')\n",
    "        \n",
    "        # Convert job start time to epoch for comparison\n",
    "        job_start_epoch=$(date -d \"$job_start\" +%s 2>/dev/null || echo \"0\")\n",
    "        \n",
    "        # Only include jobs that started after our threshold\n",
    "        if [ \"$job_start_epoch\" -ge \"$START_EPOCH\" ]; then\n",
    "            echo \"$line\" >> \"$FILTERED_FILE\"\n",
    "        fi\n",
    "    fi\n",
    "done < \"$TEMP_FILE\"\n",
    "\n",
    "# Use the filtered file\n",
    "mv \"$FILTERED_FILE\" \"$TEMP_FILE\"\n",
    "\n",
    "if [ ! -s \"$TEMP_FILE\" ]; then\n",
    "    echo \"No failed jobs found in the last $HOURS_BACK hours.\"\n",
    "    rm -f \"$TEMP_FILE\"\n",
    "    echo \"\"\n",
    "    echo \"Current running jobs: $(squeue -u $USER -t RUNNING -h | wc -l)\"\n",
    "    echo \"Use './monitor_training.sh' for detailed status\"\n",
    "    echo \"==================================================\"\n",
    "    exit 0\n",
    "fi\n",
    "\n",
    "# Parse and collect all failed jobs\n",
    "declare -a ALL_JOB_IDS=()\n",
    "declare -a ALL_JOB_NAMES=()\n",
    "declare -a ALL_JOB_STATES=()\n",
    "declare -a ALL_JOB_STARTS=()\n",
    "\n",
    "job_count=0\n",
    "while IFS= read -r line; do\n",
    "    if [ -n \"$line\" ]; then\n",
    "        job_id=$(echo \"$line\" | awk '{print $1}')\n",
    "        job_name=$(echo \"$line\" | awk '{print $2}' | sed 's/+$//')\n",
    "        job_state=$(echo \"$line\" | awk '{print $3}')\n",
    "        job_start=$(echo \"$line\" | awk '{print $4}')\n",
    "        \n",
    "        if [ -n \"$job_id\" ] && [ -n \"$job_name\" ]; then\n",
    "            job_count=$((job_count + 1))\n",
    "            \n",
    "            ALL_JOB_IDS+=(\"$job_id\")\n",
    "            ALL_JOB_NAMES+=(\"$job_name\")\n",
    "            ALL_JOB_STATES+=(\"$job_state\")\n",
    "            ALL_JOB_STARTS+=(\"$job_start\")\n",
    "        fi\n",
    "    fi\n",
    "done < \"$TEMP_FILE\"\n",
    "\n",
    "rm -f \"$TEMP_FILE\"\n",
    "\n",
    "if [ $job_count -eq 0 ]; then\n",
    "    echo \"  No failed jobs found in specified time range\"\n",
    "    echo \"==================================================\"\n",
    "    exit 0\n",
    "fi\n",
    "\n",
    "echo \"Found $job_count failed jobs total.\"\n",
    "echo \"Deduplicating jobs (keeping most recent attempt per job name)...\"\n",
    "\n",
    "# Create associative arrays for deduplication\n",
    "declare -A LATEST_JOB_ID\n",
    "declare -A LATEST_JOB_STATE  \n",
    "declare -A LATEST_JOB_START\n",
    "declare -A LATEST_JOB_START_EPOCH\n",
    "\n",
    "# Process each job to find the latest attempt for each job name\n",
    "for i in \"${!ALL_JOB_NAMES[@]}\"; do\n",
    "    job_name=\"${ALL_JOB_NAMES[$i]}\"\n",
    "    job_id=\"${ALL_JOB_IDS[$i]}\"\n",
    "    job_state=\"${ALL_JOB_STATES[$i]}\"\n",
    "    job_start=\"${ALL_JOB_STARTS[$i]}\"\n",
    "    \n",
    "    # Convert start time to epoch for comparison\n",
    "    job_start_epoch=$(date -d \"$job_start\" +%s 2>/dev/null || echo \"0\")\n",
    "    \n",
    "    # Check if this is the latest attempt for this job name\n",
    "    if [[ ! -v LATEST_JOB_START_EPOCH[\"$job_name\"] ]] || [[ $job_start_epoch -gt ${LATEST_JOB_START_EPOCH[\"$job_name\"]} ]]; then\n",
    "        LATEST_JOB_ID[\"$job_name\"]=\"$job_id\"\n",
    "        LATEST_JOB_STATE[\"$job_name\"]=\"$job_state\"\n",
    "        LATEST_JOB_START[\"$job_name\"]=\"$job_start\"\n",
    "        LATEST_JOB_START_EPOCH[\"$job_name\"]=\"$job_start_epoch\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "# Create deduplicated arrays grouped by status\n",
    "declare -a FAILED_JOB_NAMES=()\n",
    "declare -a FAILED_JOB_IDS=()\n",
    "declare -a FAILED_JOB_STARTS=()\n",
    "\n",
    "declare -a CANCELLED_JOB_NAMES=()\n",
    "declare -a CANCELLED_JOB_IDS=()\n",
    "declare -a CANCELLED_JOB_STARTS=()\n",
    "\n",
    "declare -a TIMEOUT_JOB_NAMES=()\n",
    "declare -a TIMEOUT_JOB_IDS=()\n",
    "declare -a TIMEOUT_JOB_STARTS=()\n",
    "\n",
    "# Group jobs by status\n",
    "for job_name in \"${!LATEST_JOB_ID[@]}\"; do\n",
    "    job_id=\"${LATEST_JOB_ID[\"$job_name\"]}\"\n",
    "    job_state=\"${LATEST_JOB_STATE[\"$job_name\"]}\"\n",
    "    job_start=\"${LATEST_JOB_START[\"$job_name\"]}\"\n",
    "    \n",
    "    case \"$job_state\" in\n",
    "        \"FAILED\")\n",
    "            FAILED_JOB_NAMES+=(\"$job_name\")\n",
    "            FAILED_JOB_IDS+=(\"$job_id\")\n",
    "            FAILED_JOB_STARTS+=(\"$job_start\")\n",
    "            ;;\n",
    "        \"CANCELLED\")\n",
    "            CANCELLED_JOB_NAMES+=(\"$job_name\")\n",
    "            CANCELLED_JOB_IDS+=(\"$job_id\")\n",
    "            CANCELLED_JOB_STARTS+=(\"$job_start\")\n",
    "            ;;\n",
    "        \"TIMEOUT\")\n",
    "            TIMEOUT_JOB_NAMES+=(\"$job_name\")\n",
    "            TIMEOUT_JOB_IDS+=(\"$job_id\")\n",
    "            TIMEOUT_JOB_STARTS+=(\"$job_start\")\n",
    "            ;;\n",
    "    esac\n",
    "done\n",
    "\n",
    "dedup_count=$((${#FAILED_JOB_NAMES[@]} + ${#CANCELLED_JOB_NAMES[@]} + ${#TIMEOUT_JOB_NAMES[@]}))\n",
    "echo \"After deduplication: $dedup_count unique jobs (removed $((job_count - dedup_count)) duplicates)\"\n",
    "echo \"\"\n",
    "\n",
    "# Display jobs grouped by status\n",
    "echo \"Failed jobs by status:\"\n",
    "echo \"--------------------------------------------------\"\n",
    "\n",
    "if [ ${#FAILED_JOB_NAMES[@]} -gt 0 ]; then\n",
    "    echo \"FAILED Jobs (${#FAILED_JOB_NAMES[@]} found):\"\n",
    "    for i in \"${!FAILED_JOB_NAMES[@]}\"; do\n",
    "        echo \"  $((i+1)). ${FAILED_JOB_NAMES[$i]} - Started: ${FAILED_JOB_STARTS[$i]}\"\n",
    "    done\n",
    "    echo \"\"\n",
    "fi\n",
    "\n",
    "if [ ${#CANCELLED_JOB_NAMES[@]} -gt 0 ]; then\n",
    "    echo \"CANCELLED Jobs (${#CANCELLED_JOB_NAMES[@]} found):\"\n",
    "    for i in \"${!CANCELLED_JOB_NAMES[@]}\"; do\n",
    "        echo \"  $((i+1)). ${CANCELLED_JOB_NAMES[$i]} - Started: ${CANCELLED_JOB_STARTS[$i]}\"\n",
    "    done\n",
    "    echo \"\"\n",
    "fi\n",
    "\n",
    "if [ ${#TIMEOUT_JOB_NAMES[@]} -gt 0 ]; then\n",
    "    echo \"TIMEOUT Jobs (${#TIMEOUT_JOB_NAMES[@]} found):\"\n",
    "    for i in \"${!TIMEOUT_JOB_NAMES[@]}\"; do\n",
    "        echo \"  $((i+1)). ${TIMEOUT_JOB_NAMES[$i]} - Started: ${TIMEOUT_JOB_STARTS[$i]}\"\n",
    "    done\n",
    "    echo \"\"\n",
    "fi\n",
    "\n",
    "# Check how many are already in queue\n",
    "echo \"Checking for jobs already in queue...\"\n",
    "TEMP_QUEUE_FILE=\"/tmp/queue_check_$$.txt\"\n",
    "squeue -u $USER --format=\"%j\" --noheader > \"$TEMP_QUEUE_FILE\"\n",
    "\n",
    "ALREADY_IN_QUEUE=0\n",
    "ALL_JOB_NAMES_DEDUP=($(printf '%s\\\\n' \"${FAILED_JOB_NAMES[@]}\" \"${CANCELLED_JOB_NAMES[@]}\" \"${TIMEOUT_JOB_NAMES[@]}\"))\n",
    "\n",
    "for job_name in \"${ALL_JOB_NAMES_DEDUP[@]}\"; do\n",
    "    if grep -q \"^$job_name$\" \"$TEMP_QUEUE_FILE\"; then\n",
    "        ALREADY_IN_QUEUE=$((ALREADY_IN_QUEUE + 1))\n",
    "    fi\n",
    "done\n",
    "rm -f \"$TEMP_QUEUE_FILE\"\n",
    "\n",
    "echo \"SUMMARY:\"\n",
    "echo \"--------------------------------------------------\"\n",
    "echo \"FAILED jobs: ${#FAILED_JOB_NAMES[@]}\"\n",
    "echo \"CANCELLED jobs: ${#CANCELLED_JOB_NAMES[@]}\"\n",
    "echo \"TIMEOUT jobs: ${#TIMEOUT_JOB_NAMES[@]}\"\n",
    "echo \"Total unique jobs: $dedup_count\"\n",
    "echo \"Jobs already in queue: $ALREADY_IN_QUEUE\"\n",
    "echo \"\"\n",
    "\n",
    "if [ $dedup_count -eq 0 ]; then\n",
    "    echo \"No jobs to restart.\"\n",
    "    echo \"==================================================\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "if [ $ALREADY_IN_QUEUE -eq $dedup_count ]; then\n",
    "    echo \"All failed jobs are already running or pending in the queue.\"\n",
    "    echo \"==================================================\"\n",
    "    exit 0\n",
    "fi\n",
    "\n",
    "PROCESSABLE_JOBS=$((dedup_count - ALREADY_IN_QUEUE))\n",
    "echo \"Will process $PROCESSABLE_JOBS jobs (skipping $ALREADY_IN_QUEUE already in queue)\"\n",
    "echo \"\"\n",
    "\n",
    "# Ask user what to do\n",
    "echo \"What would you like to do?\"\n",
    "echo \"  1) Restart ALL failed jobs\"\n",
    "echo \"  2) Restart only FAILED jobs\"\n",
    "echo \"  3) Restart only CANCELLED jobs\"\n",
    "echo \"  4) Restart only TIMEOUT jobs\"\n",
    "echo \"  5) Cancel\"\n",
    "echo \"\"\n",
    "echo -n \"Choose option (1-5): \"\n",
    "read -r choice\n",
    "\n",
    "# Determine which jobs to process based on choice\n",
    "declare -a TARGET_JOB_NAMES=()\n",
    "\n",
    "case $choice in\n",
    "    1)\n",
    "        echo \"\"\n",
    "        echo \"RESTARTING all failed jobs...\"\n",
    "        echo \"--------------------------------------------------\"\n",
    "        TARGET_JOB_NAMES=($(printf '%s\\\\n' \"${FAILED_JOB_NAMES[@]}\" \"${CANCELLED_JOB_NAMES[@]}\" \"${TIMEOUT_JOB_NAMES[@]}\"))\n",
    "        ;;\n",
    "    2)\n",
    "        echo \"\"\n",
    "        echo \"RESTARTING FAILED jobs only...\"\n",
    "        echo \"--------------------------------------------------\"\n",
    "        TARGET_JOB_NAMES=(\"${FAILED_JOB_NAMES[@]}\")\n",
    "        ;;\n",
    "    3)\n",
    "        echo \"\"\n",
    "        echo \"RESTARTING CANCELLED jobs only...\"\n",
    "        echo \"--------------------------------------------------\"\n",
    "        TARGET_JOB_NAMES=(\"${CANCELLED_JOB_NAMES[@]}\")\n",
    "        ;;\n",
    "    4)\n",
    "        echo \"\"\n",
    "        echo \"RESTARTING TIMEOUT jobs only...\"\n",
    "        echo \"--------------------------------------------------\"\n",
    "        TARGET_JOB_NAMES=(\"${TIMEOUT_JOB_NAMES[@]}\")\n",
    "        ;;\n",
    "    *)\n",
    "        echo \"Cancelled.\"\n",
    "        echo \"==================================================\"\n",
    "        exit 0\n",
    "        ;;\n",
    "esac\n",
    "\n",
    "if [ ${#TARGET_JOB_NAMES[@]} -eq 0 ]; then\n",
    "    echo \"No jobs selected for restart.\"\n",
    "    echo \"==================================================\"\n",
    "    exit 0\n",
    "fi\n",
    "\n",
    "# Find scripts for target jobs\n",
    "echo \"Finding scripts for ${#TARGET_JOB_NAMES[@]} jobs...\"\n",
    "echo \"\"\n",
    "\n",
    "declare -a SCRIPTS_FOUND=()\n",
    "\n",
    "for job_name in \"${TARGET_JOB_NAMES[@]}\"; do\n",
    "    echo \"Finding script for: $job_name\"\n",
    "    \n",
    "    # Find the corresponding script\n",
    "    SCRIPT=\"\"\n",
    "    \n",
    "    # Method 1: Look for exact job name match\n",
    "    SCRIPT=$(find \"$SCRIPT_DIR\" -name \"*.sh\" -exec grep -l \"#SBATCH --job-name=$job_name\" {} \\; 2>/dev/null | head -1)\n",
    "\n",
    "    if [ -n \"$SCRIPT\" ] && [ -f \"$SCRIPT\" ]; then\n",
    "        RELATIVE_SCRIPT=$(echo \"$SCRIPT\" | sed \"s|^$SCRIPT_DIR/||\")\n",
    "        echo \"  ✓ Script found: $RELATIVE_SCRIPT\"\n",
    "        SCRIPTS_FOUND+=(\"$SCRIPT\")\n",
    "    else\n",
    "        echo \"  ✗ Script not found\"\n",
    "        SCRIPTS_FOUND+=(\"\")\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "\n",
    "# Execute restart for jobs with scripts found\n",
    "echo \"Checking current queue status...\"\n",
    "QUEUE_FILE=\"/tmp/current_queue_$$.txt\"\n",
    "squeue -u $USER --format=\"%j\" --noheader > \"$QUEUE_FILE\"\n",
    "\n",
    "echo \"Current running/pending jobs: $(wc -l < \"$QUEUE_FILE\")\"\n",
    "echo \"\"\n",
    "\n",
    "SUCCESS=0\n",
    "SKIPPED=0\n",
    "ALREADY_QUEUED=0\n",
    "\n",
    "for i in \"${!TARGET_JOB_NAMES[@]}\"; do\n",
    "    job_name=\"${TARGET_JOB_NAMES[$i]}\"\n",
    "    script=\"${SCRIPTS_FOUND[$i]}\"\n",
    "    \n",
    "    # Skip if no script found\n",
    "    if [ -z \"$script\" ]; then\n",
    "        echo \"  Skipping $job_name (no script found)\"\n",
    "        SKIPPED=$((SKIPPED + 1))\n",
    "        continue\n",
    "    fi\n",
    "    \n",
    "    # Check if job is already in queue (running or pending)\n",
    "    if grep -q \"^$job_name$\" \"$QUEUE_FILE\"; then\n",
    "        job_status=$(squeue -u $USER --name=\"$job_name\" --format=\"%T\" --noheader | head -1)\n",
    "        echo \"  Skipping $job_name (already $job_status)\"\n",
    "        ALREADY_QUEUED=$((ALREADY_QUEUED + 1))\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    # RESTART job\n",
    "    echo -n \"  Restarting $(basename $script)... \"\n",
    "    result=$(sbatch \"$script\" 2>&1)\n",
    "    \n",
    "    if [ $? -eq 0 ]; then\n",
    "        job_id=$(echo \"$result\" | grep -o '[0-9]\\+')\n",
    "        echo \"SUCCESS (Job $job_id)\"\n",
    "        SUCCESS=$((SUCCESS + 1))\n",
    "    else\n",
    "        echo \"FAILED - $result\"\n",
    "    fi\n",
    "    \n",
    "    sleep 1\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"==================================================\"\n",
    "echo \"RESULTS:\"\n",
    "echo \"  Successfully restarted: $SUCCESS\"\n",
    "echo \"  Already in queue: $ALREADY_QUEUED\"\n",
    "echo \"  Skipped (no script): $SKIPPED\"\n",
    "echo \"\"\n",
    "echo \"Check status: squeue -u $USER\"\n",
    "echo \"Monitor: ./monitor_training.sh\"\n",
    "echo \"==================================================\"\n",
    "\n",
    "# Clean up temp files\n",
    "rm -f \"$QUEUE_FILE\"\n",
    "\"\"\"\n",
    "    \n",
    "    script_path = os.path.join(output_dir, \"restart_failed_jobs.sh\")\n",
    "    with open(script_path, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    os.chmod(script_path, 0o755)\n",
    "    print(f\"Created restart script: {script_path}\")\n",
    "    return script_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b018065",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING UTILITY SCRIPTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "monitor_script = create_job_monitoring_script(OUTPUT_DIR)\n",
    "restart_script = create_restart_failed_jobs_script(OUTPUT_DIR)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UTILITY SCRIPTS CREATED:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"1. Monitor jobs:        bash {os.path.basename(monitor_script)}\")\n",
    "print(f\"2. Restart failed jobs: bash {os.path.basename(restart_script)}\")\n",
    "print(f\"\\nAll utility scripts are in: {OUTPUT_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
