{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    " \n",
    "**Objective:** Evaluate segmentation models on test dataset and analyze performance.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load trained models and configurations\n",
    "2. Run inference on test dataset\n",
    "3. Calculate comprehensive metrics (IoU, mAP, F1-score)\n",
    "4. Generate visualizations and performance analysis\n",
    "5. Create ensemble predictions from k-fold models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "from loguru import logger\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ampere\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date and time settings\n",
    "todays_date = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Model paths\n",
    "INPUT_MODELS_BASE_DIR_PATH = \"data/training\"\n",
    "\n",
    "# Dataset paths\n",
    "BASE_DATASET_DIR = \"datasets/supervisely/dataset_processed_20250523-173715\"\n",
    "DATASET_IMAGES_DIR = BASE_DATASET_DIR + \"/images\"\n",
    "DATASET_MASKS_DIR = BASE_DATASET_DIR + \"/masks\"\n",
    "TEST_DATASET_FILE = BASE_DATASET_DIR + \"/test_dataset.txt\"\n",
    "IMG_SIZE = (1280, 1280)\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_BASE_DIR_PATH = \"data/notebook_10/\"\n",
    "OUTPUT_PARQUET_PATH = OUTPUT_BASE_DIR_PATH + \"parquet/\"\n",
    "GRAPHICS_PATH = OUTPUT_BASE_DIR_PATH + f\"graphics/{todays_date}/\"\n",
    "\n",
    "# Saved results paths for loading existing evaluations\n",
    "SAVED_MODEL_RESULTS_PARQUET_PATH = OUTPUT_PARQUET_PATH + \"model_evaluation_models_20250629_210320.parquet\"\n",
    "SAVED_PER_IMAGE_RESULTS_PARQUET_PATH = OUTPUT_PARQUET_PATH + \"model_evaluation_per_image_20250629_210320.parquet\"\n",
    "\n",
    "# YOLO specific paths\n",
    "TEST_DATASET_PATH = \"datasets/supervisely/yolo_processed_20250619_151249/fold_0_dataset/test\"\n",
    "\n",
    "# Analysis output directories\n",
    "VIZ_OUTPUT_DIR = Path(OUTPUT_BASE_DIR_PATH + f\"analyse_qualitative/{todays_date}/\")\n",
    "QUARTIER_OUTPUT_DIR = Path(OUTPUT_BASE_DIR_PATH + f\"analyse_quartier/{todays_date}/\")\n",
    "\n",
    "# Feature flags\n",
    "MAJ_DATASET = False      # Update dataset evaluation\n",
    "YOLO_EVALUATE = False    # Evaluate YOLO models\n",
    "EVALUER_ENSEMBLE = False # Evaluate ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "os.makedirs(OUTPUT_BASE_DIR_PATH, exist_ok=True)\n",
    "os.makedirs(OUTPUT_PARQUET_PATH, exist_ok=True)\n",
    "os.makedirs(GRAPHICS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\", format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAJ_DATASET:\n",
    "    def get_best_model_paths(base_dir):\n",
    "        \"\"\"\n",
    "        Find all model result JSON files in directory tree.\n",
    "        \n",
    "        Parameters:\n",
    "            base_dir: Root directory to search\n",
    "            \n",
    "        Returns:\n",
    "            List of paths to model result files\n",
    "        \"\"\"\n",
    "        model_paths = []\n",
    "        for root, dirs, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                if re.match(r\"single_fold_complete_results.json\", file):\n",
    "                    model_paths.append(os.path.join(root, file))\n",
    "        return model_paths\n",
    "\n",
    "    def test_json_parsing(json_file_path):\n",
    "        \"\"\"\n",
    "        Verify JSON parsing and data structure.\n",
    "        \n",
    "        Parameters:\n",
    "            json_file_path: Path to JSON file to test\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with parsed data or None if parsing fails\n",
    "        \"\"\"\n",
    "        print(\"Testing JSON parsing...\")\n",
    "        \n",
    "        try:\n",
    "            # Test single file loading\n",
    "            with open(json_file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            print(\"JSON file loads successfully\")\n",
    "            print(f\"Structure: {list(data.keys())}\")\n",
    "            \n",
    "            # Test parsing function\n",
    "            df_test = json_files_to_dataframe([json_file_path])\n",
    "            print(f\"DataFrame created with shape: {df_test.shape}\")\n",
    "            print(f\"Columns: {df_test.columns.tolist()}\")\n",
    "            \n",
    "            # Test completion function\n",
    "            df_complete = completer_df(df_test)\n",
    "            print(f\"DataFrame completed with shape: {df_complete.shape}\")\n",
    "            \n",
    "            # Show sample data\n",
    "            print(\"\\nSample data:\")\n",
    "            for col in ['config', 'validation_fold', 'architecture', 'backbone', 'test_iou', 'model_path']:\n",
    "                if col in df_complete.columns:\n",
    "                    print(f\"  {col}: {df_complete[col].iloc[0]}\")\n",
    "            \n",
    "            return df_complete\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def json_files_to_dataframe(model_paths):\n",
    "        \"\"\"\n",
    "        Read multiple JSON files and concatenate into DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "            model_paths: List of JSON file paths\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing all model results\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        for json_path in model_paths:\n",
    "            try:\n",
    "                with open(json_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                def safe_get(data, keys, default=''):\n",
    "                    \"\"\"Safely get nested dictionary values\"\"\"\n",
    "                    current = data\n",
    "                    for key in keys:\n",
    "                        if isinstance(current, dict) and key in current:\n",
    "                            current = current[key]\n",
    "                        else:\n",
    "                            return default\n",
    "                    return current\n",
    "                \n",
    "                # Extract data from JSON structure\n",
    "                flattened_data = {\n",
    "                    'file_path': json_path,\n",
    "                    'config': safe_get(data, ['config']),\n",
    "                    'validation_fold': safe_get(data, ['fold']),\n",
    "                    \n",
    "                    # Training configuration\n",
    "                    'architecture': safe_get(data, ['training_config', 'architecture']),\n",
    "                    'backbone': safe_get(data, ['training_config', 'backbone']),\n",
    "                    'encoder_weights': safe_get(data, ['training_config', 'encoder_weights']),\n",
    "                    'img_size': safe_get(data, ['training_config', 'img_size']),\n",
    "                    'num_classes': safe_get(data, ['training_config', 'num_classes']),\n",
    "                    'batch_size': safe_get(data, ['training_config', 'batch_size']),\n",
    "                    'learning_rate': safe_get(data, ['training_config', 'learning_rate']),\n",
    "                    'epochs': safe_get(data, ['training_config', 'epochs']),\n",
    "                    'patience': safe_get(data, ['training_config', 'patience']),\n",
    "                    'accumulation_steps': safe_get(data, ['training_config', 'accumulation_steps']),\n",
    "                    'auto_batch_size': safe_get(data, ['training_config', 'auto_batch_size']),\n",
    "                    'min_batch_size_search': safe_get(data, ['training_config', 'min_batch_size_search']),\n",
    "                    'max_batch_size_search': safe_get(data, ['training_config', 'max_batch_size_search']),\n",
    "                    'batch_size_test_steps': safe_get(data, ['training_config', 'batch_size_test_steps']),\n",
    "                    \n",
    "                    # Validation metrics\n",
    "                    'best_val_iou': safe_get(data, ['results', 'best_val_iou']),\n",
    "                    'final_val_iou': safe_get(data, ['results', 'final_val_metrics', 'iou']),\n",
    "                    'final_val_f1_score': safe_get(data, ['results', 'final_val_metrics', 'f1_score']),\n",
    "                    'final_val_accuracy': safe_get(data, ['results', 'final_val_metrics', 'accuracy']),\n",
    "                    'final_val_recall': safe_get(data, ['results', 'final_val_metrics', 'recall']),\n",
    "                    'final_val_precision': safe_get(data, ['results', 'final_val_metrics', 'precision']),\n",
    "                    \n",
    "                    # Test metrics\n",
    "                    'test_iou': safe_get(data, ['results', 'test_metrics', 'iou']),\n",
    "                    'test_f1_score': safe_get(data, ['results', 'test_metrics', 'f1_score']),\n",
    "                    'test_accuracy': safe_get(data, ['results', 'test_metrics', 'accuracy']),\n",
    "                    'test_recall': safe_get(data, ['results', 'test_metrics', 'recall']),\n",
    "                    'test_precision': safe_get(data, ['results', 'test_metrics', 'precision']),\n",
    "                    \n",
    "                    # Timing information\n",
    "                    'total_seconds': safe_get(data, ['results', 'timing', 'total_seconds']),\n",
    "                    'total_time_formatted': safe_get(data, ['results', 'timing', 'total_time_formatted']),\n",
    "                    'total_epochs_trained': safe_get(data, ['results', 'timing', 'total_epochs_trained']),\n",
    "                    'average_seconds_per_epoch': safe_get(data, ['results', 'timing', 'average_seconds_per_epoch']),\n",
    "                    'start_timestamp': safe_get(data, ['results', 'timing', 'start_timestamp']),\n",
    "                    'end_timestamp': safe_get(data, ['results', 'timing', 'end_timestamp']),\n",
    "                    \n",
    "                    # Paths\n",
    "                    'plot_path': safe_get(data, ['results', 'plot_path']),\n",
    "                    'pred_path': safe_get(data, ['results', 'pred_path'])\n",
    "                }\n",
    "                \n",
    "                all_data.append(flattened_data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {json_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(all_data)\n",
    "        print(f\"Successfully loaded {len(df)} records from {len(model_paths)} JSON files\")\n",
    "        return df\n",
    "\n",
    "    def completer_df(df):\n",
    "        \"\"\"\n",
    "        Complete DataFrame by processing timing data and creating model paths.\n",
    "        \n",
    "        Parameters:\n",
    "            df: Raw DataFrame from JSON files\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with additional processed columns\n",
    "        \"\"\"\n",
    "        df_temp = df.copy()\n",
    "        \n",
    "        # Create model path for evaluation\n",
    "        try:\n",
    "            df_temp[\"validation_fold_str\"] = df_temp[\"validation_fold\"].astype(str)\n",
    "            \n",
    "            df_temp[\"model_path\"] = (\n",
    "                df_temp[\"file_path\"].str.replace(\"single_fold_complete_results.json\", \"\") + \n",
    "                \"best_model_fold_\" + \n",
    "                df_temp[\"validation_fold_str\"] + \n",
    "                \".pth\"\n",
    "            )\n",
    "            \n",
    "            df_temp = df_temp.drop(\"validation_fold_str\", axis=1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating model_path: {e}\")\n",
    "            # Fallback path generation\n",
    "            df_temp[\"model_path\"] = df_temp[\"file_path\"].str.replace(\"single_fold_complete_results.json\", \"best_model.pth\")\n",
    "        \n",
    "        # Convert timestamps to datetime\n",
    "        try:\n",
    "            df_temp[\"model_start_date\"] = pd.to_datetime(df_temp[\"start_timestamp\"], errors='coerce')\n",
    "            df_temp[\"model_end_date\"] = pd.to_datetime(df_temp[\"end_timestamp\"], errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting timestamps: {e}\")\n",
    "            df_temp[\"model_start_date\"] = pd.NaT\n",
    "            df_temp[\"model_end_date\"] = pd.NaT\n",
    "        \n",
    "        # Convert training time to hours\n",
    "        try:\n",
    "            df_temp[\"total_training_time_hour\"] = pd.to_numeric(df_temp[\"total_seconds\"], errors='coerce') / 3600\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting training time: {e}\")\n",
    "            df_temp[\"total_training_time_hour\"] = np.nan\n",
    "        \n",
    "        # Convert numeric fields\n",
    "        numeric_fields = [\n",
    "            'learning_rate', 'epochs', 'patience', 'accumulation_steps', 'batch_size',\n",
    "            'num_classes', 'min_batch_size_search', 'max_batch_size_search', 'batch_size_test_steps',\n",
    "            'best_val_iou', 'final_val_iou', 'final_val_f1_score', 'final_val_accuracy', \n",
    "            'final_val_recall', 'final_val_precision',\n",
    "            'test_iou', 'test_f1_score', 'test_accuracy', 'test_recall', 'test_precision',\n",
    "            'total_seconds', 'total_epochs_trained', 'average_seconds_per_epoch', 'validation_fold'\n",
    "        ]\n",
    "        \n",
    "        for field in numeric_fields:\n",
    "            if field in df_temp.columns:\n",
    "                try:\n",
    "                    df_temp[field] = df_temp[field].replace('', np.nan)\n",
    "                    df_temp[field] = pd.to_numeric(df_temp[field], errors='coerce')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting {field} to numeric: {e}\")\n",
    "        \n",
    "        # Convert boolean fields\n",
    "        boolean_fields = ['auto_batch_size']\n",
    "        for field in boolean_fields:\n",
    "            if field in df_temp.columns:\n",
    "                try:\n",
    "                    df_temp[field] = df_temp[field].replace({'true': True, 'false': False, 'True': True, 'False': False})\n",
    "                    df_temp[field] = df_temp[field].astype('bool', errors='ignore')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting {field} to boolean: {e}\")\n",
    "        \n",
    "        # Process img_size field\n",
    "        if 'img_size' in df_temp.columns:\n",
    "            try:\n",
    "                df_temp['img_size_processed'] = df_temp['img_size'].apply(\n",
    "                    lambda x: eval(x) if isinstance(x, str) and x.startswith('(') else x\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing img_size: {e}\")\n",
    "                df_temp['img_size_processed'] = df_temp['img_size']\n",
    "        \n",
    "        print(f\"Data processing complete. DataFrame shape: {df_temp.shape}\")\n",
    "        return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAJ_DATASET:\n",
    "    # Get all JSON files\n",
    "    model_paths = get_best_model_paths(INPUT_MODELS_BASE_DIR_PATH)\n",
    "    print(f\"Found {len(model_paths)} JSON files\")\n",
    "    \n",
    "    if len(model_paths) == 0:\n",
    "        print(\"No JSON files found! Check your INPUT_MODELS_BASE_DIR_PATH\")\n",
    "        print(f\"Looking in: {INPUT_MODELS_BASE_DIR_PATH}\")\n",
    "        exit()\n",
    "    \n",
    "    # Test with first file\n",
    "    print(\"Testing with first JSON file...\")\n",
    "    test_result = test_json_parsing(model_paths[0])\n",
    "    if test_result is None:\n",
    "        print(\"JSON parsing test failed! Check your JSON structure.\")\n",
    "        exit()\n",
    "    \n",
    "    print(\"JSON parsing test successful!\")\n",
    "    \n",
    "    # Process all files\n",
    "    print(f\"\\nProcessing all {len(model_paths)} JSON files...\")\n",
    "    df_test_json = json_files_to_dataframe(model_paths)\n",
    "    df_test_models = completer_df(df_test_json)\n",
    "    \n",
    "    # Show summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATASET SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total models: {len(df_test_models)}\")\n",
    "    print(f\"Unique configs: {df_test_models['config'].nunique()}\")\n",
    "    print(f\"Validation folds: {sorted(df_test_models['validation_fold'].unique())}\")\n",
    "    print(f\"Architectures: {df_test_models['architecture'].unique()}\")\n",
    "    print(f\"Backbones: {df_test_models['backbone'].nunique()} unique\")\n",
    "    \n",
    "    # Data quality checks\n",
    "    print(\"\\nDATA QUALITY CHECKS:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Check for missing values\n",
    "    key_columns = ['config', 'architecture', 'backbone', 'test_iou', 'best_val_iou']\n",
    "    for col in key_columns:\n",
    "        if col in df_test_models.columns:\n",
    "            missing = df_test_models[col].isna().sum()\n",
    "            print(f\"Missing {col}: {missing}/{len(df_test_models)} ({missing/len(df_test_models)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for incomplete configs\n",
    "    config_counts = df_test_models['config'].value_counts()\n",
    "    incomplete_configs = config_counts[config_counts != 5]\n",
    "    if len(incomplete_configs) > 0:\n",
    "        print(f\"\\nWarning: {len(incomplete_configs)} configs don't have 5 folds:\")\n",
    "        for config, count in incomplete_configs.head().items():\n",
    "            print(f\"  {config}: {count} folds\")\n",
    "    else:\n",
    "        print(\"All configs have 5 folds\")\n",
    "    \n",
    "    # Check for missing model files\n",
    "    missing_models = df_test_models[~df_test_models['model_path'].apply(os.path.exists)]\n",
    "    if len(missing_models) > 0:\n",
    "        print(f\"\\nWarning: {len(missing_models)} model files not found:\")\n",
    "        for path in missing_models['model_path'].head(5):\n",
    "            print(f\"  {path}\")\n",
    "        \n",
    "        print(f\"\\n{len(missing_models)} out of {len(df_test_models)} model files are missing.\")\n",
    "        response = input(\"Continue with available models? (y/n): \").lower().strip()\n",
    "        if response != 'y':\n",
    "            print(\"Exiting...\")\n",
    "            exit()\n",
    "        else:\n",
    "            # Filter out missing models\n",
    "            df_test_models = df_test_models[df_test_models['model_path'].apply(os.path.exists)]\n",
    "            print(f\"Continuing with {len(df_test_models)} models that have available files\")\n",
    "    else:\n",
    "        print(\"All model files found\")\n",
    "    \n",
    "    # Filter out low performance models\n",
    "    df_test_models = df_test_models[df_test_models['test_iou'] >= 0.1]\n",
    "\n",
    "    # Show performance range\n",
    "    if 'test_iou' in df_test_models.columns:\n",
    "        test_iou_clean = df_test_models['test_iou'].dropna()\n",
    "        if len(test_iou_clean) > 0:\n",
    "            print(\"\\nPERFORMANCE RANGE:\")\n",
    "            print(f\"Test IoU: {test_iou_clean.min():.3f} - {test_iou_clean.max():.3f}\")\n",
    "            print(f\"Mean Test IoU: {test_iou_clean.mean():.3f}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"Ready to proceed with evaluation!\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate SMP Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(test_file, images_dir, masks_dir):\n",
    "    \"\"\"\n",
    "    Load test dataset paths from file.\n",
    "    \n",
    "    Parameters:\n",
    "        test_file: Path to file containing test image filenames\n",
    "        images_dir: Directory containing images\n",
    "        masks_dir: Directory containing masks\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'images' and 'masks' lists of paths\n",
    "    \"\"\"\n",
    "    with open(test_file, \"r\") as f:\n",
    "        filenames = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    images = []\n",
    "    masks = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        mask_path = os.path.join(masks_dir, filename)\n",
    "        image_path = os.path.join(images_dir, filename)\n",
    "\n",
    "        if os.path.exists(image_path) and os.path.exists(mask_path):\n",
    "            images.append(image_path)\n",
    "            masks.append(mask_path)\n",
    "\n",
    "    logger.debug(f\"Loaded {len(images)} test samples\")\n",
    "    return {\"images\": images, \"masks\": masks}\n",
    "\n",
    "\n",
    "class SimpleSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Memory-optimized dataset for segmentation tasks.\n",
    "    \n",
    "    Parameters:\n",
    "        image_paths: List of image file paths\n",
    "        mask_paths: List of mask file paths\n",
    "        img_size: Target image size (width, height)\n",
    "        transform: Optional augmentation transforms\n",
    "        cache_size: Number of images to cache in memory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, image_paths, mask_paths, img_size, transform=None, cache_size=250\n",
    "    ):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.cache_size = cache_size\n",
    "        self._cache = {}  # Simple LRU-like cache\n",
    "\n",
    "        # Validate paths\n",
    "        assert len(image_paths) == len(mask_paths), \"Mismatch between images and masks\"\n",
    "\n",
    "        # Check if files exist\n",
    "        valid_pairs = []\n",
    "        for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "            if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "                valid_pairs.append((img_path, mask_path))\n",
    "            else:\n",
    "                logger.warning(f\"Missing files: {img_path} or {mask_path}\")\n",
    "\n",
    "        self.image_paths = [pair[0] for pair in valid_pairs]\n",
    "        self.mask_paths = [pair[1] for pair in valid_pairs]\n",
    "\n",
    "        logger.debug(f\"Dataset initialized with {len(valid_pairs)} valid pairs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def _load_and_process(self, idx):\n",
    "        \"\"\"Load and process image/mask pair.\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "            image = image.resize(self.img_size, Image.Resampling.BILINEAR)\n",
    "            image = np.array(image)\n",
    "\n",
    "            # Load mask\n",
    "            mask = Image.open(self.mask_paths[idx])\n",
    "\n",
    "            # Convert to grayscale if needed\n",
    "            if mask.mode != \"L\":\n",
    "                mask = mask.convert(\"L\")\n",
    "\n",
    "            # Resize mask using nearest neighbor to preserve labels\n",
    "            mask = mask.resize(self.img_size, Image.Resampling.NEAREST)\n",
    "            mask = np.array(mask)\n",
    "\n",
    "            # Create binary mask\n",
    "            if mask.max() > 1:\n",
    "                mask = mask.astype(np.float32) / 255.0\n",
    "                mask = (mask > 0.5).astype(np.uint8)\n",
    "            else:\n",
    "                mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "            return image, mask\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading sample {idx}: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Check cache\n",
    "        if idx in self._cache:\n",
    "            image, mask = self._cache[idx]\n",
    "        else:\n",
    "            image, mask = self._load_and_process(idx)\n",
    "\n",
    "            # Add to cache if not full\n",
    "            if len(self._cache) < self.cache_size:\n",
    "                self._cache[idx] = (image.copy(), mask.copy())\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "\n",
    "        # Convert to tensors\n",
    "        image = torch.from_numpy(image.astype(np.float32) / 255.0).permute(2, 0, 1)\n",
    "        mask = torch.from_numpy(mask.astype(np.float32))\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "def calculate_metrics(pred_logits, target):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics including mAP at various thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "        pred_logits: Raw model outputs [B, H, W] or [B, 1, H, W]\n",
    "        target: Ground truth masks [B, H, W]\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with IoU, F1-score, accuracy, recall, precision, and mAP metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle different input shapes\n",
    "        if len(pred_logits.shape) == 4:\n",
    "            pred_logits = pred_logits.squeeze(1)\n",
    "        \n",
    "        # Convert to binary predictions\n",
    "        pred_binary = (torch.sigmoid(pred_logits) > 0.5).float()\n",
    "        target_binary = (target > 0.5).float()\n",
    "        \n",
    "        # Flatten for calculation\n",
    "        pred_flat = pred_binary.view(-1)\n",
    "        target_flat = target_binary.view(-1)\n",
    "        \n",
    "        # Calculate confusion matrix components\n",
    "        tp = (pred_flat * target_flat).sum()\n",
    "        fp = (pred_flat * (1 - target_flat)).sum()\n",
    "        fn = ((1 - pred_flat) * target_flat).sum()\n",
    "        tn = ((1 - pred_flat) * (1 - target_flat)).sum()\n",
    "        \n",
    "        # Calculate metrics with zero division protection\n",
    "        eps = 1e-7\n",
    "        \n",
    "        precision = tp / (tp + fp + eps)\n",
    "        recall = tp / (tp + fn + eps)\n",
    "        f1_score = 2 * precision * recall / (precision + recall + eps)\n",
    "        iou = tp / (tp + fp + fn + eps)\n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn + eps)\n",
    "        \n",
    "        # Calculate mAP@0.5-0.95 for segmentation\n",
    "        batch_size = pred_logits.shape[0]\n",
    "        iou_thresholds = torch.arange(0.5, 1.0, 0.05, device=pred_logits.device)\n",
    "        \n",
    "        # Calculate IoU for each image in batch\n",
    "        batch_ious = []\n",
    "        for i in range(batch_size):\n",
    "            pred_img = pred_binary[i].view(-1)\n",
    "            target_img = target_binary[i].view(-1)\n",
    "            \n",
    "            # Calculate IoU for this image\n",
    "            img_tp = (pred_img * target_img).sum()\n",
    "            img_fp = (pred_img * (1 - target_img)).sum()\n",
    "            img_fn = ((1 - pred_img) * target_img).sum()\n",
    "            \n",
    "            img_iou = img_tp / (img_tp + img_fp + img_fn + eps)\n",
    "            batch_ious.append(img_iou)\n",
    "        \n",
    "        batch_ious = torch.stack(batch_ious)\n",
    "        \n",
    "        # Calculate mAP at different thresholds\n",
    "        map_50 = (batch_ious >= 0.5).float().mean()\n",
    "        map_55 = (batch_ious >= 0.55).float().mean()\n",
    "        map_60 = (batch_ious >= 0.6).float().mean()\n",
    "        map_65 = (batch_ious >= 0.65).float().mean()\n",
    "        map_70 = (batch_ious >= 0.7).float().mean()\n",
    "        map_75 = (batch_ious >= 0.75).float().mean()\n",
    "        map_80 = (batch_ious >= 0.8).float().mean()\n",
    "        map_85 = (batch_ious >= 0.85).float().mean()\n",
    "        map_90 = (batch_ious >= 0.9).float().mean()\n",
    "        map_95 = (batch_ious >= 0.95).float().mean()\n",
    "        \n",
    "        return {\n",
    "            \"iou\": iou.item(),\n",
    "            \"f1_score\": f1_score.item(),\n",
    "            \"accuracy\": accuracy.item(),\n",
    "            \"recall\": recall.item(),\n",
    "            \"precision\": precision.item(),\n",
    "            \"map_50\": map_50.item(),\n",
    "            \"map_55\": map_55.item(),\n",
    "            \"map_60\": map_60.item(),\n",
    "            \"map_65\": map_65.item(),\n",
    "            \"map_70\": map_70.item(),\n",
    "            \"map_75\": map_75.item(),\n",
    "            \"map_80\": map_80.item(),\n",
    "            \"map_85\": map_85.item(),\n",
    "            \"map_90\": map_90.item(),\n",
    "            \"map_95\": map_95.item(),\n",
    "            \"mean_iou\": batch_ious.mean().item(),\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Metrics calculation error: {e}\")\n",
    "        # Return zero metrics on error\n",
    "        return {key: 0.0 for key in [\n",
    "            \"iou\", \"f1_score\", \"accuracy\", \"recall\", \"precision\",\n",
    "            \"map_50\", \"map_55\", \"map_60\", \"map_65\", \"map_70\",\n",
    "            \"map_75\", \"map_80\", \"map_85\", \"map_90\", \"map_95\", \"mean_iou\"\n",
    "        ]}\n",
    "\n",
    "\n",
    "def create_model(config):\n",
    "    \"\"\"\n",
    "    Create segmentation model based on configuration.\n",
    "    \n",
    "    Parameters:\n",
    "        config: Dictionary with architecture, backbone, encoder_weights, num_classes\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model, aux_params)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        architecture = config[\"architecture\"].lower()\n",
    "        \n",
    "        aux_params = {\n",
    "            \"dropout\": 0.2,\n",
    "            \"activation\": None,\n",
    "            \"classes\": 1\n",
    "        }\n",
    "        \n",
    "        # Create model based on architecture\n",
    "        if architecture == \"unet\":\n",
    "            model = smp.Unet(\n",
    "                encoder_name=config[\"backbone\"],\n",
    "                encoder_weights=config[\"encoder_weights\"],\n",
    "                in_channels=3,\n",
    "                classes=config[\"num_classes\"],\n",
    "                activation=None,\n",
    "                aux_params=aux_params,\n",
    "            )\n",
    "        elif architecture == \"unetplusplus\":\n",
    "            model = smp.UnetPlusPlus(\n",
    "                encoder_name=config[\"backbone\"],\n",
    "                encoder_weights=config[\"encoder_weights\"],\n",
    "                in_channels=3,\n",
    "                classes=config[\"num_classes\"],\n",
    "                activation=None,\n",
    "                aux_params=aux_params,\n",
    "            )\n",
    "        elif architecture == \"manet\":\n",
    "            model = smp.MAnet(\n",
    "                encoder_name=config[\"backbone\"],\n",
    "                encoder_weights=config[\"encoder_weights\"],\n",
    "                in_channels=3,\n",
    "                classes=config[\"num_classes\"],\n",
    "                activation=None,\n",
    "                aux_params=aux_params,\n",
    "            )\n",
    "        elif architecture == \"linknet\":\n",
    "            model = smp.Linknet(\n",
    "                encoder_name=config[\"backbone\"],\n",
    "                encoder_weights=config[\"encoder_weights\"],\n",
    "                in_channels=3,\n",
    "                classes=config[\"num_classes\"],\n",
    "                activation=None,\n",
    "                aux_params=aux_params,\n",
    "            )\n",
    "        elif architecture == \"fpn\":\n",
    "            model = smp.FPN(\n",
    "                encoder_name=config[\"backbone\"],\n",
    "                encoder_weights=config[\"encoder_weights\"],\n",
    "                in_channels=3,\n",
    "                classes=config[\"num_classes\"],\n",
    "                activation=None,\n",
    "                aux_params=aux_params,\n",
    "            )\n",
    "        elif architecture == \"pan\":\n",
    "            model = smp.PAN(\n",
    "                encoder_name=config[\"backbone\"],\n",
    "                encoder_weights=config[\"encoder_weights\"],\n",
    "                in_channels=3,\n",
    "                classes=config[\"num_classes\"],\n",
    "                activation=None,\n",
    "                aux_params=aux_params,\n",
    "            )\n",
    "        elif architecture == \"pspnet\":\n",
    "            model = smp.PSPNet(\n",
    "                encoder_name=config[\"backbone\"],\n",
    "                encoder_weights=config[\"encoder_weights\"],\n",
    "                in_channels=3,\n",
    "                classes=config[\"num_classes\"],\n",
    "                activation=None,\n",
    "                aux_params=aux_params,\n",
    "            )\n",
    "        elif architecture == \"segformer\":\n",
    "            model = smp.Segformer(\n",
    "                encoder_name=config[\"backbone\"],\n",
    "                encoder_weights=config[\"encoder_weights\"],\n",
    "                in_channels=3,\n",
    "                classes=config[\"num_classes\"],\n",
    "                activation=None,\n",
    "                aux_params=aux_params,\n",
    "            )\n",
    "        elif architecture == \"deeplabv3\":\n",
    "            model = smp.DeepLabV3Plus(\n",
    "                encoder_name=config[\"backbone\"],\n",
    "                encoder_weights=config[\"encoder_weights\"],\n",
    "                in_channels=3,\n",
    "                classes=config[\"num_classes\"],\n",
    "                activation=None,\n",
    "                aux_params=aux_params,\n",
    "            )\n",
    "        elif architecture == \"dpt\":\n",
    "            model = smp.DPT(\n",
    "                encoder_name=config[\"backbone\"],\n",
    "                encoder_weights=config[\"encoder_weights\"],\n",
    "                in_channels=3,\n",
    "                classes=config[\"num_classes\"],\n",
    "                activation=None,\n",
    "                dynamic_img_size=True,\n",
    "                aux_params=aux_params,\n",
    "            )\n",
    "        elif architecture == \"upernet\":\n",
    "            model = smp.UPerNet(\n",
    "                encoder_name=config[\"backbone\"],\n",
    "                encoder_weights=config[\"encoder_weights\"],\n",
    "                in_channels=3,\n",
    "                classes=config[\"num_classes\"],\n",
    "                activation=None,\n",
    "                aux_params=aux_params,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Architecture {config['architecture']} not supported\")\n",
    "\n",
    "        return model, aux_params\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating model: {e}\")\n",
    "        logger.error(f\"Config: {config}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def corriger_state_dict(model, model_path, logger):\n",
    "    \"\"\"\n",
    "    Load state dict with robust handling of naming mismatches.\n",
    "    \n",
    "    Parameters:\n",
    "        model: PyTorch model instance\n",
    "        model_path: Path to model checkpoint\n",
    "        logger: Logger instance\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating success\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "        \n",
    "        # Get model's expected keys\n",
    "        model_keys = set(model.state_dict().keys())\n",
    "        state_dict_keys = set(state_dict.keys())\n",
    "        \n",
    "        # If keys match exactly, load directly\n",
    "        if model_keys == state_dict_keys:\n",
    "            model.load_state_dict(state_dict)\n",
    "            logger.debug(\"State dict loaded directly (perfect match)\")\n",
    "            return True\n",
    "        \n",
    "        logger.debug(\"Keys don't match exactly, trying to fix...\")\n",
    "        \n",
    "        # Apply common fixes\n",
    "        fixed_state_dict = {}\n",
    "        used_keys = set()\n",
    "        \n",
    "        for model_key in model_keys:\n",
    "            found = False\n",
    "            \n",
    "            # Try exact match first\n",
    "            if model_key in state_dict:\n",
    "                fixed_state_dict[model_key] = state_dict[model_key]\n",
    "                used_keys.add(model_key)\n",
    "                found = True\n",
    "            else:\n",
    "                # Try common transformations\n",
    "                possible_keys = [\n",
    "                    model_key,\n",
    "                    # FastViT fix\n",
    "                    model_key.replace('encoder.', 'encoder.model.'),\n",
    "                    model_key.replace('encoder.model.', 'encoder.'),\n",
    "                    # Module prefix fixes\n",
    "                    f\"module.{model_key}\",\n",
    "                    model_key.replace('module.', ''),\n",
    "                    # Backbone/encoder swaps\n",
    "                    model_key.replace('encoder.', 'backbone.'),\n",
    "                    model_key.replace('backbone.', 'encoder.'),\n",
    "                    # Decoder fixes\n",
    "                    model_key.replace('decoder.', 'segmentation_head.'),\n",
    "                    model_key.replace('segmentation_head.', 'decoder.'),\n",
    "                ]\n",
    "                \n",
    "                for possible_key in possible_keys:\n",
    "                    if possible_key in state_dict and possible_key not in used_keys:\n",
    "                        fixed_state_dict[model_key] = state_dict[possible_key]\n",
    "                        used_keys.add(possible_key)\n",
    "                        found = True\n",
    "                        break\n",
    "            \n",
    "            if not found:\n",
    "                logger.debug(f\"Could not find match for: {model_key}\")\n",
    "        \n",
    "        # Check if we have enough keys\n",
    "        match_percentage = len(fixed_state_dict) / len(model_keys) * 100\n",
    "        logger.debug(f\"Matched {len(fixed_state_dict)}/{len(model_keys)} keys ({match_percentage:.1f}%)\")\n",
    "        \n",
    "        if match_percentage >= 80:\n",
    "            try:\n",
    "                model.load_state_dict(fixed_state_dict, strict=False)\n",
    "                logger.debug(\"State dict loaded with fixes\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load fixed state dict: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            logger.error(f\"Too few keys matched ({match_percentage:.1f}%), skipping model\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading state dict: {e}\")\n",
    "        return False\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory cache and perform garbage collection.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "# Test single model evaluation\n",
    "def test_single_model(df_models, test_paths, img_size, batch_size=4):\n",
    "    \"\"\"\n",
    "    Test evaluation with single model to debug issues.\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model information\n",
    "        test_paths: Dictionary with test image and mask paths\n",
    "        img_size: Target image size\n",
    "        batch_size: Batch size for inference\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation result dictionary\n",
    "    \"\"\"\n",
    "    print(\"Testing single model evaluation...\")\n",
    "    \n",
    "    # Take first model\n",
    "    model_info = df_models.iloc[0].to_dict()\n",
    "    \n",
    "    print(f\"Testing: {model_info['architecture']}_{model_info['backbone']}\")\n",
    "    print(f\"Model path: {model_info['model_path']}\")\n",
    "    print(f\"Path exists: {os.path.exists(model_info['model_path'])}\")\n",
    "    \n",
    "    result, per_image_data = evaluate_single_model_with_per_image(\n",
    "        model_info, test_paths, img_size, batch_size, 0, track_per_image=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Result: {result['evaluation_status']}\")\n",
    "    if result['evaluation_status'] == 'success':\n",
    "        print(f\"IoU: {result['eval_test_iou']:.3f}\")\n",
    "        print(f\"Parameters: {result['num_params_M']:.1f}M\")\n",
    "        print(f\"Images processed: {len(per_image_data)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def worker_thread_safe(gpu_id, models_subset, test_paths, img_size, batch_size, track_per_image):\n",
    "    \"\"\"\n",
    "    Thread-safe worker for multi-GPU evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "        gpu_id: GPU device ID\n",
    "        models_subset: List of model info dictionaries to process\n",
    "        test_paths: Test dataset paths\n",
    "        img_size: Target image size\n",
    "        batch_size: Batch size for inference\n",
    "        track_per_image: Whether to track per-image metrics\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model_results, per_image_results)\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    \n",
    "    # Initialize CUDA context\n",
    "    with torch.cuda.device(gpu_id):\n",
    "        torch.randn(1).to(f'cuda:{gpu_id}')\n",
    "    \n",
    "    thread_name = f\"GPU-{gpu_id}\"\n",
    "    \n",
    "    # Create individual progress bar for this thread\n",
    "    pbar = tqdm(\n",
    "        total=len(models_subset),\n",
    "        desc=f\"GPU {gpu_id}\",\n",
    "        position=gpu_id,\n",
    "        leave=True,\n",
    "        colour=['red', 'green', 'blue', 'yellow', 'magenta', 'cyan'][gpu_id % 6]\n",
    "    )\n",
    "    \n",
    "    # Create dataset and loader for this thread\n",
    "    try:\n",
    "        test_dataset = SimpleSegmentationDataset(\n",
    "            test_paths[\"images\"],\n",
    "            test_paths[\"masks\"],\n",
    "            img_size,\n",
    "            None,\n",
    "            cache_size=50,\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "    except Exception as loader_error:\n",
    "        pbar.set_description(f\"GPU {gpu_id}: DataLoader Failed\")\n",
    "        pbar.close()\n",
    "        print(f\"{thread_name}: Failed to create DataLoader: {loader_error}\")\n",
    "        return [], []\n",
    "    \n",
    "    model_results = []\n",
    "    per_image_results = []\n",
    "    successful = 0\n",
    "    \n",
    "    try:\n",
    "        for i, model_info in enumerate(models_subset):\n",
    "            model_start_time = time.time()\n",
    "            model_name = f\"{model_info['architecture']}_{model_info['backbone']}\"\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_description(f\"GPU {gpu_id}: {model_name[:20]}\")\n",
    "            \n",
    "            try:\n",
    "                # Suppress warnings during evaluation\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    \n",
    "                    # Evaluate model\n",
    "                    result, per_image_data = evaluate_single_model_with_per_image(\n",
    "                        model_info, test_paths, img_size, batch_size, gpu_id, \n",
    "                        track_per_image, test_loader, test_dataset\n",
    "                    )\n",
    "                \n",
    "                model_results.append(result)\n",
    "                per_image_results.extend(per_image_data)\n",
    "                \n",
    "                model_time = time.time() - model_start_time\n",
    "                \n",
    "                if result[\"evaluation_status\"] == \"success\":\n",
    "                    successful += 1\n",
    "                    \n",
    "                    # Update progress bar with success info\n",
    "                    pbar.set_postfix({\n",
    "                        'Status': '✓',\n",
    "                        'IoU': f\"{result['eval_test_iou']:.3f}\",\n",
    "                        'Time': f\"{model_time:.1f}s\",\n",
    "                        'Success': f\"{successful}/{i+1}\"\n",
    "                    })\n",
    "                else:\n",
    "                    # Update progress bar with failure info\n",
    "                    pbar.set_postfix({\n",
    "                        'Status': '✗', \n",
    "                        'Error': 'Failed',\n",
    "                        'Time': f\"{model_time:.1f}s\",\n",
    "                        'Success': f\"{successful}/{i+1}\"\n",
    "                    })\n",
    "                          \n",
    "            except Exception as model_error:\n",
    "                # Update progress bar with exception info\n",
    "                pbar.set_postfix({\n",
    "                    'Status': '!',\n",
    "                    'Error': str(model_error)[:10],\n",
    "                    'Success': f\"{successful}/{i+1}\"\n",
    "                })\n",
    "                \n",
    "                # Add failed result\n",
    "                result = model_info.copy()\n",
    "                result.update({\n",
    "                    \"eval_test_iou\": np.nan,\n",
    "                    \"eval_test_f1_score\": np.nan,\n",
    "                    \"eval_test_accuracy\": np.nan,\n",
    "                    \"eval_test_recall\": np.nan,\n",
    "                    \"eval_test_precision\": np.nan,\n",
    "                    \"eval_test_map_50\": np.nan,\n",
    "                    \"eval_test_map_55\": np.nan,\n",
    "                    \"eval_test_map_60\": np.nan,\n",
    "                    \"eval_test_map_65\": np.nan,\n",
    "                    \"eval_test_map_70\": np.nan,\n",
    "                    \"eval_test_map_75\": np.nan,\n",
    "                    \"eval_test_map_80\": np.nan,\n",
    "                    \"eval_test_map_85\": np.nan,\n",
    "                    \"eval_test_map_90\": np.nan,\n",
    "                    \"eval_test_map_95\": np.nan,\n",
    "                    \"eval_test_mean_iou\": np.nan,\n",
    "                    \"num_params_M\": np.nan,\n",
    "                    \"evaluation_status\": f\"failed: {str(model_error)[:50]}\",\n",
    "                    \"device_id\": gpu_id,\n",
    "                    \"num_test_images\": 0\n",
    "                })\n",
    "                model_results.append(result)\n",
    "            \n",
    "            # Update progress\n",
    "            pbar.update(1)\n",
    "    \n",
    "    except Exception as thread_error:\n",
    "        pbar.set_postfix({'Status': 'X', 'Error': 'Thread Failed'})\n",
    "        print(f\"{thread_name}: Thread failed with error: {thread_error}\")\n",
    "    \n",
    "    finally:\n",
    "        # Final progress bar update\n",
    "        pbar.set_description(f\"GPU {gpu_id}: Complete\")\n",
    "        pbar.set_postfix({\n",
    "            'Status': 'Done',\n",
    "            'Success': f\"{successful}/{len(models_subset)}\",\n",
    "            'Rate': f\"{successful/len(models_subset)*100:.0f}%\"\n",
    "        })\n",
    "        pbar.close()\n",
    "        \n",
    "        try:\n",
    "            # Clean up resources\n",
    "            del test_loader, test_dataset\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"{thread_name}: Error clearing memory: {e}\")\n",
    "            pass\n",
    "    \n",
    "    return model_results, per_image_results\n",
    "\n",
    "\n",
    "def evaluate_single_model_with_per_image(model_info, test_paths, img_size, batch_size, device_id, track_per_image, test_loader, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate single model with optional per-image tracking.\n",
    "    \n",
    "    Parameters:\n",
    "        model_info: Dictionary with model configuration\n",
    "        test_paths: Test dataset paths\n",
    "        img_size: Target image size\n",
    "        batch_size: Batch size for inference\n",
    "        device_id: GPU device ID\n",
    "        track_per_image: Whether to track per-image metrics\n",
    "        test_loader: DataLoader instance\n",
    "        test_dataset: Dataset instance\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model_results, per_image_results)\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    \n",
    "    try:\n",
    "        # Set device\n",
    "        device = torch.device(f'cuda:{device_id}')\n",
    "        \n",
    "        # Use context manager for CUDA device\n",
    "        with torch.cuda.device(device_id):\n",
    "            \n",
    "            # Extract config from model_info\n",
    "            config = {\n",
    "                \"architecture\": model_info[\"architecture\"],\n",
    "                \"backbone\": model_info[\"backbone\"],\n",
    "                \"encoder_weights\": model_info.get(\"encoder_weights\"),\n",
    "                \"num_classes\": int(model_info.get(\"num_classes\", 1)),\n",
    "                \"model_path\": model_info[\"model_path\"],\n",
    "            }\n",
    "            \n",
    "            # Check if model file exists\n",
    "            if not os.path.exists(config[\"model_path\"]):\n",
    "                raise Exception(f\"Model file not found: {config['model_path']}\")\n",
    "            \n",
    "            # Create and load model\n",
    "            import warnings\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                model, aux_params = create_model(config)\n",
    "\n",
    "            # Load weights\n",
    "            import logging\n",
    "            logger = logging.getLogger('silent')\n",
    "            logger.setLevel(logging.CRITICAL)\n",
    "            \n",
    "            if not corriger_state_dict(model, config[\"model_path\"], logger):\n",
    "                raise Exception(\"Failed to load model weights\")\n",
    "            \n",
    "            model = model.to(device)\n",
    "            \n",
    "            # Disable auxiliary head for speed\n",
    "            if hasattr(model, 'aux_params'):\n",
    "                model.aux_params = None\n",
    "            \n",
    "            num_params_M = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1_000_000\n",
    "            \n",
    "            # Evaluate model\n",
    "            model.eval()\n",
    "            all_metrics = {\n",
    "                \"iou\": [],\n",
    "                \"f1_score\": [],\n",
    "                \"accuracy\": [],\n",
    "                \"recall\": [],\n",
    "                \"precision\": [],\n",
    "                \"map_50\": [],\n",
    "                \"map_55\": [],\n",
    "                \"map_60\": [],\n",
    "                \"map_65\": [],\n",
    "                \"map_70\": [],\n",
    "                \"map_75\": [],\n",
    "                \"map_80\": [],\n",
    "                \"map_85\": [],\n",
    "                \"map_90\": [],\n",
    "                \"map_95\": [],\n",
    "                \"mean_iou\": [],\n",
    "            }\n",
    "            \n",
    "            # Per-image tracking\n",
    "            per_image_results = []\n",
    "            image_counter = 0\n",
    "            \n",
    "            # Create new iterator for each model\n",
    "            try:\n",
    "                data_iter = iter(test_loader)\n",
    "            except Exception as iter_error:\n",
    "                raise Exception(f\"Failed to create data iterator: {iter_error}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_idx = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        # Get next batch\n",
    "                        try:\n",
    "                            images, masks = next(data_iter)\n",
    "                        except StopIteration:\n",
    "                            break\n",
    "                        except Exception as batch_error:\n",
    "                            print(f\"GPU {device_id}: Batch loading error: {batch_error}\")\n",
    "                            continue\n",
    "                        \n",
    "                        images = images.to(device, non_blocking=True)\n",
    "                        masks = masks.to(device, non_blocking=True)\n",
    "                        \n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(images)\n",
    "                            \n",
    "                            # Handle tuple outputs (from aux_params)\n",
    "                            if isinstance(outputs, tuple):\n",
    "                                outputs = outputs[0]\n",
    "                            \n",
    "                            # Keep raw outputs for metrics\n",
    "                            raw_outputs = outputs.squeeze(1) if len(outputs.shape) == 4 else outputs\n",
    "                        \n",
    "                        if track_per_image:\n",
    "                            # Calculate metrics for each image\n",
    "                            for i in range(len(images)):\n",
    "                                # Pass raw logits to metrics calculation\n",
    "                                single_raw_output = raw_outputs[i : i + 1]\n",
    "                                single_mask = masks[i : i + 1]\n",
    "                                \n",
    "                                # Calculate metrics\n",
    "                                batch_metrics = calculate_metrics(single_raw_output, single_mask)\n",
    "                                \n",
    "                                # Add to overall metrics\n",
    "                                for key, value in batch_metrics.items():\n",
    "                                    all_metrics[key].append(value)\n",
    "                                \n",
    "                                # Per-image result\n",
    "                                per_image_result = {\n",
    "                                    \"image_id\": image_counter,\n",
    "                                    \"batch_idx\": batch_idx,\n",
    "                                    \"image_in_batch\": i,\n",
    "                                    **batch_metrics,\n",
    "                                }\n",
    "                                \n",
    "                                # Add model info\n",
    "                                per_image_result.update({\n",
    "                                    \"architecture\": model_info[\"architecture\"],\n",
    "                                    \"backbone\": model_info[\"backbone\"],\n",
    "                                    \"model_path\": model_info[\"model_path\"],\n",
    "                                    \"device_id\": device_id,\n",
    "                                    \"num_params_M\": num_params_M,\n",
    "                                })\n",
    "                                \n",
    "                                # Add additional metadata\n",
    "                                for key in [\"validation_fold\", \"config\", \"model_start_date\", \"model_end_date\", \"total_training_time_hour\"]:\n",
    "                                    if key in model_info:\n",
    "                                        per_image_result[key] = model_info[key]\n",
    "                                \n",
    "                                per_image_results.append(per_image_result)\n",
    "                                image_counter += 1\n",
    "                        else:\n",
    "                            # Batch-level calculation\n",
    "                            batch_metrics = calculate_metrics(raw_outputs, masks)\n",
    "                            for key, value in batch_metrics.items():\n",
    "                                all_metrics[key].append(value)\n",
    "                        \n",
    "                        # Clean up batch tensors\n",
    "                        del images, masks, outputs, raw_outputs\n",
    "                        \n",
    "                        batch_idx += 1\n",
    "                        \n",
    "                    except Exception as batch_error:\n",
    "                        print(f\"GPU {device_id}: Batch {batch_idx} failed: {batch_error}\")\n",
    "                        batch_idx += 1\n",
    "                        continue\n",
    "            \n",
    "            # Calculate average metrics\n",
    "            if any(all_metrics.values()):\n",
    "                avg_metrics = {key: np.mean(values) if values else 0.0 for key, values in all_metrics.items()}\n",
    "            else:\n",
    "                avg_metrics = {key: 0.0 for key in all_metrics.keys()}\n",
    "            \n",
    "            # Return result dictionary\n",
    "            result = model_info.copy()\n",
    "            result.update({\n",
    "                \"eval_test_iou\": avg_metrics[\"iou\"],\n",
    "                \"eval_test_f1_score\": avg_metrics[\"f1_score\"],\n",
    "                \"eval_test_accuracy\": avg_metrics[\"accuracy\"],\n",
    "                \"eval_test_recall\": avg_metrics[\"recall\"],\n",
    "                \"eval_test_precision\": avg_metrics[\"precision\"],\n",
    "                \"eval_test_map_50\": avg_metrics[\"map_50\"],\n",
    "                \"eval_test_map_55\": avg_metrics[\"map_55\"],\n",
    "                \"eval_test_map_60\": avg_metrics[\"map_60\"],\n",
    "                \"eval_test_map_65\": avg_metrics[\"map_65\"],\n",
    "                \"eval_test_map_70\": avg_metrics[\"map_70\"],\n",
    "                \"eval_test_map_75\": avg_metrics[\"map_75\"],\n",
    "                \"eval_test_map_80\": avg_metrics[\"map_80\"],\n",
    "                \"eval_test_map_85\": avg_metrics[\"map_85\"],\n",
    "                \"eval_test_map_90\": avg_metrics[\"map_90\"],\n",
    "                \"eval_test_map_95\": avg_metrics[\"map_95\"],\n",
    "                \"eval_test_mean_iou\": avg_metrics[\"mean_iou\"],\n",
    "                \"num_params_M\": num_params_M,\n",
    "                \"evaluation_status\": \"success\",\n",
    "                \"device_id\": device_id,\n",
    "                \"num_test_images\": image_counter if track_per_image else len(test_dataset)\n",
    "            })\n",
    "            \n",
    "            return result, per_image_results if track_per_image else []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"GPU {device_id}: Model evaluation failed: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"GPU {device_id}: Traceback: {traceback.format_exc()}\")\n",
    "        \n",
    "        # Return failed result\n",
    "        result = model_info.copy()\n",
    "        result.update({\n",
    "            \"eval_test_iou\": np.nan,\n",
    "            \"eval_test_f1_score\": np.nan,\n",
    "            \"eval_test_accuracy\": np.nan,\n",
    "            \"eval_test_recall\": np.nan,\n",
    "            \"eval_test_precision\": np.nan,\n",
    "            \"eval_test_map_50\": np.nan,\n",
    "            \"eval_test_map_55\": np.nan,\n",
    "            \"eval_test_map_60\": np.nan,\n",
    "            \"eval_test_map_65\": np.nan,\n",
    "            \"eval_test_map_70\": np.nan,\n",
    "            \"eval_test_map_75\": np.nan,\n",
    "            \"eval_test_map_80\": np.nan,\n",
    "            \"eval_test_map_85\": np.nan,\n",
    "            \"eval_test_map_90\": np.nan,\n",
    "            \"eval_test_map_95\": np.nan,\n",
    "            \"eval_test_mean_iou\": np.nan,\n",
    "            \"num_params_M\": np.nan,\n",
    "            \"evaluation_status\": f\"failed: {str(e)[:50]}\",\n",
    "            \"device_id\": device_id,\n",
    "            \"num_test_images\": 0\n",
    "        })\n",
    "        \n",
    "        return result, []\n",
    "        \n",
    "    finally:\n",
    "        try:\n",
    "            if model is not None:\n",
    "                del model\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                torch.cuda.reset_peak_memory_stats(device)\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"GPU {device_id}: Cleanup error: {cleanup_error}\")\n",
    "\n",
    "\n",
    "def evaluate_models_threading_with_per_image(df_models, test_paths, img_size, batch_size=4, num_gpus=4, track_per_image=True, timeout_per_model=300):\n",
    "    \"\"\"\n",
    "    Multi-GPU model evaluation using threading.\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model information\n",
    "        test_paths: Test dataset paths\n",
    "        img_size: Target image size\n",
    "        batch_size: Batch size for inference\n",
    "        num_gpus: Number of GPUs to use\n",
    "        track_per_image: Whether to track per-image metrics\n",
    "        timeout_per_model: Timeout per model in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model_results_df, per_image_results_df)\n",
    "    \"\"\"\n",
    "    print(\"Starting Multi-GPU Evaluation with Progress Tracking\")\n",
    "    print(f\"Models: {len(df_models)} | GPUs: {num_gpus} | Batch Size: {batch_size}\")\n",
    "    print(f\"Per-image tracking: {'Enabled' if track_per_image else 'Disabled'}\")\n",
    "    print(f\"Timeout per model: {timeout_per_model}s\")\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    all_models_list = []\n",
    "    for idx, row in df_models.iterrows():\n",
    "        model_dict = row.to_dict()\n",
    "        model_dict['original_index'] = idx\n",
    "        all_models_list.append(model_dict)\n",
    "    \n",
    "    # Split models across GPUs evenly\n",
    "    models_per_gpu = len(all_models_list) // num_gpus\n",
    "    remainder = len(all_models_list) % num_gpus\n",
    "    \n",
    "    gpu_assignments = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for gpu_id in range(num_gpus):\n",
    "        # Add extra model to first GPUs if remainder exists\n",
    "        current_batch_size = models_per_gpu + (1 if gpu_id < remainder else 0)\n",
    "        end_idx = start_idx + current_batch_size\n",
    "        \n",
    "        gpu_models = all_models_list[start_idx:end_idx]\n",
    "        gpu_assignments.append((gpu_id, gpu_models))\n",
    "        print(f\"GPU {gpu_id}: {len(gpu_models)} models\")\n",
    "        \n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Create main progress bar\n",
    "    main_pbar = tqdm(\n",
    "        total=len(df_models),\n",
    "        desc=\"Overall Progress\",\n",
    "        position=num_gpus,\n",
    "        colour='white'\n",
    "    )\n",
    "    \n",
    "    # Run threads\n",
    "    start_time = time.time()\n",
    "    all_model_results = []\n",
    "    all_per_image_results = []\n",
    "    \n",
    "    # Calculate total timeout\n",
    "    max_models_per_gpu = max(len(models) for _, models in gpu_assignments)\n",
    "    total_timeout = max_models_per_gpu * timeout_per_model + 60  # 60s buffer\n",
    "    \n",
    "    print(f\"Total timeout: {total_timeout}s ({total_timeout//60:.1f} minutes)\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Starting GPU workers...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_gpus, thread_name_prefix=\"GPU-Worker\") as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_gpu = {}\n",
    "        for gpu_id, models in gpu_assignments:\n",
    "            future = executor.submit(\n",
    "                worker_thread_safe,\n",
    "                gpu_id, \n",
    "                models, \n",
    "                test_paths, \n",
    "                img_size, \n",
    "                batch_size, \n",
    "                track_per_image\n",
    "            )\n",
    "            future_to_gpu[future] = (gpu_id, len(models))\n",
    "        \n",
    "        # Collect results\n",
    "        completed_gpus = set()\n",
    "        \n",
    "        try:\n",
    "            for future in as_completed(future_to_gpu, timeout=total_timeout):\n",
    "                gpu_id, num_models = future_to_gpu[future]\n",
    "                try:\n",
    "                    model_results, per_image_results = future.result(timeout=30)\n",
    "                    all_model_results.extend(model_results)\n",
    "                    all_per_image_results.extend(per_image_results)\n",
    "                    completed_gpus.add(gpu_id)\n",
    "                    \n",
    "                    # Update main progress bar\n",
    "                    main_pbar.update(num_models)\n",
    "                    main_pbar.set_postfix({\n",
    "                        'Completed_GPUs': f\"{len(completed_gpus)}/{num_gpus}\",\n",
    "                        'Success_Rate': f\"{sum(1 for r in all_model_results if r.get('evaluation_status') == 'success')}/{len(all_model_results)}\"\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"GPU {gpu_id} completed successfully ({len(model_results)} models)\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"GPU {gpu_id} failed: {e}\")\n",
    "                    main_pbar.update(num_models)\n",
    "                    \n",
    "        except TimeoutError:\n",
    "            print(f\"Evaluation timed out after {total_timeout}s\")\n",
    "            \n",
    "            # Cancel remaining futures\n",
    "            for future in future_to_gpu:\n",
    "                if not future.done():\n",
    "                    gpu_id, _ = future_to_gpu[future]\n",
    "                    print(f\"Cancelling GPU {gpu_id}\")\n",
    "                    future.cancel()\n",
    "        \n",
    "        finally:\n",
    "            main_pbar.close()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Create DataFrames\n",
    "    df_model_results = pd.DataFrame(all_model_results)\n",
    "    df_per_image_results = pd.DataFrame(all_per_image_results) if track_per_image else pd.DataFrame()\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in all_model_results if r.get(\"evaluation_status\") == \"success\")\n",
    "    total_images = len(all_per_image_results) if track_per_image else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EVALUATION COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total Time: {total_time//60:.0f}m {total_time%60:.0f}s\")\n",
    "    print(f\"Models Processed: {len(all_model_results)}\")\n",
    "    print(f\"Successful Models: {successful}/{len(df_models)} ({100*successful/len(df_models):.1f}%)\")\n",
    "    if len(all_model_results) > 0:\n",
    "        print(f\"Model Processing Speed: {len(all_model_results)*3600/total_time:.1f} models/hour\")\n",
    "    print(f\"Completed GPUs: {len(completed_gpus)}/{num_gpus}\")\n",
    "    \n",
    "    if track_per_image and total_images > 0:\n",
    "        print(f\"Total Images Evaluated: {total_images:,}\")\n",
    "        print(f\"Image Processing Speed: {total_images*3600/total_time:.0f} images/hour\")\n",
    "        \n",
    "        # Show per-image statistics\n",
    "        if successful > 0:\n",
    "            successful_images = df_per_image_results[\n",
    "                df_per_image_results['architecture'].isin(\n",
    "                    df_model_results[df_model_results['evaluation_status'] == 'success']['architecture']\n",
    "                )\n",
    "            ]\n",
    "            if len(successful_images) > 0:\n",
    "                avg_image_iou = successful_images['iou'].mean()\n",
    "                std_image_iou = successful_images['iou'].std()\n",
    "                print(f\"Average Image IoU: {avg_image_iou:.3f} ± {std_image_iou:.3f}\")\n",
    "    \n",
    "    print(f\"Model Results DataFrame shape: {df_model_results.shape}\")\n",
    "    if track_per_image:\n",
    "        print(f\"Per-Image Results DataFrame shape: {df_per_image_results.shape}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return df_model_results, df_per_image_results\n",
    "\n",
    "def save_results_with_per_image(df_model_results, df_per_image_results, output_path, filename_prefix=\"model_evaluation\"):\n",
    "    \"\"\"\n",
    "    Save evaluation results to parquet files.\n",
    "    \n",
    "    Parameters:\n",
    "        df_model_results: DataFrame with model-level results\n",
    "        df_per_image_results: DataFrame with per-image results\n",
    "        output_path: Output directory path\n",
    "        filename_prefix: Prefix for output filenames\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model_filepath, per_image_filepath)\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Save model results\n",
    "    model_filename = f\"{filename_prefix}_models_{timestamp}.parquet\"\n",
    "    model_filepath = os.path.join(output_path, model_filename)\n",
    "    df_model_results.to_parquet(model_filepath, index=False)\n",
    "    \n",
    "    print(f\"Model results saved to: {model_filepath}\")\n",
    "    print(f\"Saved {len(df_model_results)} model results\")\n",
    "    \n",
    "    # Save per-image results if available\n",
    "    per_image_filepath = None\n",
    "    if len(df_per_image_results) > 0:\n",
    "        per_image_filename = f\"{filename_prefix}_per_image_{timestamp}.parquet\"\n",
    "        per_image_filepath = os.path.join(output_path, per_image_filename)\n",
    "        df_per_image_results.to_parquet(per_image_filepath, index=False)\n",
    "        \n",
    "        print(f\"Per-image results saved to: {per_image_filepath}\")\n",
    "        print(f\"Saved {len(df_per_image_results)} per-image results\")\n",
    "    else:\n",
    "        print(\"No per-image results to save\")\n",
    "    \n",
    "    return model_filepath, per_image_filepath\n",
    "\n",
    "# Backward compatibility functions\n",
    "def evaluate_models_threading(df_models, test_paths, img_size, batch_size=4, num_gpus=4):\n",
    "    \"\"\"Backward compatible function - returns only model results\"\"\"\n",
    "    df_model_results, _ = evaluate_models_threading_with_per_image(\n",
    "        df_models, test_paths, img_size, batch_size, num_gpus, track_per_image=False\n",
    "    )\n",
    "    return df_model_results\n",
    "\n",
    "def save_results_clean(df_results, output_path, filename_prefix=\"model_evaluation\"):\n",
    "    \"\"\"Backward compatible save function\"\"\"\n",
    "    model_filepath, _ = save_results_with_per_image(\n",
    "        df_results, pd.DataFrame(), output_path, filename_prefix\n",
    "    )\n",
    "    return model_filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAJ_DATASET:\n",
    "    test_paths = load_test_data(TEST_DATASET_FILE, DATASET_IMAGES_DIR, DATASET_MASKS_DIR)\n",
    "    \n",
    "    # Run evaluation with per-image tracking\n",
    "    df_model_results, df_per_image_results = evaluate_models_threading_with_per_image(\n",
    "        df_test_models,\n",
    "        test_paths, \n",
    "        IMG_SIZE, \n",
    "        batch_size=2,\n",
    "        num_gpus=4,\n",
    "        track_per_image=True,\n",
    "        timeout_per_model=25\n",
    "    )\n",
    "\n",
    "    # Save both DataFrames\n",
    "    model_file, per_image_file = save_results_with_per_image(\n",
    "        df_model_results, \n",
    "        df_per_image_results, \n",
    "        OUTPUT_PARQUET_PATH\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluation completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    logger.info(\"Skipping dataset loading, using existing DataFrame\")\n",
    "    df_model_results = pd.read_parquet(SAVED_MODEL_RESULTS_PARQUET_PATH)\n",
    "    df_per_image_results = pd.read_parquet(SAVED_PER_IMAGE_RESULTS_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_image_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check DataFrame structure\n",
    "print(\"Original df_model_results columns related to config:\")\n",
    "config_cols = [col for col in df_model_results.columns if 'config' in col.lower()]\n",
    "print(config_cols)\n",
    "\n",
    "print(f\"\\n'config' in columns: {'config' in df_model_results.columns}\")\n",
    "print(f\"'config_' in columns: {'config_' in df_model_results.columns}\")\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "print(df_model_results[['config', 'validation_fold', 'architecture', 'eval_test_iou']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_complete_trainings(df):\n",
    "    \"\"\"\n",
    "    Filter to include only training runs with all 5 folds.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame with model results\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with only complete training runs\n",
    "    \"\"\"\n",
    "    # Count folds per config\n",
    "    fold_counts = df.groupby(['config'])['validation_fold'].agg(['count', 'nunique']).reset_index()\n",
    "    fold_counts.columns = ['config', 'total_folds', 'unique_folds']\n",
    "    \n",
    "    # Identify complete training runs\n",
    "    complete_trainings = fold_counts[fold_counts['unique_folds'] == 5]\n",
    "    \n",
    "    # Check for configs with multiple training sessions\n",
    "    multiple_sessions = fold_counts[fold_counts['total_folds'] > 5]\n",
    "    if len(multiple_sessions) > 0:\n",
    "        print(f\"Warning: {len(multiple_sessions)} configs have more than 5 folds (multiple training sessions)\")\n",
    "        print(\"Using all available folds for these configs (will average across all sessions).\")\n",
    "        complete_trainings = fold_counts[fold_counts['unique_folds'] == 5]\n",
    "    \n",
    "    # Verify fold numbers are exactly 0, 1, 2, 3, 4\n",
    "    expected_folds = set([0, 1, 2, 3, 4])\n",
    "    \n",
    "    def has_all_folds(group):\n",
    "        return set(group['validation_fold'].unique()) == expected_folds\n",
    "    \n",
    "    complete_check = df.groupby(['config']).apply(has_all_folds).reset_index()\n",
    "    complete_check.columns = ['config', 'has_all_folds']\n",
    "    \n",
    "    complete_trainings = complete_trainings.merge(complete_check, on=['config'])\n",
    "    complete_trainings = complete_trainings[complete_trainings['has_all_folds']]\n",
    "    \n",
    "    # Filter original dataframe\n",
    "    df_complete = df.merge(complete_trainings[['config']], on=['config'], how='inner')\n",
    "    \n",
    "    print(f\"Original configs: {df['config'].nunique()}\")\n",
    "    print(f\"Complete configs (all 5 folds): {len(complete_trainings)}\")\n",
    "    print(f\"Filtered from {len(df)} to {len(df_complete)} rows\")\n",
    "    \n",
    "    return df_complete\n",
    "\n",
    "\n",
    "def show_incomplete_trainings(df):\n",
    "    \"\"\"Show which training configurations were incomplete.\"\"\"\n",
    "    fold_summary = df.groupby(['config']).agg({\n",
    "        'validation_fold': lambda x: sorted(list(x.unique()))\n",
    "    }).reset_index()\n",
    "    fold_summary.columns = ['config', 'available_folds']\n",
    "    \n",
    "    incomplete = fold_summary[fold_summary['available_folds'].apply(lambda x: len(x) < 5 or x != [0,1,2,3,4])]\n",
    "    \n",
    "    if len(incomplete) > 0:\n",
    "        print(f\"Incomplete configs removed: {len(incomplete)}\")\n",
    "        for _, row in incomplete.iterrows():\n",
    "            print(f\"  {row['config']}: folds {row['available_folds']}\")\n",
    "    else:\n",
    "        print(\"No incomplete trainings found\")\n",
    "\n",
    "def show_available_metrics(df_clean):\n",
    "    \"\"\"Show available metrics in the cleaned dataset.\"\"\"\n",
    "    print(\"AVAILABLE METRICS IN CLEANED DATASET:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    original_test_cols = [col for col in df_clean.columns if col.startswith('test_') and col.endswith('_mean')]\n",
    "    eval_test_cols = [col for col in df_clean.columns if col.startswith('eval_test_') and col.endswith('_mean')]\n",
    "    val_cols = [col for col in df_clean.columns if any(prefix in col.lower() for prefix in ['val_', 'validation_', 'best_val']) and col.endswith('_mean')]\n",
    "    \n",
    "    exclude_cols = original_test_cols + eval_test_cols + val_cols + ['config', 'model_start_date']\n",
    "    static_cols = [col for col in df_clean.columns if col not in exclude_cols]\n",
    "    \n",
    "    if original_test_cols:\n",
    "        print(\"Original Test Metrics (from JSON):\")\n",
    "        for col in sorted(original_test_cols):\n",
    "            print(f\"  - {col}\")\n",
    "    \n",
    "    if eval_test_cols:\n",
    "        print(\"Evaluation Test Metrics (from re-evaluation):\")\n",
    "        for col in sorted(eval_test_cols):\n",
    "            print(f\"  - {col}\")\n",
    "    \n",
    "    if val_cols:\n",
    "        print(\"Validation Metrics:\")\n",
    "        for col in sorted(val_cols):\n",
    "            print(f\"  - {col}\")\n",
    "    \n",
    "    if static_cols:\n",
    "        print(\"Model Characteristics:\")\n",
    "        for col in sorted(static_cols):\n",
    "            print(f\"  - {col}\")\n",
    "\n",
    "def clean_cv_results(df):\n",
    "    \"\"\"\n",
    "    Clean cross-validation results by aggregating across folds.\n",
    "    \n",
    "    Parameters:\n",
    "        df: Raw DataFrame with all fold results\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with one row per model configuration\n",
    "    \"\"\"\n",
    "    # Filter low performance models\n",
    "    df_filtered = df[df['eval_test_iou'] > 0.2].copy()\n",
    "    \n",
    "    print(f\"Starting with {len(df_filtered)} rows from {df_filtered['config'].nunique()} configs\")\n",
    "    print(f\"(Filtered out {len(df) - len(df_filtered)} rows with IoU <= 0.2)\")\n",
    "    \n",
    "    # Select best evaluation per config/fold\n",
    "    print(\"\\nSelecting best evaluation per fold for each config...\")\n",
    "    \n",
    "    best_per_config_fold = []\n",
    "    configs_processed = 0\n",
    "    configs_kept = 0\n",
    "    \n",
    "    for config in df_filtered['config'].unique():\n",
    "        config_data = df_filtered[df_filtered['config'] == config]\n",
    "        configs_processed += 1\n",
    "        \n",
    "        # Check which folds are available\n",
    "        available_folds = set(config_data['validation_fold'].unique())\n",
    "        required_folds = {0, 1, 2, 3, 4}\n",
    "        \n",
    "        if available_folds == required_folds:\n",
    "            configs_kept += 1\n",
    "            \n",
    "            for fold in range(5):\n",
    "                fold_data = config_data[config_data['validation_fold'] == fold]\n",
    "                if len(fold_data) > 1:\n",
    "                    # Multiple evaluations for this fold - keep the best\n",
    "                    best_idx = fold_data['eval_test_iou'].idxmax()\n",
    "                    best_per_config_fold.append(fold_data.loc[best_idx])\n",
    "                else:\n",
    "                    # Single evaluation for this fold\n",
    "                    best_per_config_fold.append(fold_data.iloc[0])\n",
    "        else:\n",
    "            # Missing some folds\n",
    "            missing_folds = required_folds - available_folds\n",
    "            if configs_processed <= 10:\n",
    "                print(f\"  Skipping {config}: missing folds {sorted(missing_folds)}\")\n",
    "    \n",
    "    print(f\"\\nKept {configs_kept} out of {configs_processed} configs (all with complete 5-fold CV)\")\n",
    "    \n",
    "    # Create dataframe with best evaluations\n",
    "    df_complete = pd.DataFrame(best_per_config_fold)\n",
    "    print(f\"Selected best evaluations: {len(df_complete)} rows from {df_complete['config'].nunique()} configs\")\n",
    "    \n",
    "    # Verify each config has exactly 5 folds\n",
    "    fold_check = df_complete.groupby('config')['validation_fold'].agg(['count', 'nunique'])\n",
    "    assert all(fold_check['count'] == 5), \"Some configs don't have exactly 5 rows\"\n",
    "    assert all(fold_check['nunique'] == 5), \"Some configs don't have 5 unique folds\"\n",
    "    print(\"Verified: All configs have exactly 5 unique folds\")\n",
    "    \n",
    "    # Aggregate by config\n",
    "    print(\"\\nAggregating metrics by config...\")\n",
    "    \n",
    "    # Identify metrics to aggregate\n",
    "    original_test_metrics = [col for col in df_complete.columns if col.startswith('test_')]\n",
    "    eval_test_metrics = [col for col in df_complete.columns if col.startswith('eval_test_')]\n",
    "    val_metrics = [col for col in df_complete.columns if any(prefix in col.lower() for prefix in ['final_val_', 'best_val_'])]\n",
    "    \n",
    "    print(f\"Found {len(original_test_metrics)} original test metrics\")\n",
    "    print(f\"Found {len(eval_test_metrics)} evaluation test metrics\")\n",
    "    print(f\"Found {len(val_metrics)} validation metrics\")\n",
    "    \n",
    "    # Build aggregation dictionary\n",
    "    agg_dict = {}\n",
    "    \n",
    "    # Aggregate all metrics - take mean across folds\n",
    "    for metric in original_test_metrics + eval_test_metrics + val_metrics:\n",
    "        agg_dict[metric] = 'mean'\n",
    "    \n",
    "    # Model characteristics - should be same across folds\n",
    "    static_cols = ['architecture', 'backbone', 'encoder_weights', 'img_size', \n",
    "                   'num_classes', 'batch_size', 'learning_rate', 'epochs', \n",
    "                   'patience', 'accumulation_steps', 'auto_batch_size',\n",
    "                   'min_batch_size_search', 'max_batch_size_search', 'batch_size_test_steps']\n",
    "    \n",
    "    for col in static_cols:\n",
    "        if col in df_complete.columns:\n",
    "            agg_dict[col] = 'first'\n",
    "    \n",
    "    # Training time - sum across all folds\n",
    "    if 'total_training_time_hour' in df_complete.columns:\n",
    "        agg_dict['total_training_time_hour'] = 'sum'\n",
    "    \n",
    "    # Parameter count - should be same across folds\n",
    "    if 'num_params_M' in df_complete.columns:\n",
    "        agg_dict['num_params_M'] = 'first'\n",
    "    \n",
    "    # Group by config and aggregate\n",
    "    df_clean = df_complete.groupby('config').agg(agg_dict)\n",
    "    \n",
    "    # Handle model_start_date separately\n",
    "    if 'model_start_date' in df_complete.columns:\n",
    "        date_agg = df_complete.groupby('config')['model_start_date'].agg(['min', 'max', 'nunique'])\n",
    "        df_clean['first_training_date'] = date_agg['min']\n",
    "        df_clean['last_training_date'] = date_agg['max']\n",
    "        df_clean['num_training_sessions'] = date_agg['nunique']\n",
    "    \n",
    "    # Add fold count for verification\n",
    "    fold_counts = df_complete.groupby('config')['validation_fold'].agg(['count', 'nunique'])\n",
    "    df_clean['fold_count'] = fold_counts['count']\n",
    "    df_clean['fold_unique'] = fold_counts['nunique']\n",
    "    \n",
    "    # Reset index\n",
    "    df_clean = df_clean.reset_index()\n",
    "    \n",
    "    print(f\"Aggregated to {len(df_clean)} unique configs\")\n",
    "    \n",
    "    # Rename columns to add _mean suffix\n",
    "    rename_dict = {}\n",
    "    for col in df_clean.columns:\n",
    "        if col in original_test_metrics + eval_test_metrics + val_metrics:\n",
    "            rename_dict[col] = f\"{col}_mean\"\n",
    "    \n",
    "    df_clean = df_clean.rename(columns=rename_dict)\n",
    "    \n",
    "    # Add config_ column for compatibility\n",
    "    if 'config' in df_clean.columns:\n",
    "        df_clean['config_'] = df_clean['config']\n",
    "    \n",
    "    # Final verification\n",
    "    assert all(df_clean['fold_count'] == 5), \"Some configs don't have exactly 5 folds after aggregation\"\n",
    "    assert all(df_clean['fold_unique'] == 5), \"Some configs don't have 5 unique folds after aggregation\"\n",
    "    \n",
    "    # Clean up verification columns\n",
    "    df_clean = df_clean.drop(['fold_count', 'fold_unique'], axis=1)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total configs in final dataset: {len(df_clean)}\")\n",
    "    \n",
    "    if 'eval_test_iou_mean' in df_clean.columns:\n",
    "        print(f\"\\nPerformance statistics:\")\n",
    "        print(f\"  Min IoU: {df_clean['eval_test_iou_mean'].min():.3f}\")\n",
    "        print(f\"  Max IoU: {df_clean['eval_test_iou_mean'].max():.3f}\")\n",
    "        print(f\"  Mean IoU: {df_clean['eval_test_iou_mean'].mean():.3f}\")\n",
    "        print(f\"  Std IoU: {df_clean['eval_test_iou_mean'].std():.3f}\")\n",
    "        \n",
    "        # Compare to original data\n",
    "        original_configs = df_filtered['config'].unique()\n",
    "        kept_configs = set(df_clean['config'])\n",
    "        dropped_configs = set(original_configs) - kept_configs\n",
    "        \n",
    "        if dropped_configs:\n",
    "            print(f\"\\nDropped {len(dropped_configs)} configs due to incomplete folds\")\n",
    "            print(\"Examples of dropped configs:\")\n",
    "            for i, config in enumerate(sorted(dropped_configs)[:5]):\n",
    "                config_data = df_filtered[df_filtered['config'] == config]\n",
    "                available_folds = sorted(config_data['validation_fold'].unique())\n",
    "                print(f\"  - {config}: has folds {available_folds}\")\n",
    "            if len(dropped_configs) > 5:\n",
    "                print(f\"  ... and {len(dropped_configs) - 5} more\")\n",
    "    \n",
    "    if 'num_training_sessions' in df_clean.columns:\n",
    "        multi_session = df_clean[df_clean['num_training_sessions'] > 1]\n",
    "        if len(multi_session) > 0:\n",
    "            print(f\"\\n{len(multi_session)} configs had multiple training sessions\")\n",
    "            print(\"(We kept the best evaluation per fold across all sessions)\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def compare_original_vs_evaluation_metrics(df_clean):\n",
    "    \"\"\"Compare original JSON test metrics with re-evaluation metrics.\"\"\"\n",
    "    \n",
    "    original_iou = None\n",
    "    eval_iou = None\n",
    "    \n",
    "    for col in df_clean.columns:\n",
    "        if col.startswith('test_') and 'iou' in col and col.endswith('_mean'):\n",
    "            original_iou = col\n",
    "        elif col.startswith('eval_test_') and 'iou' in col and col.endswith('_mean'):\n",
    "            eval_iou = col\n",
    "    \n",
    "    if original_iou and eval_iou:\n",
    "        print(\"COMPARISON: Original vs Re-evaluation Metrics\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        iou_diff = df_clean[eval_iou] - df_clean[original_iou]\n",
    "        \n",
    "        print(f\"Original Test IoU (from JSON): {df_clean[original_iou].mean():.3f} ± {df_clean[original_iou].std():.3f}\")\n",
    "        print(f\"Re-eval Test IoU: {df_clean[eval_iou].mean():.3f} ± {df_clean[eval_iou].std():.3f}\")\n",
    "        print(f\"Average difference (re-eval - original): {iou_diff.mean():.3f} ± {iou_diff.std():.3f}\")\n",
    "        \n",
    "        df_temp = df_clean.copy()\n",
    "        df_temp['iou_difference'] = iou_diff\n",
    "        \n",
    "        # Determine config column\n",
    "        config_col = 'config' if 'config' in df_temp.columns else 'config_' if 'config_' in df_temp.columns else None\n",
    "        \n",
    "        if config_col:\n",
    "            print(\"\\nLargest improvements in re-evaluation:\")\n",
    "            top_improvements = df_temp.nlargest(3, 'iou_difference')[[config_col, original_iou, eval_iou, 'iou_difference']]\n",
    "            print(top_improvements.to_string(index=False))\n",
    "            \n",
    "            print(\"\\nLargest drops in re-evaluation:\")\n",
    "            top_drops = df_temp.nsmallest(3, 'iou_difference')[[config_col, original_iou, eval_iou, 'iou_difference']]\n",
    "            print(top_drops.to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\nWarning: No config column found for detailed comparison\")\n",
    "        \n",
    "        return df_temp\n",
    "    else:\n",
    "        print(\"Cannot compare metrics - missing columns:\")\n",
    "        print(f\"  Original IoU column: {original_iou}\")\n",
    "        print(f\"  Evaluation IoU column: {eval_iou}\")\n",
    "        return df_clean\n",
    "\n",
    "def fix_column_names_for_graphics(df):\n",
    "    \"\"\"Add compatibility columns for graphics functions.\"\"\"\n",
    "    df_fixed = df.copy()\n",
    "    \n",
    "    if 'config' in df_fixed.columns and 'config_' not in df_fixed.columns:\n",
    "        df_fixed['config_'] = df_fixed['config']\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "def run_cv_analysis(df_clean, save_path=None, use_evaluation_metrics=True):\n",
    "    \"\"\"\n",
    "    Run analysis on cleaned cross-validation data.\n",
    "    \n",
    "    Parameters:\n",
    "        df_clean: Cleaned DataFrame with aggregated results\n",
    "        save_path: Path to save analysis results\n",
    "        use_evaluation_metrics: Whether to use re-evaluation metrics\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame ready for analysis\n",
    "    \"\"\"\n",
    "    # Determine performance metric to use\n",
    "    available_eval_test = [col for col in df_clean.columns if col.startswith('eval_test_') and 'iou' in col and col.endswith('_mean')]\n",
    "    available_original_test = [col for col in df_clean.columns if col.startswith('test_') and 'iou' in col and col.endswith('_mean')]\n",
    "    available_val = [col for col in df_clean.columns if ('val' in col.lower() or 'validation' in col.lower()) and 'iou' in col and col.endswith('_mean')]\n",
    "    \n",
    "    if use_evaluation_metrics and available_eval_test:\n",
    "        performance_metric = available_eval_test[0]\n",
    "        print(f\"Using evaluation metric: {performance_metric}\")\n",
    "    elif available_original_test:\n",
    "        performance_metric = available_original_test[0]\n",
    "        print(f\"Using original test metric: {performance_metric}\")\n",
    "    elif available_val:\n",
    "        performance_metric = available_val[0]\n",
    "        print(f\"Using validation metric: {performance_metric}\")\n",
    "    else:\n",
    "        print(\"No IoU metrics found\")\n",
    "        return\n",
    "    \n",
    "    # Create analysis copy with compatibility columns\n",
    "    df_analysis = df_clean.copy()\n",
    "    \n",
    "    column_mapping = {\n",
    "        performance_metric: 'test_iou',\n",
    "        'total_training_time_hour': 'total_training_time_hour',\n",
    "        'num_params_M': 'num_params_M'\n",
    "    }\n",
    "    \n",
    "    for old_col, new_col in column_mapping.items():\n",
    "        if old_col in df_analysis.columns and old_col != new_col:\n",
    "            df_analysis[new_col] = df_analysis[old_col]\n",
    "    \n",
    "    # Analysis summary\n",
    "    print(f\"Running analysis on {len(df_analysis)} complete training runs\")\n",
    "    print(f\"Performance range: {df_analysis['test_iou'].min():.3f} - {df_analysis['test_iou'].max():.3f}\")\n",
    "    print(f\"Mean performance: {df_analysis['test_iou'].mean():.3f}\")\n",
    "    \n",
    "    threshold = 0.5\n",
    "    good_models = df_analysis[df_analysis['test_iou'] > threshold]\n",
    "    print(f\"Models above {threshold} IoU: {len(good_models)}\")\n",
    "    \n",
    "    if len(good_models) > 0:\n",
    "        print(\"\\nDATA ANALYSIS SUMMARY:\")\n",
    "        print(f\"Models above threshold: {len(good_models)}\")\n",
    "        print(f\"Best performing model: {good_models['test_iou'].max():.3f} IoU\")\n",
    "        \n",
    "        top_5 = good_models.nlargest(5, 'test_iou')\n",
    "        print(\"\\nTop 5 Models:\")\n",
    "        for i, (_, row) in enumerate(top_5.iterrows(), 1):\n",
    "            config_name = row.get('config', f'Model {i}')\n",
    "            architecture = row.get('architecture', 'Unknown')\n",
    "            print(f\"  {i}. {config_name} ({architecture}): {row['test_iou']:.3f} IoU\")\n",
    "    else:\n",
    "        print(\"No models above threshold for analysis\")\n",
    "        \n",
    "    return df_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand data structure\n",
    "print(\"DIAGNOSTICS: Understanding the data structure\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check sample config\n",
    "sample_config = 'pan_regnety_080_imagenet'\n",
    "sample_data = df_model_results[df_model_results['config'] == sample_config].sort_values(['model_start_date', 'validation_fold'])\n",
    "\n",
    "print(f\"\\nSample config: {sample_config}\")\n",
    "print(f\"Total rows: {len(sample_data)}\")\n",
    "print(\"\\nData structure:\")\n",
    "print(sample_data[['config', 'validation_fold', 'model_start_date', 'eval_test_iou']].to_string())\n",
    "\n",
    "print(\"\\nUnique model_start_dates:\")\n",
    "print(sample_data['model_start_date'].value_counts())\n",
    "\n",
    "# Clean CV results\n",
    "df_model_results_clean = clean_cv_results(df_model_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANED DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total configs: {len(df_model_results_clean)}\")\n",
    "print(f\"Original had: {df_model_results['config'].nunique()} unique configs\")\n",
    "print(f\"Kept: {len(df_model_results_clean) / df_model_results['config'].nunique() * 100:.1f}%\")\n",
    "\n",
    "# Run analysis\n",
    "df_analysis = run_cv_analysis(df_model_results_clean, save_path=GRAPHICS_PATH)\n",
    "\n",
    "# Show top models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 MODELS (Best Sessions Only)\")\n",
    "print(\"=\"*60)\n",
    "if 'eval_test_iou_mean' in df_model_results_clean.columns:\n",
    "    top_10 = df_model_results_clean.nlargest(10, 'eval_test_iou_mean')[['config', 'architecture', 'backbone', 'eval_test_iou_mean', 'num_params_M']]\n",
    "    print(top_10.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate YOLO Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_to_mask(polygon, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert normalized polygon coordinates to binary mask.\n",
    "    \n",
    "    Parameters:\n",
    "        polygon: List of normalized coordinates\n",
    "        img_width: Image width in pixels\n",
    "        img_height: Image height in pixels\n",
    "        \n",
    "    Returns:\n",
    "        Binary mask array\n",
    "    \"\"\"\n",
    "    # Denormalize coordinates\n",
    "    polygon = np.array(polygon).reshape(-1, 2)\n",
    "    polygon[:, 0] *= img_width\n",
    "    polygon[:, 1] *= img_height\n",
    "    polygon = polygon.astype(np.int32)\n",
    "    \n",
    "    # Create mask\n",
    "    mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [polygon], 1)\n",
    "    return mask\n",
    "\n",
    "def parse_yolo_segmentation(label_path, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Parse YOLO segmentation format label file.\n",
    "    \n",
    "    Parameters:\n",
    "        label_path: Path to label file\n",
    "        img_width: Image width\n",
    "        img_height: Image height\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (masks, classes) lists\n",
    "    \"\"\"\n",
    "    masks = []\n",
    "    classes = []\n",
    "    \n",
    "    if not os.path.exists(label_path):\n",
    "        return masks, classes\n",
    "    \n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) < 7:  # At least class + 3 points (6 coordinates)\n",
    "            continue\n",
    "        \n",
    "        class_id = int(parts[0])\n",
    "        polygon = [float(x) for x in parts[1:]]\n",
    "        \n",
    "        mask = polygon_to_mask(polygon, img_width, img_height)\n",
    "        masks.append(mask)\n",
    "        classes.append(class_id)\n",
    "    \n",
    "    return masks, classes\n",
    "\n",
    "def calculate_iou(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union between two masks.\n",
    "    \n",
    "    Parameters:\n",
    "        mask1: First binary mask\n",
    "        mask2: Second binary mask\n",
    "        \n",
    "    Returns:\n",
    "        IoU value\n",
    "    \"\"\"\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "def match_predictions_to_ground_truth(pred_masks, pred_classes, gt_masks, gt_classes, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Match predictions to ground truth based on IoU.\n",
    "    \n",
    "    Parameters:\n",
    "        pred_masks: List of predicted masks\n",
    "        pred_classes: List of predicted classes\n",
    "        gt_masks: List of ground truth masks\n",
    "        gt_classes: List of ground truth classes\n",
    "        iou_threshold: Minimum IoU for matching\n",
    "        \n",
    "    Returns:\n",
    "        List of (pred_idx, gt_idx, iou) tuples\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    matched_gt = set()\n",
    "    \n",
    "    for i, (pred_mask, pred_class) in enumerate(zip(pred_masks, pred_classes)):\n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        for j, (gt_mask, gt_class) in enumerate(zip(gt_masks, gt_classes)):\n",
    "            if j in matched_gt:\n",
    "                continue\n",
    "            \n",
    "            if pred_class != gt_class:\n",
    "                continue\n",
    "            \n",
    "            iou = calculate_iou(pred_mask, gt_mask)\n",
    "            if iou > best_iou and iou >= iou_threshold:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = j\n",
    "        \n",
    "        if best_gt_idx >= 0:\n",
    "            matches.append((i, best_gt_idx, best_iou))\n",
    "            matched_gt.add(best_gt_idx)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def calculate_metrics_for_image(pred_masks, pred_classes, pred_scores, gt_masks, gt_classes, iou_thresholds):\n",
    "    \"\"\"\n",
    "    Calculate metrics for a single image.\n",
    "    \n",
    "    Parameters:\n",
    "        pred_masks: List of predicted masks\n",
    "        pred_classes: List of predicted classes\n",
    "        pred_scores: List of confidence scores\n",
    "        gt_masks: List of ground truth masks\n",
    "        gt_classes: List of ground truth classes\n",
    "        iou_thresholds: List of IoU thresholds for mAP calculation\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Handle empty predictions or ground truth\n",
    "    if len(pred_masks) == 0 and len(gt_masks) == 0:\n",
    "        # Perfect case - no predictions and no ground truth\n",
    "        for iou_thresh in iou_thresholds:\n",
    "            metrics[f'map_{int(iou_thresh*100)}'] = 1.0\n",
    "        metrics['precision'] = 1.0\n",
    "        metrics['recall'] = 1.0\n",
    "        metrics['f1_score'] = 1.0\n",
    "        metrics['accuracy'] = 1.0\n",
    "        metrics['iou'] = 1.0\n",
    "        metrics['mean_iou'] = 1.0\n",
    "        return metrics\n",
    "    \n",
    "    if len(pred_masks) == 0:\n",
    "        # No predictions but there are ground truth objects\n",
    "        for iou_thresh in iou_thresholds:\n",
    "            metrics[f'map_{int(iou_thresh*100)}'] = 0.0\n",
    "        metrics['precision'] = 0.0\n",
    "        metrics['recall'] = 0.0\n",
    "        metrics['f1_score'] = 0.0\n",
    "        metrics['accuracy'] = 0.0\n",
    "        metrics['iou'] = 0.0\n",
    "        metrics['mean_iou'] = 0.0\n",
    "        return metrics\n",
    "    \n",
    "    if len(gt_masks) == 0:\n",
    "        # Predictions but no ground truth\n",
    "        for iou_thresh in iou_thresholds:\n",
    "            metrics[f'map_{int(iou_thresh*100)}'] = 0.0\n",
    "        metrics['precision'] = 0.0\n",
    "        metrics['recall'] = 1.0\n",
    "        metrics['f1_score'] = 0.0\n",
    "        metrics['accuracy'] = 0.0\n",
    "        metrics['iou'] = 0.0\n",
    "        metrics['mean_iou'] = 0.0\n",
    "        return metrics\n",
    "    \n",
    "    # Calculate metrics for each IoU threshold\n",
    "    ious_for_matches = []\n",
    "    \n",
    "    for iou_thresh in iou_thresholds:\n",
    "        matches = match_predictions_to_ground_truth(pred_masks, pred_classes, gt_masks, gt_classes, iou_thresh)\n",
    "        \n",
    "        true_positives = len(matches)\n",
    "        false_positives = len(pred_masks) - true_positives\n",
    "        false_negatives = len(gt_masks) - true_positives\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        \n",
    "        # Calculate AP for this threshold\n",
    "        if len(pred_scores) > 0:\n",
    "            # Sort by confidence\n",
    "            sorted_indices = np.argsort(pred_scores)[::-1]\n",
    "            sorted_pred_masks = [pred_masks[i] for i in sorted_indices]\n",
    "            sorted_pred_classes = [pred_classes[i] for i in sorted_indices]\n",
    "            \n",
    "            # Recalculate matches with sorted predictions\n",
    "            matches = match_predictions_to_ground_truth(sorted_pred_masks, sorted_pred_classes, gt_masks, gt_classes, iou_thresh)\n",
    "            \n",
    "            # Calculate AP\n",
    "            precisions = []\n",
    "            recalls = []\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            \n",
    "            matched_gt = set()\n",
    "            for i in range(len(sorted_pred_masks)):\n",
    "                matched = False\n",
    "                for match in matches:\n",
    "                    if match[0] == i:\n",
    "                        tp += 1\n",
    "                        matched = True\n",
    "                        ious_for_matches.append(match[2])\n",
    "                        break\n",
    "                \n",
    "                if not matched:\n",
    "                    fp += 1\n",
    "                \n",
    "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                recall = tp / len(gt_masks) if len(gt_masks) > 0 else 0\n",
    "                \n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "            \n",
    "            # Calculate AP using 11-point interpolation\n",
    "            ap = 0\n",
    "            for r in np.linspace(0, 1, 11):\n",
    "                if len(recalls) > 0:\n",
    "                    prec_at_recall = [p for p, rec in zip(precisions, recalls) if rec >= r]\n",
    "                    if prec_at_recall:\n",
    "                        ap += max(prec_at_recall) / 11\n",
    "            \n",
    "            metrics[f'map_{int(iou_thresh*100)}'] = ap\n",
    "        else:\n",
    "            metrics[f'map_{int(iou_thresh*100)}'] = precision\n",
    "    \n",
    "    # Calculate overall metrics at IoU 0.5\n",
    "    matches = match_predictions_to_ground_truth(pred_masks, pred_classes, gt_masks, gt_classes, 0.5)\n",
    "    \n",
    "    true_positives = len(matches)\n",
    "    false_positives = len(pred_masks) - true_positives\n",
    "    false_negatives = len(gt_masks) - true_positives\n",
    "    \n",
    "    metrics['precision'] = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    metrics['recall'] = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    metrics['f1_score'] = 2 * metrics['precision'] * metrics['recall'] / (metrics['precision'] + metrics['recall']) if (metrics['precision'] + metrics['recall']) > 0 else 0\n",
    "    \n",
    "    # Accuracy\n",
    "    total_predictions = true_positives + false_positives + false_negatives\n",
    "    metrics['accuracy'] = true_positives / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    # Mean IoU of matched predictions\n",
    "    metrics['mean_iou'] = np.mean(ious_for_matches) if ious_for_matches else 0.0\n",
    "    metrics['iou'] = metrics['mean_iou']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_model_on_test_set(model_path, test_images_dir, test_labels_dir):\n",
    "    \"\"\"\n",
    "    Evaluate a single YOLO model on test dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        model_path: Path to YOLO model weights\n",
    "        test_images_dir: Directory containing test images\n",
    "        test_labels_dir: Directory containing test labels\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (per_image_results, overall_metrics)\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_path}: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Get all test images\n",
    "    image_files = list(Path(test_images_dir).glob(\"*.jpg\")) + \\\n",
    "                  list(Path(test_images_dir).glob(\"*.png\")) + \\\n",
    "                  list(Path(test_images_dir).glob(\"*.jpeg\"))\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(f\"No images found in {test_images_dir}\")\n",
    "        return None, None\n",
    "    \n",
    "    per_image_results = []\n",
    "    all_metrics = defaultdict(list)\n",
    "    \n",
    "    # IoU thresholds for mAP calculation\n",
    "    iou_thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=\"Evaluating images\"):\n",
    "        # Read image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "        \n",
    "        img_height, img_width = img.shape[:2]\n",
    "        \n",
    "        # Get corresponding label\n",
    "        label_name = img_path.stem + \".txt\"\n",
    "        label_path = os.path.join(test_labels_dir, label_name)\n",
    "        \n",
    "        # Parse ground truth\n",
    "        gt_masks, gt_classes = parse_yolo_segmentation(label_path, img_width, img_height)\n",
    "        \n",
    "        # Run inference\n",
    "        try:\n",
    "            results = model(img, verbose=False)\n",
    "            \n",
    "            pred_masks = []\n",
    "            pred_classes = []\n",
    "            pred_scores = []\n",
    "            \n",
    "            if results[0].masks is not None:\n",
    "                masks_data = results[0].masks.data.cpu().numpy()\n",
    "                boxes = results[0].boxes\n",
    "                \n",
    "                for i in range(len(masks_data)):\n",
    "                    mask = masks_data[i]\n",
    "                    # Resize mask to original image size\n",
    "                    mask = cv2.resize(mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
    "                    mask = (mask > 0.5).astype(np.uint8)\n",
    "                    \n",
    "                    pred_masks.append(mask)\n",
    "                    pred_classes.append(int(boxes.cls[i]))\n",
    "                    pred_scores.append(float(boxes.conf[i]))\n",
    "            \n",
    "            # Calculate metrics for this image\n",
    "            image_metrics = calculate_metrics_for_image(\n",
    "                pred_masks, pred_classes, pred_scores,\n",
    "                gt_masks, gt_classes,\n",
    "                iou_thresholds\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            image_result = {\n",
    "                'image_name': img_path.name,\n",
    "                **image_metrics\n",
    "            }\n",
    "            per_image_results.append(image_result)\n",
    "            \n",
    "            # Accumulate for overall metrics\n",
    "            for key, value in image_metrics.items():\n",
    "                all_metrics[key].append(value)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate overall test set metrics\n",
    "    overall_metrics = {}\n",
    "    for key, values in all_metrics.items():\n",
    "        overall_metrics[key] = np.mean(values)\n",
    "    \n",
    "    return per_image_results, overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if YOLO_EVALUATE:\n",
    "    # Verify paths exist\n",
    "    if not os.path.exists(INPUT_MODELS_BASE_DIR_PATH):\n",
    "        print(f\"ERROR: INPUT_MODELS_BASE_DIR_PATH does not exist: {INPUT_MODELS_BASE_DIR_PATH}\")\n",
    "\n",
    "    if not os.path.exists(TEST_DATASET_PATH):\n",
    "        print(f\"WARNING: TEST_DATASET_PATH does not exist: {TEST_DATASET_PATH}\")\n",
    "        print(\"Trying to find it relative to INPUT_MODELS_BASE_DIR_PATH...\")\n",
    "        alt_test_path = os.path.join(INPUT_MODELS_BASE_DIR_PATH, TEST_DATASET_PATH)\n",
    "        if os.path.exists(alt_test_path):\n",
    "            TEST_DATASET_PATH = alt_test_path\n",
    "            print(f\"Found test dataset at: {TEST_DATASET_PATH}\")\n",
    "        else:\n",
    "            print(\"ERROR: Could not find test dataset\")\n",
    "            print(\"Please update TEST_DATASET_PATH in the script\")\n",
    "\n",
    "    # Find all model directories\n",
    "    print(f\"\\nSearching for model directories in: {INPUT_MODELS_BASE_DIR_PATH}\")\n",
    "    all_dirs = os.listdir(INPUT_MODELS_BASE_DIR_PATH)\n",
    "    model_dirs = [d for d in all_dirs \n",
    "                    if d.startswith(\"01_training_yolo\") and os.path.isdir(os.path.join(INPUT_MODELS_BASE_DIR_PATH, d))]\n",
    "\n",
    "    print(f\"Found {len(model_dirs)} model directories starting with '01_training_yolo':\")\n",
    "    for d in model_dirs:\n",
    "        print(f\"  - {d}\")\n",
    "\n",
    "    if len(model_dirs) == 0:\n",
    "        print(\"\\nNo directories starting with '01_training_yolo' found.\")\n",
    "        print(\"Available directories:\")\n",
    "        for d in all_dirs:\n",
    "            if os.path.isdir(os.path.join(INPUT_MODELS_BASE_DIR_PATH, d)):\n",
    "                print(f\"  - {d}\")\n",
    "\n",
    "    all_results = []\n",
    "    models_found = 0\n",
    "\n",
    "    for model_dir in model_dirs:\n",
    "        model_dir_path = os.path.join(INPUT_MODELS_BASE_DIR_PATH, model_dir)\n",
    "        print(f\"\\nSearching in: {model_dir_path}\")\n",
    "        \n",
    "        # Look for training_metrics.json files\n",
    "        for root, dirs, files in os.walk(model_dir_path):\n",
    "            if \"training_metrics.json\" in files:\n",
    "                metrics_path = os.path.join(root, \"training_metrics.json\")\n",
    "                print(f\"Found training_metrics.json at: {metrics_path}\")\n",
    "                \n",
    "                try:\n",
    "                    # Load existing metrics\n",
    "                    with open(metrics_path, 'r') as f:\n",
    "                        existing_metrics = json.load(f)\n",
    "                    \n",
    "                    model_path = existing_metrics.get(\"model_path\")\n",
    "                    if not model_path:\n",
    "                        print(f\"  No model path found in {metrics_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Construct full model path if relative\n",
    "                    if not os.path.isabs(model_path):\n",
    "                        # Try multiple possible base paths\n",
    "                        possible_paths = [\n",
    "                            os.path.join(INPUT_MODELS_BASE_DIR_PATH, model_path),\n",
    "                            os.path.join(root, model_path),\n",
    "                            os.path.join(root, \"weights\", \"best.pt\"),\n",
    "                            os.path.join(root, \"..\", model_path),\n",
    "                            os.path.join(root, \"..\", \"..\", model_path)\n",
    "                        ]\n",
    "                        \n",
    "                        model_found = False\n",
    "                        for p in possible_paths:\n",
    "                            p = os.path.normpath(p)\n",
    "                            if os.path.exists(p):\n",
    "                                model_path = p\n",
    "                                model_found = True\n",
    "                                break\n",
    "                        \n",
    "                        if not model_found:\n",
    "                            print(\"  Model not found at any of these locations:\")\n",
    "                            for p in possible_paths:\n",
    "                                print(f\"    - {os.path.normpath(p)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    if not os.path.exists(model_path):\n",
    "                        print(f\"  Model not found: {model_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    models_found += 1\n",
    "                    print(f\"  Found model: {model_path}\")\n",
    "                    print(\"  Evaluating model...\")\n",
    "                    \n",
    "                    # Evaluate model\n",
    "                    test_images_dir = os.path.join(TEST_DATASET_PATH, \"images\")\n",
    "                    test_labels_dir = os.path.join(TEST_DATASET_PATH, \"labels\")\n",
    "                    \n",
    "                    if not os.path.exists(test_images_dir):\n",
    "                        print(f\"  ERROR: Test images directory not found: {test_images_dir}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if not os.path.exists(test_labels_dir):\n",
    "                        print(f\"  ERROR: Test labels directory not found: {test_labels_dir}\")\n",
    "                        continue\n",
    "                    \n",
    "                    per_image_results, overall_metrics = evaluate_model_on_test_set(\n",
    "                        model_path, test_images_dir, test_labels_dir\n",
    "                    )\n",
    "                    \n",
    "                    if overall_metrics:\n",
    "                        # Create result entry\n",
    "                        result = {\n",
    "                            'model_name': os.path.basename(model_dir),\n",
    "                            'model_path': model_path,\n",
    "                            'fold': existing_metrics.get(\"fold\", 0),\n",
    "                            'training_time_minutes': existing_metrics.get(\"training_time_minutes\", 0),\n",
    "                            **overall_metrics\n",
    "                        }\n",
    "                        all_results.append(result)\n",
    "                        \n",
    "                        # Save per-image results\n",
    "                        if per_image_results:\n",
    "                            per_image_df = pd.DataFrame(per_image_results)\n",
    "                            per_image_output_path = os.path.join(\n",
    "                                root, \n",
    "                                \"test_evaluation_per_image.csv\"\n",
    "                            )\n",
    "                            per_image_df.to_csv(per_image_output_path, index=False)\n",
    "                            print(f\"  Saved per-image results to: {per_image_output_path}\")\n",
    "                    else:\n",
    "                        print(\"  Failed to evaluate model\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing {metrics_path}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "\n",
    "    print(f\"\\nTotal models found: {models_found}\")\n",
    "\n",
    "    # Create overall results dataframe\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Reorder columns\n",
    "        column_order = [\n",
    "            'model_name', 'model_path', 'fold', 'training_time_minutes',\n",
    "            'iou', 'f1_score', 'accuracy', 'recall', 'precision',\n",
    "            'map_50', 'map_55', 'map_60', 'map_65', 'map_70',\n",
    "            'map_75', 'map_80', 'map_85', 'map_90', 'map_95',\n",
    "            'mean_iou'\n",
    "        ]\n",
    "        \n",
    "        # Ensure all columns exist\n",
    "        for col in column_order:\n",
    "            if col not in results_df.columns:\n",
    "                results_df[col] = np.nan\n",
    "        \n",
    "        results_df = results_df[column_order]\n",
    "        \n",
    "        # Save overall results\n",
    "        output_path = \"yolo_models_test_evaluation_results.csv\"\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nSaved overall results to: {output_path}\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(\"\\nEvaluation Summary:\")\n",
    "        print(results_df.to_string())\n",
    "    else:\n",
    "        print(\"\\nNo models were successfully evaluated.\")\n",
    "        if models_found > 0:\n",
    "            print(\"Models were found but evaluation failed. Check error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the results\n",
    "yolo_results = pd.read_csv(\"yolo_models_test_evaluation_results.csv\")\n",
    "\n",
    "# Define aggregation: sum training time, mean for metrics\n",
    "custom_agg = {\n",
    "    'fold': 'count',\n",
    "    'training_time_minutes': 'sum',\n",
    "    'iou': 'mean',\n",
    "    'f1_score': 'mean',\n",
    "    'accuracy': 'mean',\n",
    "    'recall': 'mean',\n",
    "    'precision': 'mean',\n",
    "    'map_50': 'mean',\n",
    "    'map_55': 'mean',\n",
    "    'map_60': 'mean',\n",
    "    'map_65': 'mean',\n",
    "    'map_70': 'mean',\n",
    "    'map_75': 'mean',\n",
    "    'map_80': 'mean',\n",
    "    'map_85': 'mean',\n",
    "    'map_90': 'mean',\n",
    "    'map_95': 'mean',\n",
    "    'mean_iou': 'mean'\n",
    "}\n",
    "yolo_grouped_custom = yolo_results.groupby('model_name').agg(custom_agg).reset_index()\n",
    "\n",
    "# Drop mean_iou if exists\n",
    "yolo_grouped_custom.drop(columns=['mean_iou'], inplace=True, errors='ignore')\n",
    "\n",
    "# Convert training time from minutes to hours\n",
    "yolo_grouped_custom['total_training_time_hour'] = yolo_grouped_custom['training_time_minutes'] / 60\n",
    "\n",
    "# Drop the original training_time_minutes column\n",
    "yolo_grouped_custom.drop(columns=['training_time_minutes'], inplace=True)\n",
    "\n",
    "# Add parameter count for YOLO models\n",
    "yolo_params_m = {\n",
    "    \"01_training_yolo12n_20250623\": 2.8,\n",
    "    \"01_training_yolo12s_20250623\": 9.8,\n",
    "    \"01_training_yolo12m_20250623\": 21.9,\n",
    "    \"01_training_yolo12l_20250623\": 28.8,\n",
    "    \"01_training_yolo12x_20250623\": 64.5}\n",
    "yolo_grouped_custom['num_params_M'] = yolo_grouped_custom['model_name'].map(yolo_params_m)\n",
    "\n",
    "# Create architecture/backbone columns\n",
    "yolo_grouped_custom['architecture'] = 'YOLO'\n",
    "yolo_grouped_custom['backbone'] = yolo_grouped_custom['model_name'].apply(lambda x: '_'.join(x.split('_')[2:3]))\n",
    "\n",
    "# Rename columns for consistency\n",
    "yolo_grouped_custom.rename(columns={\n",
    "    'fold': 'num_training_sessions',\n",
    "    'model_name': 'config',\n",
    "    'iou': 'eval_test_iou_mean',\n",
    "    'f1_score': 'eval_test_f1_score_mean',\n",
    "    'accuracy': 'eval_test_accuracy_mean',\n",
    "    'recall': 'eval_test_recall_mean',\n",
    "    'precision': 'eval_test_precision_mean',\n",
    "    'map_50': 'eval_test_map_50_mean',\n",
    "    'map_55': 'eval_test_map_55_mean',\n",
    "    'map_60': 'eval_test_map_60_mean',\n",
    "    'map_65': 'eval_test_map_65_mean',\n",
    "    'map_70': 'eval_test_map_70_mean',\n",
    "    'map_75': 'eval_test_map_75_mean',\n",
    "    'map_80': 'eval_test_map_80_mean',\n",
    "    'map_85': 'eval_test_map_85_mean',\n",
    "    'map_90': 'eval_test_map_90_mean',\n",
    "    'map_95': 'eval_test_map_95_mean',\n",
    "}, inplace=True)\n",
    "\n",
    "# Filter models with IoU > 0.2\n",
    "yolo_grouped_custom = yolo_grouped_custom[yolo_grouped_custom['eval_test_iou_mean'] > 0.2]\n",
    "\n",
    "display(yolo_grouped_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine SMP and YOLO Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate SMP and YOLO results\n",
    "df_model_results_clean_concat_yolo_smp = pd.concat([df_model_results_clean, yolo_grouped_custom], ignore_index=True)\n",
    "\n",
    "display(df_model_results_clean_concat_yolo_smp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global style parameters\n",
    "def set_plot_style():\n",
    "    \"\"\"Set consistent style for all plots.\"\"\"\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"pastel\")\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 10,\n",
    "        'axes.titlesize': 11,\n",
    "        'axes.titleweight': 'bold',\n",
    "        'axes.labelsize': 10,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10,\n",
    "        'legend.fontsize': 10,\n",
    "        'figure.dpi': 150,\n",
    "        'savefig.dpi': 300\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture-Backbone Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_architecture_backbone_heatmap(df_models, performance_metric='eval_test_iou_mean', save_path=None):\n",
    "    \"\"\"\n",
    "    Create heatmap showing performance for architecture-backbone combinations.\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model results\n",
    "        performance_metric: Metric to visualize\n",
    "        save_path: Path to save figure\n",
    "    \"\"\"\n",
    "    sns.set_style(\"white\")\n",
    "    \n",
    "    if 'architecture' not in df_models.columns or 'backbone' not in df_models.columns:\n",
    "        print(\"Architecture or backbone columns not found\")\n",
    "        return\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 7.5))\n",
    "    \n",
    "    # Create pivot table with mean performance\n",
    "    heatmap_data = df_models.pivot_table(\n",
    "        values=performance_metric,\n",
    "        index='architecture',\n",
    "        columns='backbone',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Sort architectures and backbones\n",
    "    heatmap_data = heatmap_data.sort_index()\n",
    "    \n",
    "    # Clean up backbone names\n",
    "    clean_columns = []\n",
    "    for col in heatmap_data.columns:\n",
    "        clean_name = col\n",
    "        for prefix in ['timm-', 'tu-', 'tv-']:\n",
    "            if clean_name.startswith(prefix):\n",
    "                clean_name = clean_name[len(prefix):]\n",
    "        clean_columns.append(clean_name)\n",
    "    \n",
    "    heatmap_data.columns = clean_columns\n",
    "    heatmap_data = heatmap_data[sorted(heatmap_data.columns)]\n",
    "    \n",
    "    # Create mask for NaN values\n",
    "    mask = heatmap_data.isna()\n",
    "    \n",
    "    # Determine metric display name and graphic number\n",
    "    metric_names = {\n",
    "        'eval_test_iou_mean': (\"IoU moyen 5 folds\", \"01\"),\n",
    "        'eval_test_map_50_mean': (\"mAP@50 moyen 5 folds\", \"02\"),\n",
    "        'eval_test_map_75_mean': (\"mAP@75 moyen 5 folds\", \"03\"),\n",
    "        'eval_test_map_95_mean': (\"mAP@95 moyen 5 folds\", \"04\"),\n",
    "        'eval_test_accuracy_mean': (\"Accuracy moyenne 5 folds\", \"05\"),\n",
    "        'eval_test_recall_mean': (\"Recall moyen 5 folds\", \"06\"),\n",
    "        'eval_test_precision_mean': (\"Precision moyenne 5 folds\", \"07\"),\n",
    "        'eval_test_f1_score_mean': (\"F1-score moyen 5 folds\", \"08\")\n",
    "    }\n",
    "    \n",
    "    texte_performance, graphic_number = metric_names.get(performance_metric, (performance_metric, \"99\"))\n",
    "\n",
    "    # Create heatmap\n",
    "    sns.heatmap(heatmap_data,\n",
    "               annot=True,\n",
    "               fmt=\".2f\",\n",
    "               cmap='coolwarm',\n",
    "               linewidth=.5,\n",
    "               annot_kws={\"fontsize\":9},\n",
    "               mask=mask)\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_title('Heatmap Encodeur vs Décodeur - ' + texte_performance, \n",
    "                fontsize=11, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Rotate labels\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right', fontsize=10)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=10)\n",
    "    \n",
    "    ax.grid(False)\n",
    "   \n",
    "    # Remove spines\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/ch4_01_architecture_backbone_heatmap_{graphic_number}_{performance_metric}.png\", \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Plot heatmaps for different metrics\n",
    "plot_architecture_backbone_heatmap(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_iou_mean', save_path=GRAPHICS_PATH)\n",
    "plot_architecture_backbone_heatmap(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_50_mean', save_path=GRAPHICS_PATH)\n",
    "plot_architecture_backbone_heatmap(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_75_mean', save_path=GRAPHICS_PATH)\n",
    "plot_architecture_backbone_heatmap(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_95_mean', save_path=GRAPHICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Models by Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_models_performance(df_models, performance_metric='eval_test_iou_mean', save_path=None):\n",
    "    \"\"\"\n",
    "    Plot top 10 models by specified performance metric.\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model results\n",
    "        performance_metric: Metric to rank models by\n",
    "        save_path: Path to save figure\n",
    "    \"\"\"\n",
    "    set_plot_style()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 4.5))\n",
    "    \n",
    "    if performance_metric in df_models.columns:\n",
    "        # Get top 10 models\n",
    "        top_models = df_models.nlargest(10, performance_metric).copy()\n",
    "        \n",
    "        if not top_models.empty:\n",
    "            y_pos = range(len(top_models) - 1, -1, -1)\n",
    "            bars = ax.barh(y_pos, top_models[performance_metric], alpha=0.7)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, (bar, perf_val) in enumerate(zip(bars, top_models[performance_metric])):\n",
    "                ax.text(bar.get_width()*1.005, bar.get_y() + bar.get_height()/2, \n",
    "                       f'{perf_val:.3f}', ha='left', va='center', fontsize=9)\n",
    "            \n",
    "            # Add more space for labels\n",
    "            ax.set_xlim(left=0, right=ax.get_xlim()[1]*1.05)\n",
    "            \n",
    "            # Create model labels\n",
    "            model_labels = []\n",
    "            for _, row in top_models.iterrows():\n",
    "                label_parts = []\n",
    "                \n",
    "                # Add config name\n",
    "                if 'config_' in row and pd.notna(row['config_']):\n",
    "                    config_name = str(row['config_'])\n",
    "                    # Clean up config name\n",
    "                    if '_' in config_name:\n",
    "                        config_name = '_'.join(config_name.split('_')[1:])\n",
    "                    config_name = config_name.replace('_imagenet', '')\n",
    "                    label_parts.append(config_name)\n",
    "                \n",
    "                # Add architecture\n",
    "                if 'architecture' in row and pd.notna(row['architecture']):\n",
    "                    label_parts.append(f\"({row['architecture']})\")\n",
    "                \n",
    "                # Add parameter count\n",
    "                if 'num_params_M' in row and pd.notna(row['num_params_M']):\n",
    "                    label_parts.append(f\"[{row['num_params_M']:.0f}M]\")\n",
    "                \n",
    "                # If no config name found, use index\n",
    "                if not label_parts:\n",
    "                    label_parts.append(f\"Model {row.name}\")\n",
    "                \n",
    "                model_labels.append(\" \".join(label_parts))\n",
    "            \n",
    "            # Place labels inside bars\n",
    "            ax.set_yticks(y_pos)\n",
    "            for i, (bar, label) in enumerate(zip(bars, model_labels)):\n",
    "                ax.text(ax.get_xlim()[1]*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                       label, ha='left', va='center', fontsize=9, color='black', zorder=10)\n",
    "            \n",
    "            ax.set_yticklabels([])\n",
    "            ax.tick_params(axis='y', pad=-5)\n",
    "            \n",
    "            # Set labels and title based on metric\n",
    "            metric_info = {\n",
    "                'eval_test_iou_mean': ('Test IoU (moyenne des 5 folds)', 'Top 10 modèles par IoU', \"01\"),\n",
    "                'eval_test_map_50_mean': ('Test mAP@0.5 (moyenne des 5 folds)', 'Top 10 modèles par mAP@0.5', \"02\"),\n",
    "                'eval_test_map_75_mean': ('Test mAP@0.75 (moyenne des 5 folds)', 'Top 10 modèles par mAP@0.75', \"03\"),\n",
    "                'eval_test_map_95_mean': ('Test mAP@0.95 (moyenne des 5 folds)', 'Top 10 modèles par mAP@0.95', \"04\"),\n",
    "                'eval_test_accuracy_mean': ('Test Accuracy (moyenne des 5 folds)', 'Top 10 modèles par Accuracy', \"05\"),\n",
    "                'eval_test_recall_mean': ('Test Recall (moyenne des 5 folds)', 'Top 10 modèles par Recall', \"06\"),\n",
    "                'eval_test_precision_mean': ('Test Precision (moyenne des 5 folds)', 'Top 10 modèles par Precision', \"07\"),\n",
    "                'eval_test_f1_score_mean': ('Test F1 Score (moyenne des 5 folds)', 'Top 10 modèles par F1 Score', \"08\")\n",
    "            }\n",
    "            \n",
    "            x_label, title, graphic_number = metric_info.get(performance_metric, (performance_metric, f'Top 10 modèles', \"99\"))\n",
    "            ax.set_xlabel(x_label, fontsize=10)\n",
    "            ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/ch4_02_top_models_performance_{graphic_number}_{performance_metric}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for different metrics\n",
    "plot_top_models_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_iou_mean', save_path=GRAPHICS_PATH)\n",
    "plot_top_models_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_50_mean', save_path=GRAPHICS_PATH)\n",
    "plot_top_models_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_75_mean', save_path=GRAPHICS_PATH)\n",
    "plot_top_models_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_95_mean', save_path=GRAPHICS_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_architecture_performance(df_models, performance_metric='eval_test_iou_mean', save_path=None):\n",
    "    \"\"\"\n",
    "    Plot performance by decoder architecture (top 10).\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model results\n",
    "        performance_metric: Metric to visualize\n",
    "        save_path: Path to save figure\n",
    "    \"\"\"\n",
    "    set_plot_style()\n",
    "    \n",
    "    if 'architecture' not in df_models.columns:\n",
    "        print(\"Architecture column not found\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 4.5))\n",
    "    \n",
    "    # Calculate performance by architecture\n",
    "    architecture_performance = df_models.groupby('architecture')[performance_metric].agg(['mean', 'count']).reset_index()\n",
    "    architecture_performance = architecture_performance[architecture_performance['count'] >= 1]\n",
    "    architecture_performance = architecture_performance.sort_values('mean', ascending=False).head(10)\n",
    "    \n",
    "    if not architecture_performance.empty:\n",
    "        y_pos = range(len(architecture_performance) - 1, -1, -1)\n",
    "        bars = ax.barh(y_pos, architecture_performance['mean'], alpha=0.7)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, mean_val) in enumerate(zip(bars, architecture_performance['mean'])):\n",
    "            ax.text(bar.get_width()*1.005, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{mean_val:.3f}', ha='left', va='center', fontsize=9)\n",
    "        \n",
    "        ax.set_xlim(left=0, right=ax.get_xlim()[1]*1.05)\n",
    "        ax.set_yticks(y_pos)\n",
    "        \n",
    "        # Place architecture names inside bars\n",
    "        architecture_names = list(architecture_performance['architecture'])\n",
    "        for i, bar in enumerate(bars):\n",
    "            ax.text(ax.get_xlim()[1]*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                   architecture_names[i], ha='left', va='center', fontsize=9, color='black', zorder=10)\n",
    "        \n",
    "        ax.set_yticklabels([])\n",
    "        ax.tick_params(axis='y', pad=-5)\n",
    "        \n",
    "        # Set labels and title based on metric\n",
    "        metric_info = {\n",
    "            'eval_test_iou_mean': ('Test IoU (moyenne des 5 folds de tous les encodeurs)', 'Top 10 décodeurs par IoU', \"01\"),\n",
    "            'eval_test_map_50_mean': ('Test mAP@0.5 (moyenne des 5 folds de tous les encodeurs)', 'Top 10 décodeurs par mAP@0.5', \"02\"),\n",
    "            'eval_test_map_75_mean': ('Test mAP@0.75 (moyenne des 5 folds de tous les encodeurs)', 'Top 10 décodeurs par mAP@0.75', \"03\"),\n",
    "            'eval_test_map_95_mean': ('Test mAP@0.95 (moyenne des 5 folds de tous les encodeurs)', 'Top 10 décodeurs par mAP@0.95', \"04\"),\n",
    "            'eval_test_accuracy_mean': ('Test Accuracy (moyenne des 5 folds de tous les encodeurs)', 'Top 10 décodeurs par Accuracy', \"05\"),\n",
    "            'eval_test_recall_mean': ('Test Recall (moyenne des 5 folds de tous les encodeurs)', 'Top 10 décodeurs par Recall', \"06\"),\n",
    "            'eval_test_precision_mean': ('Test Precision (moyenne des 5 folds de tous les encodeurs)', 'Top 10 décodeurs par Precision', \"07\"),\n",
    "            'eval_test_f1_score_mean': ('Test F1 Score (moyenne des 5 folds de tous les encodeurs)', 'Top 10 décodeurs par F1 Score', \"08\")\n",
    "        }\n",
    "        \n",
    "        x_label, title, graphic_number = metric_info.get(performance_metric, (performance_metric, f'Top 10 décodeurs', \"99\"))\n",
    "        ax.set_xlabel(x_label, fontsize=10)\n",
    "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/ch4_03_plot_architecture_performance_{graphic_number}_{performance_metric}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for different metrics\n",
    "plot_architecture_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_iou_mean', save_path=GRAPHICS_PATH)\n",
    "plot_architecture_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_50_mean', save_path=GRAPHICS_PATH)\n",
    "plot_architecture_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_75_mean', save_path=GRAPHICS_PATH)\n",
    "plot_architecture_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_95_mean', save_path=GRAPHICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_backbone_performance(df_models, performance_metric='eval_test_iou_mean', save_path=None):\n",
    "    \"\"\"\n",
    "    Plot performance by encoder backbone (top 10).\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model results\n",
    "        performance_metric: Metric to visualize\n",
    "        save_path: Path to save figure\n",
    "    \"\"\"\n",
    "    set_plot_style()\n",
    "    \n",
    "    if 'backbone' not in df_models.columns:\n",
    "        print(\"Backbone column not found\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 4.5))\n",
    "    \n",
    "    # Calculate performance by backbone\n",
    "    backbone_performance = df_models.groupby('backbone')[performance_metric].agg(['mean', 'count']).reset_index()\n",
    "    backbone_performance = backbone_performance[backbone_performance['count'] >= 1]\n",
    "    backbone_performance = backbone_performance.sort_values('mean', ascending=False).head(10)\n",
    "    \n",
    "    if not backbone_performance.empty:\n",
    "        y_pos = range(len(backbone_performance) - 1, -1, -1)\n",
    "        bars = ax.barh(y_pos, backbone_performance['mean'], alpha=0.7)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, mean_val) in enumerate(zip(bars, backbone_performance['mean'])):\n",
    "            ax.text(bar.get_width()*1.005, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{mean_val:.3f}', ha='left', va='center', fontsize=9)\n",
    "        \n",
    "        ax.set_xlim(left=0, right=ax.get_xlim()[1]*1.05)\n",
    "        ax.set_yticks(y_pos)\n",
    "        \n",
    "        # Place backbone names inside bars\n",
    "        backbone_names = list(backbone_performance['backbone'])\n",
    "        for i, bar in enumerate(bars):\n",
    "            ax.text(ax.get_xlim()[1]*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                   backbone_names[i], ha='left', va='center', fontsize=9, color='black', zorder=10)\n",
    "        \n",
    "        ax.set_yticklabels([])\n",
    "        ax.tick_params(axis='y', pad=-5)\n",
    "        \n",
    "        # Set labels and title based on metric\n",
    "        metric_info = {\n",
    "            'eval_test_iou_mean': ('Test IoU (moyenne des 5 folds de tous les décodeurs)', 'Top 10 encodeurs par IoU', \"01\"),\n",
    "            'eval_test_map_50_mean': ('Test mAP@0.5 (moyenne des 5 folds de tous les décodeurs)', 'Top 10 encodeurs par mAP@0.5', \"02\"),\n",
    "            'eval_test_map_75_mean': ('Test mAP@0.75 (moyenne des 5 folds de tous les décodeurs)', 'Top 10 encodeurs par mAP@0.75', \"03\"),\n",
    "            'eval_test_map_95_mean': ('Test mAP@0.95 (moyenne des 5 folds de tous les décodeurs)', 'Top 10 encodeurs par mAP@0.95', \"04\"),\n",
    "            'eval_test_accuracy_mean': ('Test Accuracy (moyenne des 5 folds de tous les décodeurs)', 'Top 10 encodeurs par Accuracy', \"05\"),\n",
    "            'eval_test_recall_mean': ('Test Recall (moyenne des 5 folds de tous les décodeurs)', 'Top 10 encodeurs par Recall', \"06\"),\n",
    "            'eval_test_precision_mean': ('Test Precision (moyenne des 5 folds de tous les décodeurs)', 'Top 10 encodeurs par Precision', \"07\"),\n",
    "            'eval_test_f1_score_mean': ('Test F1 Score (moyenne des 5 folds de tous les décodeurs)', 'Top 10 encodeurs par F1 Score', \"08\")\n",
    "        }\n",
    "        \n",
    "        x_label, title, graphic_number = metric_info.get(performance_metric, (performance_metric, f'Top 10 encodeurs', \"99\"))\n",
    "        ax.set_xlabel(x_label, fontsize=10)\n",
    "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/ch4_04_backbone_performance_{graphic_number}_{performance_metric}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for different metrics\n",
    "plot_backbone_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_iou_mean', save_path=GRAPHICS_PATH)\n",
    "plot_backbone_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_50_mean', save_path=GRAPHICS_PATH)\n",
    "plot_backbone_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_75_mean', save_path=GRAPHICS_PATH)\n",
    "plot_backbone_performance(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_95_mean', save_path=GRAPHICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 Models by Size Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_models_by_size_category(df_models, performance_metric='eval_test_iou_mean', save_path=None):\n",
    "    \"\"\"\n",
    "    Plot top 3 models in each parameter count size category.\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model results\n",
    "        performance_metric: Metric to visualize\n",
    "        save_path: Path to save figure\n",
    "    \"\"\"\n",
    "    set_plot_style()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 4.5))\n",
    "    \n",
    "    if 'num_params_M' in df_models.columns and performance_metric in df_models.columns:\n",
    "        # Define size bins\n",
    "        size_bins = [(0, 10), (10, 25), (25, 50), (50, 75), (75, 100), (100, float('inf'))]\n",
    "        bin_labels = ['0-10M', '10-25M', '25-50M', '50-75M', '75-100M', '100M+']\n",
    "        \n",
    "        # Pastel colors for each size category\n",
    "        pastel_colors = ['#FFB3BA', '#FFDFBA', '#FFFFBA', '#BAFFC9', '#BAE1FF', '#DCC9FF']\n",
    "        \n",
    "        # Filter out missing values\n",
    "        size_data = df_models[['num_params_M', performance_metric, 'config_', 'architecture']].dropna()\n",
    "        \n",
    "        if not size_data.empty:\n",
    "            # Fixed positioning parameters\n",
    "            models_per_bin = 3\n",
    "            bin_spacing = 0.5\n",
    "            total_bins = len(bin_labels)\n",
    "            \n",
    "            # Calculate fixed positions for each bin\n",
    "            bin_positions = {}\n",
    "            current_y = 0\n",
    "            for i, label in enumerate(bin_labels):\n",
    "                bin_positions[label] = {\n",
    "                    'start': current_y,\n",
    "                    'middle': current_y + (models_per_bin - 1) / 2,\n",
    "                    'end': current_y + models_per_bin - 1\n",
    "                }\n",
    "                current_y += models_per_bin + bin_spacing\n",
    "            \n",
    "            all_models = []\n",
    "            \n",
    "            for i, ((min_size, max_size), label) in enumerate(zip(size_bins, bin_labels)):\n",
    "                # Filter models in this size range\n",
    "                if max_size == float('inf'):\n",
    "                    bin_models = size_data[size_data['num_params_M'] >= min_size]\n",
    "                else:\n",
    "                    bin_models = size_data[(size_data['num_params_M'] >= min_size) & \n",
    "                                         (size_data['num_params_M'] < max_size)]\n",
    "                \n",
    "                # Get top 3 models in this bin\n",
    "                top3 = bin_models.nlargest(3, performance_metric)\n",
    "                \n",
    "                if not top3.empty:\n",
    "                    top3_reversed = top3.iloc[::-1]\n",
    "                    bin_start_y = bin_positions[label]['start']\n",
    "                    \n",
    "                    for rank, (_, model) in enumerate(top3_reversed.iterrows()):\n",
    "                        # Clean config name\n",
    "                        config_name = str(model['config_'])\n",
    "                        if '_' in config_name:\n",
    "                            config_name = '_'.join(config_name.split('_')[1:])\n",
    "                        config_name = config_name.replace('_imagenet', '')\n",
    "                        \n",
    "                        all_models.append({\n",
    "                            'performance': model[performance_metric],\n",
    "                            'label': f\"{config_name} ({model['architecture']}) [{model['num_params_M']:.0f}M]\",\n",
    "                            'color': pastel_colors[i],\n",
    "                            'bin_label': label,\n",
    "                            'y_pos': bin_start_y + rank\n",
    "                        })\n",
    "            \n",
    "            if all_models:\n",
    "                # Create the plot\n",
    "                for model in all_models:\n",
    "                    bar = ax.barh(model['y_pos'], model['performance'], \n",
    "                                 color=model['color'], alpha=0.8, height=0.8)\n",
    "                    \n",
    "                    # Add performance value label\n",
    "                    ax.text(model['performance']*1.005, model['y_pos'], \n",
    "                           f\"{model['performance']:.3f}\", \n",
    "                           ha='left', va='center', fontsize=9, color='black')\n",
    "                    \n",
    "                    # Add model label inside bar\n",
    "                    ax.text(ax.get_xlim()[1]*0.01, model['y_pos'], model['label'], \n",
    "                           ha='left', va='center', fontsize=9, \n",
    "                           color='black', zorder=10)\n",
    "                \n",
    "                # Set fixed y-axis labels and ticks\n",
    "                y_ticks = [bin_positions[label]['middle'] for label in bin_labels]\n",
    "                y_labels = bin_labels\n",
    "                \n",
    "                ax.set_yticks(y_ticks)\n",
    "                ax.set_yticklabels(y_labels, fontsize=10)\n",
    "                \n",
    "                # Set labels and title based on metric\n",
    "                metric_info = {\n",
    "                    'eval_test_iou_mean': ('Test IoU (moyenne des 5 folds)', 'Top 3 modèles par IoU par taille', \"01\"),\n",
    "                    'eval_test_map_50_mean': ('Test mAP@0.5 (moyenne des 5 folds)', 'Top 3 modèles par mAP@0.5 par taille', \"02\"),\n",
    "                    'eval_test_map_75_mean': ('Test mAP@0.75 (moyenne des 5 folds)', 'Top 3 modèles par mAP@0.75 par taille', \"03\"),\n",
    "                    'eval_test_map_95_mean': ('Test mAP@0.95 (moyenne des 5 folds)', 'Top 3 modèles par mAP@0.95 par taille', \"04\"),\n",
    "                    'eval_test_accuracy_mean': ('Test Accuracy (moyenne des 5 folds)', 'Top 3 modèles par Accuracy par taille', \"05\"),\n",
    "                    'eval_test_recall_mean': ('Test Recall (moyenne des 5 folds)', 'Top 3 modèles par Recall par taille', \"06\"),\n",
    "                    'eval_test_precision_mean': ('Test Precision (moyenne des 5 folds)', 'Top 3 modèles par Precision par taille', \"07\"),\n",
    "                    'eval_test_f1_score_mean': ('Test F1 Score (moyenne des 5 folds)', 'Top 3 modèles par F1 Score par taille', \"08\")\n",
    "                }\n",
    "                \n",
    "                x_label, title, graphic_number = metric_info.get(performance_metric, (performance_metric, f'Top 3 modèles par taille', \"99\"))\n",
    "                ax.set_xlabel(x_label, fontsize=10)\n",
    "                ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "                ax.grid(True, alpha=0.3, axis='x')\n",
    "                \n",
    "                # Add more space for labels\n",
    "                ax.set_xlim(left=0, right=ax.get_xlim()[1]*1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/ch4_05_models_by_size_category_{graphic_number}_{performance_metric}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for different metrics\n",
    "plot_models_by_size_category(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_iou_mean', save_path=GRAPHICS_PATH)\n",
    "plot_models_by_size_category(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_50_mean', save_path=GRAPHICS_PATH)\n",
    "plot_models_by_size_category(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_75_mean', save_path=GRAPHICS_PATH)\n",
    "plot_models_by_size_category(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_95_mean', save_path=GRAPHICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plots by Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_architecture_boxplot(df_models, performance_metric='eval_test_iou_mean', save_path=None):\n",
    "    \"\"\"\n",
    "    Create box plot showing performance distribution by architecture.\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model results\n",
    "        performance_metric: Metric to visualize\n",
    "        save_path: Path to save figure\n",
    "    \"\"\"\n",
    "    set_plot_style()\n",
    "    \n",
    "    if 'architecture' not in df_models.columns:\n",
    "        print(\"Architecture column not found\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 4.5))\n",
    "    \n",
    "    plot_data = df_models[[performance_metric, 'architecture']].dropna()\n",
    "    if len(plot_data) > 0:\n",
    "        sns.boxplot(data=plot_data, y='architecture', x=performance_metric, ax=ax)\n",
    "        \n",
    "        # Set labels and title based on metric\n",
    "        metric_info = {\n",
    "            'eval_test_iou_mean': ('Test IoU (moyenne des 5 folds de tous les encodeurs)', 'Test IoU par décodeur', \"01\"),\n",
    "            'eval_test_map_50_mean': ('Test mAP@0.5 (moyenne des 5 folds de tous les encodeurs)', 'Test mAP@0.5 par décodeur', \"02\"),\n",
    "            'eval_test_map_75_mean': ('Test mAP@0.75 (moyenne des 5 folds de tous les encodeurs)', 'Test mAP@0.75 par décodeur', \"03\"),\n",
    "            'eval_test_map_95_mean': ('Test mAP@0.95 (moyenne des 5 folds de tous les encodeurs)', 'Test mAP@0.95 par décodeur', \"04\"),\n",
    "            'eval_test_accuracy_mean': ('Test Accuracy (moyenne des 5 folds de tous les encodeurs)', 'Test Accuracy par décodeur', \"05\"),\n",
    "            'eval_test_recall_mean': ('Test Recall (moyenne des 5 folds de tous les encodeurs)', 'Test Recall par décodeur', \"06\"),\n",
    "            'eval_test_precision_mean': ('Test Precision (moyenne des 5 folds de tous les encodeurs)', 'Test Precision par décodeur', \"07\"),\n",
    "            'eval_test_f1_score_mean': ('Test F1 Score (moyenne des 5 folds de tous les encodeurs)', 'Test F1 Score par décodeur', \"08\")\n",
    "        }\n",
    "        \n",
    "        x_label, title, graphic_number = metric_info.get(performance_metric, (performance_metric, f'Performance par décodeur', \"99\"))\n",
    "        ax.set_xlabel(x_label, fontsize=10)\n",
    "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/ch4_06_architecture_boxplot_{graphic_number}_{performance_metric}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for different metrics\n",
    "plot_architecture_boxplot(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_iou_mean', save_path=GRAPHICS_PATH)\n",
    "plot_architecture_boxplot(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_50_mean', save_path=GRAPHICS_PATH)\n",
    "plot_architecture_boxplot(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_75_mean', save_path=GRAPHICS_PATH)\n",
    "plot_architecture_boxplot(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_95_mean', save_path=GRAPHICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plots by Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_backbone_boxplot(df_models, performance_metric='eval_test_iou_mean', save_path=None):\n",
    "    \"\"\"\n",
    "    Create box plot showing performance distribution by backbone.\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model results\n",
    "        performance_metric: Metric to visualize\n",
    "        save_path: Path to save figure\n",
    "    \"\"\"\n",
    "    set_plot_style()\n",
    "    \n",
    "    if 'backbone' not in df_models.columns:\n",
    "        print(\"Backbone column not found\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 4.5))\n",
    "    \n",
    "    plot_data = df_models[[performance_metric, 'backbone']].dropna()\n",
    "    if len(plot_data) > 0:\n",
    "        sns.boxplot(data=plot_data, y='backbone', x=performance_metric, ax=ax)\n",
    "        \n",
    "        # Set labels and title based on metric\n",
    "        metric_info = {\n",
    "            'eval_test_iou_mean': ('Test IoU (moyenne des 5 folds)', 'Test IoU par encodeur', \"01\"),\n",
    "            'eval_test_map_50_mean': ('Test mAP@0.5 (moyenne des 5 folds)', 'Test mAP@0.5 par encodeur', \"02\"),\n",
    "            'eval_test_map_75_mean': ('Test mAP@0.75 (moyenne des 5 folds)', 'Test mAP@0.75 par encodeur', \"03\"),\n",
    "            'eval_test_map_95_mean': ('Test mAP@0.95 (moyenne des 5 folds)', 'Test mAP@0.95 par encodeur', \"04\"),\n",
    "            'eval_test_accuracy_mean': ('Test Accuracy (moyenne des 5 folds)', 'Test Accuracy par encodeur', \"05\"),\n",
    "            'eval_test_recall_mean': ('Test Recall (moyenne des 5 folds)', 'Test Recall par encodeur', \"06\"),\n",
    "            'eval_test_precision_mean': ('Test Precision (moyenne des 5 folds)', 'Test Precision par encodeur', \"07\"),\n",
    "            'eval_test_f1_score_mean': ('Test F1 Score (moyenne des 5 folds)', 'Test F1 Score par encodeur', \"08\")\n",
    "        }\n",
    "        \n",
    "        x_label, title, graphic_number = metric_info.get(performance_metric, (performance_metric, f'Performance par encodeur', \"99\"))\n",
    "        ax.set_xlabel(x_label, fontsize=10)\n",
    "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('', fontsize=10)\n",
    "        ax.tick_params(axis='y', labelsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/ch4_07_backbone_boxplot_{graphic_number}_{performance_metric}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for different metrics\n",
    "plot_backbone_boxplot(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_iou_mean', save_path=GRAPHICS_PATH)\n",
    "plot_backbone_boxplot(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_50_mean', save_path=GRAPHICS_PATH)\n",
    "plot_backbone_boxplot(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_75_mean', save_path=GRAPHICS_PATH)\n",
    "plot_backbone_boxplot(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_95_mean', save_path=GRAPHICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto Frontier - Performance vs Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_vs_training_time_pareto(df_models, performance_metric='eval_test_iou_mean', save_path=None):\n",
    "    \"\"\"\n",
    "    Plot performance vs training time with Pareto frontier.\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model results\n",
    "        performance_metric: Metric to visualize\n",
    "        save_path: Path to save figure\n",
    "    \"\"\"\n",
    "    set_plot_style()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 5.5))\n",
    "    \n",
    "    if 'total_training_time_hour' in df_models.columns and 'architecture' in df_models.columns:\n",
    "        # Check available columns\n",
    "        required_cols = [performance_metric, 'total_training_time_hour', 'architecture', 'config_']\n",
    "        if 'backbone' in df_models.columns:\n",
    "            required_cols.append('backbone')\n",
    "        scatter_data = df_models[required_cols].dropna()\n",
    "        \n",
    "        if len(scatter_data) > 0:\n",
    "            # Plot all points with low alpha\n",
    "            for i, arch in enumerate(scatter_data['architecture'].unique()):\n",
    "                arch_data = scatter_data[scatter_data['architecture'] == arch]\n",
    "                ax.scatter(arch_data['total_training_time_hour'], \n",
    "                          arch_data[performance_metric], \n",
    "                          label=arch, alpha=0.3, s=60)\n",
    "            \n",
    "            # Find Pareto frontier\n",
    "            pareto_models = []\n",
    "            \n",
    "            for idx, row in scatter_data.iterrows():\n",
    "                is_pareto = True\n",
    "                for idx2, row2 in scatter_data.iterrows():\n",
    "                    if idx != idx2:\n",
    "                        # Check if row2 dominates row (better performance AND faster)\n",
    "                        if (row2[performance_metric] >= row[performance_metric] and \n",
    "                            row2['total_training_time_hour'] <= row['total_training_time_hour'] and\n",
    "                            (row2[performance_metric] > row[performance_metric] or \n",
    "                             row2['total_training_time_hour'] < row['total_training_time_hour'])):\n",
    "                            is_pareto = False\n",
    "                            break\n",
    "                \n",
    "                if is_pareto:\n",
    "                    pareto_models.append(idx)\n",
    "            \n",
    "            # Highlight Pareto frontier models\n",
    "            pareto_data = scatter_data.loc[pareto_models].sort_values('total_training_time_hour')\n",
    "            \n",
    "            # Plot Pareto frontier line\n",
    "            ax.plot(pareto_data['total_training_time_hour'], \n",
    "                   pareto_data[performance_metric], \n",
    "                   'r-', linewidth=2, alpha=0.7)\n",
    "            \n",
    "            # Highlight Pareto points\n",
    "            ax.scatter(pareto_data['total_training_time_hour'], \n",
    "                      pareto_data[performance_metric], \n",
    "                      color='red', s=100, zorder=5, edgecolors='black', linewidth=0.5, alpha=0.8)\n",
    "            \n",
    "            # Create model name list for legend\n",
    "            model_legend_text = []\n",
    "            \n",
    "            # Annotate Pareto frontier models\n",
    "            for i, (_, row) in enumerate(pareto_data.iterrows(), 1):\n",
    "                # Clean config name\n",
    "                config_name = str(row['config_'])\n",
    "                if '_' in config_name:\n",
    "                    config_name = '_'.join(config_name.split('_')[1:])\n",
    "                config_name = config_name.replace('_imagenet', '')\n",
    "                \n",
    "                # Get decoder\n",
    "                decoder = row['architecture']\n",
    "                \n",
    "                # Alternate annotation position\n",
    "                if i % 2 == 1:\n",
    "                    offset_x = -10\n",
    "                else:\n",
    "                    offset_x = 10\n",
    "                ax.annotate(str(i), \n",
    "                           xy=(row['total_training_time_hour'], row[performance_metric]),\n",
    "                           xytext=(offset_x, -5), textcoords='offset points',\n",
    "                           fontsize=8,\n",
    "                           ha='center', va='center',\n",
    "                           color='black', zorder=10,)\n",
    "                \n",
    "                # Add to legend text\n",
    "                model_legend_text.append(f\"{i}. {config_name} ({decoder})\")\n",
    "            \n",
    "            # Set labels and title\n",
    "            ax.set_xlabel(\"Temps d'entraînement moyen des 5 folds\\nde tous les encodeurs (heures)\", fontsize=10)\n",
    "            \n",
    "            # Set labels and title based on metric\n",
    "            metric_info = {\n",
    "                'eval_test_iou_mean': ('Test IoU', \"IoU vs Temps d'entraînement - Modèles optimaux\", \"01\"),\n",
    "                'eval_test_map_50_mean': ('Test mAP@0.5', \"mAP@0.5 vs Temps d'entraînement - Modèles optimaux\", \"02\"),\n",
    "                'eval_test_map_75_mean': ('Test mAP@0.75', \"mAP@0.75 vs Temps d'entraînement - Modèles optimaux\", \"03\"),\n",
    "                'eval_test_map_95_mean': ('Test mAP@0.95', \"mAP@0.95 vs Temps d'entraînement - Modèles optimaux\", \"04\"),\n",
    "                'eval_test_accuracy_mean': ('Test Accuracy', \"Accuracy vs Temps d'entraînement - Modèles optimaux\", \"05\"),\n",
    "                'eval_test_recall_mean': ('Test Recall', \"Recall vs Temps d'entraînement - Modèles optimaux\", \"06\"),\n",
    "                'eval_test_precision_mean': ('Test Precision', \"Precision vs Temps d'entraînement - Modèles optimaux\", \"07\"),\n",
    "                'eval_test_f1_score_mean': ('Test F1 Score', \"F1 Score vs Temps d'entraînement - Modèles optimaux\", \"08\")\n",
    "            }\n",
    "            \n",
    "            y_label, title, graphic_number = metric_info.get(performance_metric, (performance_metric, f'Performance vs Temps', \"99\"))\n",
    "            ax.set_ylabel(y_label, fontsize=10)\n",
    "            ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "            \n",
    "            # Create architecture legend\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            legend1 = ax.legend(handles, labels,\n",
    "                               bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "                               fontsize=8, framealpha=0.9)\n",
    "            \n",
    "            ax.add_artist(legend1)\n",
    "            \n",
    "            # Create model numbers legend\n",
    "            if model_legend_text:\n",
    "                from matplotlib.patches import Rectangle\n",
    "                extra = Rectangle((0, 0), 1, 1, fc=\"w\", fill=False, edgecolor='none', linewidth=0)\n",
    "                \n",
    "                legend2 = ax.legend([extra]*len(model_legend_text), model_legend_text,\n",
    "                                  title='Modèles optimaux', title_fontsize=9,\n",
    "                                  bbox_to_anchor=(1.05, 0.00), loc='lower left',\n",
    "                                  fontsize=8, framealpha=0.9,\n",
    "                                  handlelength=0, handletextpad=0)\n",
    "            \n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/ch4_08_performance_vs_training_time_pareto_{graphic_number}_{performance_metric}.png\", \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for different metrics\n",
    "plot_performance_vs_training_time_pareto(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_iou_mean', save_path=GRAPHICS_PATH)\n",
    "plot_performance_vs_training_time_pareto(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_50_mean', save_path=GRAPHICS_PATH)\n",
    "plot_performance_vs_training_time_pareto(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_75_mean', save_path=GRAPHICS_PATH)\n",
    "plot_performance_vs_training_time_pareto(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_95_mean', save_path=GRAPHICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto Frontier - Performance vs Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_vs_parameters_pareto(df_models, performance_metric='eval_test_iou_mean', save_path=None):\n",
    "    \"\"\"\n",
    "    Plot performance vs number of parameters with Pareto frontier.\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model results\n",
    "        performance_metric: Metric to visualize\n",
    "        save_path: Path to save figure\n",
    "    \"\"\"\n",
    "    set_plot_style()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 5.5))\n",
    "    \n",
    "    if 'num_params_M' in df_models.columns and 'architecture' in df_models.columns:\n",
    "        # Check available columns\n",
    "        required_cols = [performance_metric, 'num_params_M', 'architecture', 'config_']\n",
    "        if 'backbone' in df_models.columns:\n",
    "            required_cols.append('backbone')\n",
    "        scatter_data = df_models[required_cols].dropna()\n",
    "        \n",
    "        if len(scatter_data) > 0:\n",
    "            # Plot all points with low alpha\n",
    "            for i, arch in enumerate(scatter_data['architecture'].unique()):\n",
    "                arch_data = scatter_data[scatter_data['architecture'] == arch]\n",
    "                ax.scatter(arch_data['num_params_M'], \n",
    "                          arch_data[performance_metric], \n",
    "                          label=arch, alpha=0.3, s=60)\n",
    "            \n",
    "            # Find Pareto frontier\n",
    "            pareto_models = []\n",
    "            \n",
    "            for idx, row in scatter_data.iterrows():\n",
    "                is_pareto = True\n",
    "                for idx2, row2 in scatter_data.iterrows():\n",
    "                    if idx != idx2:\n",
    "                        # Check if row2 dominates row (better performance AND fewer parameters)\n",
    "                        if (row2[performance_metric] >= row[performance_metric] and \n",
    "                            row2['num_params_M'] <= row['num_params_M'] and\n",
    "                            (row2[performance_metric] > row[performance_metric] or \n",
    "                             row2['num_params_M'] < row['num_params_M'])):\n",
    "                            is_pareto = False\n",
    "                            break\n",
    "                \n",
    "                if is_pareto:\n",
    "                    pareto_models.append(idx)\n",
    "            \n",
    "            # Highlight Pareto frontier models\n",
    "            pareto_data = scatter_data.loc[pareto_models].sort_values('num_params_M')\n",
    "            \n",
    "            # Plot Pareto frontier line\n",
    "            ax.plot(pareto_data['num_params_M'], \n",
    "                   pareto_data[performance_metric], \n",
    "                   'r-', linewidth=2, alpha=0.7)\n",
    "            \n",
    "            # Highlight Pareto points\n",
    "            ax.scatter(pareto_data['num_params_M'], \n",
    "                      pareto_data[performance_metric], \n",
    "                      color='red', s=100, zorder=5, edgecolors='black', linewidth=0.5, alpha=0.8)\n",
    "            \n",
    "            # Create model name list for legend\n",
    "            model_legend_text = []\n",
    "            \n",
    "            # Annotate Pareto frontier models\n",
    "            for i, (_, row) in enumerate(pareto_data.iterrows(), 1):\n",
    "                # Clean config name\n",
    "                config_name = str(row['config_'])\n",
    "                if '_' in config_name:\n",
    "                    config_name = '_'.join(config_name.split('_')[1:])\n",
    "                config_name = config_name.replace('_imagenet', '')\n",
    "                \n",
    "                # Get decoder\n",
    "                decoder = row['architecture']\n",
    "                \n",
    "                # Alternate annotation position\n",
    "                if i % 2 == 1:\n",
    "                    offset_x = -10\n",
    "                else:\n",
    "                    offset_x = 10\n",
    "                ax.annotate(str(i), \n",
    "                           xy=(row['num_params_M'], row[performance_metric]),\n",
    "                           xytext=(offset_x, -5), textcoords='offset points',\n",
    "                           fontsize=8,\n",
    "                           ha='center', va='center',\n",
    "                           color='black', zorder=10,)\n",
    "                \n",
    "                # Add to legend text\n",
    "                model_legend_text.append(f\"{i}. {config_name} ({decoder})\")\n",
    "            \n",
    "            # Set labels and title\n",
    "            ax.set_xlabel(\"Nombre de paramètres (millions)\", fontsize=10)\n",
    "            \n",
    "            # Set labels and title based on metric\n",
    "            metric_info = {\n",
    "                'eval_test_iou_mean': ('Test IoU', \"IoU vs Nombre de paramètres - Modèles optimaux\", \"01\", 0.00),\n",
    "                'eval_test_map_50_mean': ('Test mAP@0.5', \"mAP@0.5 vs Nombre de paramètres - Modèles optimaux\", \"02\", 0.00),\n",
    "                'eval_test_map_75_mean': ('Test mAP@0.75', \"mAP@0.75 vs Nombre de paramètres - Modèles optimaux\", \"03\", 0.00),\n",
    "                'eval_test_map_95_mean': ('Test mAP@0.95', \"mAP@0.95 vs Nombre de paramètres - Modèles optimaux\", \"04\", 0.00),\n",
    "                'eval_test_accuracy_mean': ('Test Accuracy', \"Accuracy vs Nombre de paramètres - Modèles optimaux\", \"05\", 0.00),\n",
    "                'eval_test_recall_mean': ('Test Recall', \"Recall vs Nombre de paramètres - Modèles optimaux\", \"06\", 0.00),\n",
    "                'eval_test_precision_mean': ('Test Precision', \"Precision vs Nombre de paramètres - Modèles optimaux\", \"07\", 0.00),\n",
    "                'eval_test_f1_score_mean': ('Test F1 Score', \"F1 Score vs Nombre de paramètres - Modèles optimaux\", \"08\", 0.00)\n",
    "            }\n",
    "            \n",
    "            y_label, title, graphic_number, legend_y = metric_info.get(performance_metric, (performance_metric, f'Performance vs Paramètres', \"99\", 0.00))\n",
    "            ax.set_ylabel(y_label, fontsize=10)\n",
    "            ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "            \n",
    "            # Create architecture legend\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            legend1 = ax.legend(handles, labels,\n",
    "                               bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "                               fontsize=8, framealpha=0.9)\n",
    "            \n",
    "            ax.add_artist(legend1)\n",
    "            \n",
    "            # Create model numbers legend\n",
    "            if model_legend_text:\n",
    "                from matplotlib.patches import Rectangle\n",
    "                extra = Rectangle((0, 0), 1, 1, fc=\"w\", fill=False, edgecolor='none', linewidth=0)\n",
    "                \n",
    "                legend2 = ax.legend([extra]*len(model_legend_text), model_legend_text,\n",
    "                                  title='Modèles optimaux', title_fontsize=9,\n",
    "                                  bbox_to_anchor=(1.05, legend_y), loc='lower left',\n",
    "                                  fontsize=8, framealpha=0.9,\n",
    "                                  handlelength=0, handletextpad=0)\n",
    "            \n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/ch4_09_performance_vs_parameters_pareto_{graphic_number}_{performance_metric}.png\", \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for different metrics\n",
    "plot_performance_vs_parameters_pareto(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_iou_mean', save_path=GRAPHICS_PATH)\n",
    "plot_performance_vs_parameters_pareto(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_50_mean', save_path=GRAPHICS_PATH)\n",
    "plot_performance_vs_parameters_pareto(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_75_mean', save_path=GRAPHICS_PATH)\n",
    "plot_performance_vs_parameters_pareto(df_model_results_clean_concat_yolo_smp, performance_metric='eval_test_map_95_mean', save_path=GRAPHICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto Frontier - IoU vs Other Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iou_vs_metric_pareto(df_models, second_metric='eval_test_f1_score_mean', save_path=None):\n",
    "    \"\"\"\n",
    "    Plot IoU vs another metric with Pareto frontier.\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model results\n",
    "        second_metric: Second metric to compare against IoU\n",
    "        save_path: Path to save figure\n",
    "    \"\"\"\n",
    "    set_plot_style()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 4.5))\n",
    "    \n",
    "    iou_metric = 'eval_test_iou_mean'\n",
    "    \n",
    "    if iou_metric in df_models.columns and second_metric in df_models.columns and 'architecture' in df_models.columns:\n",
    "        # Check available columns\n",
    "        required_cols = [iou_metric, second_metric, 'architecture', 'config_', 'num_params_M']\n",
    "        scatter_data = df_models[required_cols].dropna()\n",
    "        \n",
    "        if len(scatter_data) > 0:\n",
    "            # Plot all points with low alpha\n",
    "            for i, arch in enumerate(scatter_data['architecture'].unique()):\n",
    "                arch_data = scatter_data[scatter_data['architecture'] == arch]\n",
    "                ax.scatter(arch_data[iou_metric], \n",
    "                          arch_data[second_metric], \n",
    "                          label=arch, alpha=0.3, s=60)\n",
    "            \n",
    "            # Find Pareto frontier\n",
    "            pareto_models = []\n",
    "            \n",
    "            for idx, row in scatter_data.iterrows():\n",
    "                is_pareto = True\n",
    "                for idx2, row2 in scatter_data.iterrows():\n",
    "                    if idx != idx2:\n",
    "                        # Check if row2 dominates row (better IoU AND better second metric)\n",
    "                        if (row2[iou_metric] >= row[iou_metric] and \n",
    "                            row2[second_metric] >= row[second_metric] and\n",
    "                            (row2[iou_metric] > row[iou_metric] or \n",
    "                             row2[second_metric] > row[second_metric])):\n",
    "                            is_pareto = False\n",
    "                            break\n",
    "                \n",
    "                if is_pareto:\n",
    "                    pareto_models.append(idx)\n",
    "            \n",
    "            # Highlight Pareto frontier models\n",
    "            pareto_data = scatter_data.loc[pareto_models].sort_values(iou_metric)\n",
    "            \n",
    "            # Plot Pareto frontier line\n",
    "            ax.plot(pareto_data[iou_metric], \n",
    "                   pareto_data[second_metric], \n",
    "                   'r-', linewidth=2, alpha=0.7)\n",
    "            \n",
    "            # Highlight Pareto points\n",
    "            ax.scatter(pareto_data[iou_metric], \n",
    "                      pareto_data[second_metric], \n",
    "                      color='red', s=100, zorder=5, edgecolors='black', linewidth=0.5, alpha=0.8)\n",
    "            \n",
    "            # Create model name list for legend\n",
    "            model_legend_text = []\n",
    "            \n",
    "            # Annotate Pareto frontier models\n",
    "            for i, (_, row) in enumerate(pareto_data.iterrows(), 1):\n",
    "                # Clean config name\n",
    "                config_name = str(row['config_'])\n",
    "                if '_' in config_name:\n",
    "                    config_name = '_'.join(config_name.split('_')[1:])\n",
    "                config_name = config_name.replace('_imagenet', '')\n",
    "                \n",
    "                # Get decoder and parameters\n",
    "                decoder = row['architecture']\n",
    "                params = row['num_params_M']\n",
    "                \n",
    "                # Alternate annotation position\n",
    "                if i % 2 == 1:\n",
    "                    offset_x = -10\n",
    "                else:\n",
    "                    offset_x = 10\n",
    "                ax.annotate(str(i), \n",
    "                           xy=(row[iou_metric], row[second_metric]),\n",
    "                           xytext=(offset_x, -5), textcoords='offset points',\n",
    "                           fontsize=8,\n",
    "                           ha='center', va='center',\n",
    "                           color='black', zorder=10,)\n",
    "                \n",
    "                # Add to legend text\n",
    "                model_legend_text.append(f\"{i}. {config_name} ({decoder}) [{params:.0f}M]\")\n",
    "            \n",
    "            # Set labels\n",
    "            ax.set_xlabel(\"Test IoU (moyenne des 5 folds)\", fontsize=10)\n",
    "            \n",
    "            # Determine Y-axis label and title based on second metric\n",
    "            metric_info = {\n",
    "                'eval_test_f1_score_mean': ('Test F1-Score (moyenne des 5 folds)', \"IoU vs F1-Score - Modèles optimaux\", \"01\"),\n",
    "                'eval_test_map_50_mean': ('Test mAP@0.5 (moyenne des 5 folds)', \"IoU vs mAP@0.5 - Modèles optimaux\", \"02\"),\n",
    "                'eval_test_map_75_mean': ('Test mAP@0.75 (moyenne des 5 folds)', \"IoU vs mAP@0.75 - Modèles optimaux\", \"03\"),\n",
    "                'eval_test_map_95_mean': ('Test mAP@0.95 (moyenne des 5 folds)', \"IoU vs mAP@0.95 - Modèles optimaux\", \"04\"),\n",
    "                'eval_test_accuracy_mean': ('Test Accuracy (moyenne des 5 folds)', \"IoU vs Accuracy - Modèles optimaux\", \"05\"),\n",
    "                'eval_test_recall_mean': ('Test Recall (moyenne des 5 folds)', \"IoU vs Recall - Modèles optimaux\", \"06\"),\n",
    "                'eval_test_precision_mean': ('Test Precision (moyenne des 5 folds)', \"IoU vs Precision - Modèles optimaux\", \"07\")\n",
    "            }\n",
    "            \n",
    "            y_label, title, graphic_number = metric_info.get(second_metric, (second_metric, f'IoU vs {second_metric}', \"99\"))\n",
    "            ax.set_ylabel(y_label, fontsize=10)\n",
    "            ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "            \n",
    "            # Add diagonal reference line for comparable metrics\n",
    "            if second_metric in ['eval_test_f1_score_mean', 'eval_test_accuracy_mean']:\n",
    "                min_val = min(ax.get_xlim()[0], ax.get_ylim()[0])\n",
    "                max_val = max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "                ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.3, linewidth=1)\n",
    "            \n",
    "            # Create architecture legend\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            legend1 = ax.legend(handles, labels,\n",
    "                               bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "                               fontsize=8, framealpha=0.9)\n",
    "            \n",
    "            ax.add_artist(legend1)\n",
    "            \n",
    "            # Create model numbers legend\n",
    "            if model_legend_text:\n",
    "                from matplotlib.patches import Rectangle\n",
    "                extra = Rectangle((0, 0), 1, 1, fc=\"w\", fill=False, edgecolor='none', linewidth=0)\n",
    "                \n",
    "                legend2 = ax.legend([extra]*len(model_legend_text), model_legend_text,\n",
    "                                  title='Modèles optimaux', title_fontsize=9,\n",
    "                                  bbox_to_anchor=(1.05, 0.00), loc='lower left',\n",
    "                                  fontsize=8, framealpha=0.9,\n",
    "                                  handlelength=0, handletextpad=0)\n",
    "            \n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/ch4_10_{graphic_number}_iou_vs_{second_metric}.png\", \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot IoU vs different metrics\n",
    "plot_iou_vs_metric_pareto(df_model_results_clean_concat_yolo_smp, second_metric='eval_test_f1_score_mean', save_path=GRAPHICS_PATH)\n",
    "plot_iou_vs_metric_pareto(df_model_results_clean_concat_yolo_smp, second_metric='eval_test_map_50_mean', save_path=GRAPHICS_PATH)\n",
    "plot_iou_vs_metric_pareto(df_model_results_clean_concat_yolo_smp, second_metric='eval_test_map_75_mean', save_path=GRAPHICS_PATH)\n",
    "plot_iou_vs_metric_pareto(df_model_results_clean_concat_yolo_smp, second_metric='eval_test_map_95_mean', save_path=GRAPHICS_PATH)\n",
    "plot_iou_vs_metric_pareto(df_model_results_clean_concat_yolo_smp, second_metric='eval_test_accuracy_mean', save_path=GRAPHICS_PATH)\n",
    "plot_iou_vs_metric_pareto(df_model_results_clean_concat_yolo_smp, second_metric='eval_test_recall_mean', save_path=GRAPHICS_PATH)\n",
    "plot_iou_vs_metric_pareto(df_model_results_clean_concat_yolo_smp, second_metric='eval_test_precision_mean', save_path=GRAPHICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Time Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_time_distribution(df_models, save_path=None):\n",
    "    \"\"\"\n",
    "    Create histogram showing training time distribution.\n",
    "    \n",
    "    Parameters:\n",
    "        df_models: DataFrame with model results\n",
    "        save_path: Path to save figure\n",
    "    \"\"\"\n",
    "    set_plot_style()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 4.5))\n",
    "    \n",
    "    if 'total_training_time_hour' in df_models.columns:\n",
    "        valid_times = df_models['total_training_time_hour'].dropna()\n",
    "        if len(valid_times) > 0:\n",
    "            # Create bins every 2 hours\n",
    "            max_time = valid_times.max()\n",
    "            min_time = valid_times.min()\n",
    "            \n",
    "            # Round to even numbers\n",
    "            max_bin = int(np.ceil(max_time / 2) * 2)\n",
    "            min_bin = int(np.floor(min_time / 2) * 2)\n",
    "            \n",
    "            # Create bins\n",
    "            bins = np.arange(min_bin, max_bin + 2, 2)\n",
    "            \n",
    "            # Plot histogram\n",
    "            n, bins, patches = ax.hist(valid_times, bins=bins, alpha=0.7, \n",
    "                                      edgecolor='white', linewidth=1)\n",
    "            \n",
    "            # Add mean line\n",
    "            mean_val = valid_times.mean()\n",
    "            ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, alpha=0.5,\n",
    "                      label=f'Moyenne: {mean_val:.1f}h')\n",
    "            \n",
    "            # Add median line\n",
    "            median_val = valid_times.median()\n",
    "            ax.axvline(median_val, color='green', linestyle='--', linewidth=2, alpha=0.5,\n",
    "                      label=f'Médiane: {median_val:.1f}h')\n",
    "            \n",
    "            # Set labels and title\n",
    "            ax.set_title(\"Distribution du temps d'entraînement\", fontsize=11, fontweight='bold')\n",
    "            ax.set_xlabel(\"Temps d'entraînement des 5 folds (heures)\", fontsize=10)\n",
    "            ax.set_ylabel('Nombre de modèles', fontsize=10)\n",
    "            \n",
    "            # Set x-axis ticks\n",
    "            ax.set_xticks(bins)\n",
    "            ax.set_xticklabels([f'{int(b)}' for b in bins], fontsize=9)\n",
    "            \n",
    "            # Add grid\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            ax.grid(True, alpha=0.1, axis='x')\n",
    "            \n",
    "            # Legend\n",
    "            ax.legend(fontsize=9, loc='upper right')\n",
    "            \n",
    "            # Add statistics text\n",
    "            stats_text = f'Total: {len(valid_times)} modèles\\n'\n",
    "            stats_text += f'Min: {valid_times.min():.1f}h\\n'\n",
    "            stats_text += f'Max: {valid_times.max():.1f}h'\n",
    "            \n",
    "            ax.text(0.787, 0.835, stats_text, transform=ax.transAxes, \n",
    "                   fontsize=9, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.5,\n",
    "                            edgecolor='lightgray', zorder=10))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/ch4_11_training_time_dist_09.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot training time distribution\n",
    "plot_training_time_distribution(df_model_results_clean, save_path=GRAPHICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "### Top 10 Models by IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 10 models by IoU\n",
    "top_10_iou = df_model_results_clean_concat_yolo_smp.sort_values('eval_test_iou_mean', ascending=False).head(10)[\n",
    "    [\"config\", \"eval_test_iou_mean\", \"eval_test_f1_score_mean\", \"eval_test_map_50_mean\", \n",
    "     \"eval_test_map_75_mean\", \"eval_test_map_95_mean\", \"num_params_M\", \"total_training_time_hour\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(top_10_iou.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display overall statistics\n",
    "metrics = ['eval_test_iou_mean', 'eval_test_f1_score_mean', 'eval_test_map_50_mean', \n",
    "           'eval_test_map_75_mean', 'eval_test_map_95_mean', 'eval_test_accuracy_mean', \n",
    "           'eval_test_recall_mean', 'eval_test_precision_mean']\n",
    "\n",
    "print(\"Mean performance of all models:\")\n",
    "for metric in metrics:\n",
    "    if metric in df_model_results_clean_concat_yolo_smp.columns:\n",
    "        mean_val = df_model_results_clean_concat_yolo_smp[metric].mean()\n",
    "        print(f\"{metric.replace('eval_test_', '').replace('_mean', '')}: {mean_val:.3f}\")\n",
    "\n",
    "print(\"\\nDetailed statistics (LaTeX format):\")\n",
    "for metric in metrics:\n",
    "    if metric in df_model_results_clean_concat_yolo_smp.columns:\n",
    "        data = df_model_results_clean_concat_yolo_smp[metric]\n",
    "        metric_name = metric.replace('eval_test_', '').replace('_mean', '').replace('_', ' ').title()\n",
    "        print(f\"{metric_name} & {data.mean():.3f} \\\\pm {data.std():.3f} & {data.median():.3f} & {data.min():.3f} & {data.max():.3f} \\\\\\\\\")\n",
    "\n",
    "# Add parameter and training time statistics\n",
    "print(f\"Nombre de paramètres & {df_model_results_clean_concat_yolo_smp['num_params_M'].mean():.3f} \\\\pm {df_model_results_clean_concat_yolo_smp['num_params_M'].std():.3f} & {df_model_results_clean_concat_yolo_smp['num_params_M'].median():.3f} & {df_model_results_clean_concat_yolo_smp['num_params_M'].min():.3f} & {df_model_results_clean_concat_yolo_smp['num_params_M'].max():.3f} \\\\\\\\\")\n",
    "print(f\"Temps d'entraînement (heures) & {df_model_results_clean_concat_yolo_smp['total_training_time_hour'].mean():.3f} \\\\pm {df_model_results_clean_concat_yolo_smp['total_training_time_hour'].std():.3f} & {df_model_results_clean_concat_yolo_smp['total_training_time_hour'].median():.3f} & {df_model_results_clean_concat_yolo_smp['total_training_time_hour'].min():.3f} & {df_model_results_clean_concat_yolo_smp['total_training_time_hour'].max():.3f} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance by Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_results_clean_concat_yolo_smp.groupby('architecture').agg({\n",
    "    'eval_test_iou_mean': ['mean', 'std', 'median', 'min', 'max'],\n",
    "\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_model_results_clean_concat_yolo_smp.groupby('architecture').agg({\n",
    "    'eval_test_iou_mean': ['mean', 'std', 'median', 'min', 'max'],\n",
    "}).reset_index().to_latex(index=False, \n",
    "    caption=\"Statistiques IoU par décodeur\",\n",
    "    label=\"tab:statistique_par_decodeur_iou\",\n",
    "    float_format=\"%.3f\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_model_results_clean_concat_yolo_smp.groupby('architecture').agg({\n",
    "    'eval_test_map_95_mean': ['mean', 'std', 'median', 'min', 'max'],\n",
    "}).reset_index().to_latex(index=False, \n",
    "    caption=\"Statistiques mAP@0.95 par décodeur\",\n",
    "    label=\"tab:statistique_par_decodeur_map95\",\n",
    "    float_format=\"%.3f\",\n",
    "    column_format=\"lccccccccc\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance by Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_model_results_clean_concat_yolo_smp.groupby('backbone').agg({\n",
    "    'eval_test_iou_mean': ['mean', 'std', 'median', 'min', 'max'],\n",
    "}).reset_index().to_latex(index=False, \n",
    "    caption=\"Statistiques IoU par décodeur\",\n",
    "    label=\"tab:statistique_par_encodeur_iou\",\n",
    "    float_format=\"%.3f\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_model_results_clean_concat_yolo_smp.groupby('backbone').agg({\n",
    "    'eval_test_map_95_mean': ['mean', 'std', 'median', 'min', 'max'],\n",
    "}).reset_index().to_latex(index=False, \n",
    "    caption=\"Statistiques mAP@0.95 par décodeur\",\n",
    "    label=\"tab:statistique_par_encodeur_map95\",\n",
    "    float_format=\"%.3f\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto Optimal Models Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specific models from Pareto frontier\n",
    "pareto_models = ['linknet_timm_efficientnet_b5_imagenet', \n",
    "                 'segformer_regnety_032_imagenet',\n",
    "                 'unetplusplus_efficientnet_b3_imagenet']\n",
    "\n",
    "for model_config in pareto_models:\n",
    "    model_data = df_model_results_clean_concat_yolo_smp[\n",
    "        df_model_results_clean_concat_yolo_smp[\"config\"] == model_config\n",
    "    ]\n",
    "    if not model_data.empty:\n",
    "        print(f\"\\n{model_config}:\")\n",
    "        print(model_data[[\"config\", \"eval_test_iou_mean\", \"eval_test_f1_score_mean\", \n",
    "                         \"eval_test_map_50_mean\", \"eval_test_map_75_mean\", \"eval_test_map_95_mean\",\n",
    "                         \"num_params_M\", \"total_training_time_hour\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Threshold mAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by architecture and calculate mean mAP at different thresholds\n",
    "df_grouped = df_model_results_clean_concat_yolo_smp.groupby('architecture').agg({\n",
    "    'eval_test_map_50_mean': ['mean'],\n",
    "    'eval_test_map_65_mean': ['mean'],\n",
    "    'eval_test_map_75_mean': ['mean'],\n",
    "    'eval_test_map_85_mean': ['mean'],\n",
    "    'eval_test_map_90_mean': ['mean'],\n",
    "    'eval_test_map_95_mean': ['mean'],\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "df_grouped.columns = ['architecture', 'map_50_mean', 'map_65_mean', 'map_75_mean', \n",
    "                      'map_85_mean', 'map_90_mean', 'map_95_mean']\n",
    "\n",
    "# Calculate percentage drop\n",
    "df_grouped['percentage_drop_50_to_95'] = ((df_grouped['map_50_mean'] - df_grouped['map_95_mean']) / df_grouped['map_50_mean']) * 100\n",
    "\n",
    "print(\"Multi-threshold mAP Analysis:\")\n",
    "print(df_grouped.to_latex(index=False, \n",
    "    caption=\"Performances moyennes mAP à différents seuils\",\n",
    "    label=\"tab:performance_moyenne_map_different_seuils\",\n",
    "    float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Analysis on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_results_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "\n",
    "class LaTeXVisualizationSystem:\n",
    "    \"\"\"\n",
    "    Visualization system for segmentation results with ensemble support.\n",
    "    \n",
    "    Parameters:\n",
    "        img_size: Target image size (width, height)\n",
    "        device: CUDA device to use\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=(256, 256), device='cuda:0'):\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "        self.overlay_alpha = 0.5\n",
    "        self.mask_color_gt = [0, 0, 255]      # Blue for ground truth\n",
    "        self.mask_color_pred = [255, 0, 0]    # Red for prediction\n",
    "        self.mask_color_ensemble = [0, 255, 0] # Green for ensemble\n",
    "        \n",
    "    def load_model_for_inference(self, model_info, device_id=0):\n",
    "        \"\"\"\n",
    "        Load a model for inference.\n",
    "        \n",
    "        Parameters:\n",
    "            model_info: Dictionary with model configuration\n",
    "            device_id: GPU device ID\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (model, device)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            device = torch.device(f'cuda:{device_id}')\n",
    "            \n",
    "            config = {\n",
    "                \"architecture\": model_info[\"architecture\"],\n",
    "                \"backbone\": model_info[\"backbone\"],\n",
    "                \"encoder_weights\": model_info.get(\"encoder_weights\"),\n",
    "                \"num_classes\": int(model_info.get(\"num_classes\", 1)),\n",
    "                \"model_path\": model_info[\"model_path\"],\n",
    "            }\n",
    "            \n",
    "            # Create model\n",
    "            model, _ = create_model(config)\n",
    "            \n",
    "            # Load weights\n",
    "            import logging\n",
    "            logger = logging.getLogger('silent')\n",
    "            logger.setLevel(logging.CRITICAL)\n",
    "            \n",
    "            if not corriger_state_dict(model, config[\"model_path\"], logger):\n",
    "                raise Exception(\"Failed to load model weights\")\n",
    "            \n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            return model, device\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def predict_batch(self, model, image_paths, device, batch_size=8):\n",
    "        \"\"\"\n",
    "        Generate predictions for a batch of images.\n",
    "        \n",
    "        Parameters:\n",
    "            model: PyTorch model\n",
    "            image_paths: List of image paths\n",
    "            device: Torch device\n",
    "            batch_size: Batch size for inference\n",
    "            \n",
    "        Returns:\n",
    "            List of prediction dictionaries\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            \n",
    "            # Load and preprocess batch\n",
    "            for image_path in batch_paths:\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                image_resized = image.resize(self.img_size, Image.Resampling.BILINEAR)\n",
    "                image_array = np.array(image_resized).astype(np.float32) / 255.0\n",
    "                batch_images.append(image_array)\n",
    "            \n",
    "            # Convert to tensor\n",
    "            batch_array = np.stack(batch_images)\n",
    "            batch_tensor = torch.from_numpy(batch_array).permute(0, 3, 1, 2)\n",
    "            batch_tensor = batch_tensor.to(device)\n",
    "            \n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(batch_tensor)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    \n",
    "                    # Apply sigmoid and threshold\n",
    "                    pred_masks = torch.sigmoid(outputs).squeeze(1).cpu().numpy()\n",
    "                    pred_binaries = (pred_masks > 0.5).astype(np.uint8)\n",
    "            \n",
    "            # Store results\n",
    "            for j, (pred_mask, pred_binary) in enumerate(zip(pred_masks, pred_binaries)):\n",
    "                predictions.append({\n",
    "                    'image_array': batch_images[j],\n",
    "                    'pred_mask': pred_mask,\n",
    "                    'pred_binary': pred_binary\n",
    "                })\n",
    "            \n",
    "            # Clean up\n",
    "            del batch_tensor, outputs\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def ensemble_predict_batch(self, model_infos, image_paths, device_id=0, batch_size=8):\n",
    "        \"\"\"\n",
    "        Generate ensemble predictions using sequential model loading.\n",
    "        \n",
    "        Parameters:\n",
    "            model_infos: List of model info dictionaries\n",
    "            image_paths: List of image paths\n",
    "            device_id: GPU device ID\n",
    "            batch_size: Batch size for inference\n",
    "            \n",
    "        Returns:\n",
    "            List of ensemble prediction dictionaries\n",
    "        \"\"\"\n",
    "        ensemble_predictions = None\n",
    "        num_models = len(model_infos)\n",
    "        \n",
    "        print(f\"Ensemble prediction with {num_models} models...\")\n",
    "        \n",
    "        # Process each model sequentially\n",
    "        for model_idx, model_info in enumerate(tqdm(model_infos, desc=\"Processing models\")):\n",
    "            # Load model\n",
    "            model, device = self.load_model_for_inference(model_info, device_id)\n",
    "            if model is None:\n",
    "                print(f\"Skipping model {model_idx+1}: failed to load\")\n",
    "                continue\n",
    "            \n",
    "            # Initialize storage for this model's predictions\n",
    "            model_predictions = []\n",
    "            \n",
    "            # Process batches\n",
    "            for i in range(0, len(image_paths), batch_size):\n",
    "                batch_paths = image_paths[i:i+batch_size]\n",
    "                batch_images = []\n",
    "                \n",
    "                # Load and preprocess batch\n",
    "                for image_path in batch_paths:\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "                    image_resized = image.resize(self.img_size, Image.Resampling.BILINEAR)\n",
    "                    image_array = np.array(image_resized).astype(np.float32) / 255.0\n",
    "                    batch_images.append(image_array)\n",
    "                \n",
    "                # Convert to tensor\n",
    "                batch_array = np.stack(batch_images)\n",
    "                batch_tensor = torch.from_numpy(batch_array).permute(0, 3, 1, 2)\n",
    "                batch_tensor = batch_tensor.to(device)\n",
    "                \n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(batch_tensor)\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            outputs = outputs[0]\n",
    "                        \n",
    "                        # Apply sigmoid to get probabilities\n",
    "                        pred_probs = torch.sigmoid(outputs).squeeze(1).cpu().numpy()\n",
    "                \n",
    "                # Store predictions for this batch\n",
    "                for j, pred_prob in enumerate(pred_probs):\n",
    "                    if model_idx == 0:\n",
    "                        # First model - store image arrays too\n",
    "                        model_predictions.append({\n",
    "                            'image_array': batch_images[j],\n",
    "                            'pred_prob': pred_prob\n",
    "                        })\n",
    "                    else:\n",
    "                        model_predictions.append(pred_prob)\n",
    "                \n",
    "                # Clean up batch\n",
    "                del batch_tensor, outputs\n",
    "            \n",
    "            # Accumulate predictions\n",
    "            if model_idx == 0:\n",
    "                # Initialize ensemble predictions with first model\n",
    "                ensemble_predictions = model_predictions\n",
    "            else:\n",
    "                # Add to ensemble\n",
    "                for i, pred_prob in enumerate(model_predictions):\n",
    "                    ensemble_predictions[i]['pred_prob'] += pred_prob\n",
    "            \n",
    "            # Free memory before loading next model\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Average predictions and create binary masks\n",
    "        final_predictions = []\n",
    "        for pred_data in ensemble_predictions:\n",
    "            # Average probabilities\n",
    "            avg_prob = pred_data['pred_prob'] / num_models\n",
    "            binary_mask = (avg_prob > 0.5).astype(np.uint8)\n",
    "            \n",
    "            final_predictions.append({\n",
    "                'image_array': pred_data['image_array'],\n",
    "                'pred_mask': avg_prob,\n",
    "                'pred_binary': binary_mask,\n",
    "                'ensemble': True\n",
    "            })\n",
    "        \n",
    "        return final_predictions\n",
    "    \n",
    "    def load_ground_truth(self, mask_path):\n",
    "        \"\"\"Load and process ground truth mask.\"\"\"\n",
    "        mask = Image.open(mask_path)\n",
    "        if mask.mode != \"L\":\n",
    "            mask = mask.convert(\"L\")\n",
    "        \n",
    "        mask = mask.resize(self.img_size, Image.Resampling.NEAREST)\n",
    "        mask_array = np.array(mask)\n",
    "        \n",
    "        # Normalize to binary\n",
    "        if mask_array.max() > 1:\n",
    "            mask_array = (mask_array > 127).astype(np.uint8)\n",
    "        else:\n",
    "            mask_array = (mask_array > 0).astype(np.uint8)\n",
    "            \n",
    "        return mask_array\n",
    "    \n",
    "    def calculate_iou(self, pred_binary, gt_mask):\n",
    "        \"\"\"Calculate IoU for a single image.\"\"\"\n",
    "        intersection = np.logical_and(pred_binary, gt_mask).sum()\n",
    "        union = np.logical_or(pred_binary, gt_mask).sum()\n",
    "        iou = intersection / (union + 1e-6)\n",
    "        return iou\n",
    "    \n",
    "    def save_individual_images(self, image_array, gt_mask, pred_binary, output_dir, image_name, is_ensemble=False):\n",
    "        \"\"\"Save individual images for visualization.\"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save original image\n",
    "        original_img = Image.fromarray((image_array * 255).astype(np.uint8))\n",
    "        original_img.save(output_dir / f\"{image_name}_original.png\")\n",
    "        \n",
    "        # Save ground truth mask\n",
    "        gt_img = Image.fromarray((gt_mask * 255).astype(np.uint8), mode='L')\n",
    "        gt_img.save(output_dir / f\"{image_name}_gt.png\")\n",
    "        \n",
    "        # Save prediction mask\n",
    "        pred_img = Image.fromarray((pred_binary * 255).astype(np.uint8), mode='L')\n",
    "        pred_img.save(output_dir / f\"{image_name}_pred.png\")\n",
    "        \n",
    "        # Save overlay versions\n",
    "        # Ground truth overlay\n",
    "        overlay_gt = image_array.copy()\n",
    "        mask_indices = gt_mask > 0\n",
    "        for c, color_val in enumerate(self.mask_color_gt):\n",
    "            overlay_gt[mask_indices, c] = (1 - self.overlay_alpha) * overlay_gt[mask_indices, c] + \\\n",
    "                                          self.overlay_alpha * (color_val / 255.0)\n",
    "        \n",
    "        overlay_gt_img = Image.fromarray((overlay_gt * 255).astype(np.uint8))\n",
    "        overlay_gt_img.save(output_dir / f\"{image_name}_overlay_gt.png\")\n",
    "        \n",
    "        # Prediction overlay\n",
    "        overlay_pred = image_array.copy()\n",
    "        mask_indices = pred_binary > 0\n",
    "        mask_color = self.mask_color_ensemble if is_ensemble else self.mask_color_pred\n",
    "        for c, color_val in enumerate(mask_color):\n",
    "            overlay_pred[mask_indices, c] = (1 - self.overlay_alpha) * overlay_pred[mask_indices, c] + \\\n",
    "                                           self.overlay_alpha * (color_val / 255.0)\n",
    "        \n",
    "        overlay_pred_img = Image.fromarray((overlay_pred * 255).astype(np.uint8))\n",
    "        overlay_pred_img.save(output_dir / f\"{image_name}_overlay_pred.png\")\n",
    "    \n",
    "    def process_ensemble_models(self, model_infos, test_paths, output_base_dir, \n",
    "                               device_id=0, save_best_worst=True):\n",
    "        \"\"\"\n",
    "        Process test images using ensemble of models.\n",
    "        \n",
    "        Parameters:\n",
    "            model_infos: List of model info dictionaries\n",
    "            test_paths: Test dataset paths\n",
    "            output_base_dir: Output directory\n",
    "            device_id: GPU device ID\n",
    "            save_best_worst: Whether to save best/worst cases\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results\n",
    "        \"\"\"\n",
    "        ensemble_name = f\"ensemble_{len(model_infos)}_models\"\n",
    "        print(f\"\\nProcessing ensemble of {len(model_infos)} models\")\n",
    "        \n",
    "        # Create output directory\n",
    "        model_output_dir = Path(output_base_dir) / ensemble_name\n",
    "        model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Process all images with ensemble\n",
    "        all_ious = []\n",
    "        image_metrics = []\n",
    "        \n",
    "        print(\"Generating ensemble predictions...\")\n",
    "        predictions = self.ensemble_predict_batch(model_infos, test_paths['images'], device_id, batch_size=8)\n",
    "        \n",
    "        print(\"Saving visualizations...\")\n",
    "        for idx, (image_path, mask_path, pred_data) in enumerate(\n",
    "            tqdm(zip(test_paths['images'], test_paths['masks'], predictions), \n",
    "                 total=len(test_paths['images']), desc=\"Saving images\")):\n",
    "            \n",
    "            # Load ground truth\n",
    "            gt_mask = self.load_ground_truth(mask_path)\n",
    "            \n",
    "            # Calculate IoU\n",
    "            iou = self.calculate_iou(pred_data['pred_binary'], gt_mask)\n",
    "            all_ious.append(iou)\n",
    "            \n",
    "            # Save images\n",
    "            image_name = Path(image_path).stem\n",
    "            self.save_individual_images(\n",
    "                pred_data['image_array'], \n",
    "                gt_mask, \n",
    "                pred_data['pred_binary'],\n",
    "                model_output_dir / \"all_images\",\n",
    "                image_name,\n",
    "                is_ensemble=True\n",
    "            )\n",
    "            \n",
    "            # Track metrics\n",
    "            image_metrics.append({\n",
    "                'image_idx': idx,\n",
    "                'image_name': image_name,\n",
    "                'image_path': image_path,\n",
    "                'iou': iou\n",
    "            })\n",
    "        \n",
    "        # Save best and worst cases\n",
    "        if save_best_worst and len(image_metrics) > 10:\n",
    "            # Sort by IoU\n",
    "            sorted_metrics = sorted(image_metrics, key=lambda x: x['iou'])\n",
    "            \n",
    "            # Get worst 5 and best 5\n",
    "            worst_5 = sorted_metrics[:5]\n",
    "            best_5 = sorted_metrics[-5:]\n",
    "            \n",
    "            print(\"\\nSaving best and worst cases...\")\n",
    "            \n",
    "            # Save worst cases\n",
    "            worst_dir = model_output_dir / \"worst_cases\"\n",
    "            for rank, metric in enumerate(worst_5):\n",
    "                idx = metric['image_idx']\n",
    "                pred_data = predictions[idx]\n",
    "                gt_mask = self.load_ground_truth(test_paths['masks'][idx])\n",
    "                \n",
    "                self.save_individual_images(\n",
    "                    pred_data['image_array'],\n",
    "                    gt_mask,\n",
    "                    pred_data['pred_binary'],\n",
    "                    worst_dir,\n",
    "                    f\"worst_{rank+1}_iou{metric['iou']:.3f}_{metric['image_name']}\",\n",
    "                    is_ensemble=True\n",
    "                )\n",
    "            \n",
    "            # Save best cases\n",
    "            best_dir = model_output_dir / \"best_cases\"\n",
    "            for rank, metric in enumerate(best_5):\n",
    "                idx = metric['image_idx']\n",
    "                pred_data = predictions[idx]\n",
    "                gt_mask = self.load_ground_truth(test_paths['masks'][idx])\n",
    "                \n",
    "                self.save_individual_images(\n",
    "                    pred_data['image_array'],\n",
    "                    gt_mask,\n",
    "                    pred_data['pred_binary'],\n",
    "                    best_dir,\n",
    "                    f\"best_{rank+1}_iou{metric['iou']:.3f}_{metric['image_name']}\",\n",
    "                    is_ensemble=True\n",
    "                )\n",
    "            \n",
    "            # Save summary JSON\n",
    "            summary = {\n",
    "                'model_name': ensemble_name,\n",
    "                'ensemble_models': [f\"{m['architecture']}_{m['backbone']}\" for m in model_infos],\n",
    "                'total_images': len(test_paths['images']),\n",
    "                'mean_iou': float(np.mean(all_ious)),\n",
    "                'std_iou': float(np.std(all_ious)),\n",
    "                'min_iou': float(np.min(all_ious)),\n",
    "                'max_iou': float(np.max(all_ious)),\n",
    "                'worst_5_cases': worst_5,\n",
    "                'best_5_cases': best_5\n",
    "            }\n",
    "            \n",
    "            with open(model_output_dir / 'summary.json', 'w') as f:\n",
    "                json.dump(summary, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"\\nEnsemble: {ensemble_name}\")\n",
    "            print(f\"Mean IoU: {summary['mean_iou']:.3f} ± {summary['std_iou']:.3f}\")\n",
    "            print(f\"IoU range: [{summary['min_iou']:.3f}, {summary['max_iou']:.3f}]\")\n",
    "            print(\"\\nWorst 5 cases:\")\n",
    "            for case in worst_5:\n",
    "                print(f\"  - {case['image_name']}: IoU = {case['iou']:.3f}\")\n",
    "            print(\"\\nBest 5 cases:\")\n",
    "            for case in best_5:\n",
    "                print(f\"  - {case['image_name']}: IoU = {case['iou']:.3f}\")\n",
    "        \n",
    "        # Save detailed metrics CSV\n",
    "        metrics_df = pd.DataFrame(image_metrics)\n",
    "        metrics_df.to_csv(model_output_dir / 'image_metrics.csv', index=False)\n",
    "        \n",
    "        return {\n",
    "            'model_name': ensemble_name,\n",
    "            'output_dir': str(model_output_dir),\n",
    "            'mean_iou': np.mean(all_ious),\n",
    "            'image_metrics': image_metrics\n",
    "        }\n",
    "    \n",
    "    def process_all_images_for_model(self, model_info, test_paths, output_base_dir, \n",
    "                                   device_id=3, save_best_worst=True):\n",
    "        \"\"\"\n",
    "        Process all test images for a single model.\n",
    "        \n",
    "        Parameters:\n",
    "            model_info: Model configuration dictionary\n",
    "            test_paths: Test dataset paths\n",
    "            output_base_dir: Output directory\n",
    "            device_id: GPU device ID\n",
    "            save_best_worst: Whether to save best/worst cases\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results\n",
    "        \"\"\"\n",
    "        model_name = f\"{model_info['architecture']}_{model_info['backbone']}\"\n",
    "        print(f\"\\nProcessing model: {model_name}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        model_output_dir = Path(output_base_dir) / model_name\n",
    "        model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Load model\n",
    "        model, device = self.load_model_for_inference(model_info, device_id)\n",
    "        if model is None:\n",
    "            print(f\"Failed to load model: {model_name}\")\n",
    "            return None\n",
    "        \n",
    "        # Process all images\n",
    "        all_ious = []\n",
    "        image_metrics = []\n",
    "        \n",
    "        print(\"Generating predictions...\")\n",
    "        predictions = self.predict_batch(model, test_paths['images'], device, batch_size=8)\n",
    "        \n",
    "        print(\"Saving visualizations...\")\n",
    "        for idx, (image_path, mask_path, pred_data) in enumerate(\n",
    "            tqdm(zip(test_paths['images'], test_paths['masks'], predictions), \n",
    "                 total=len(test_paths['images']), desc=\"Saving images\")):\n",
    "            \n",
    "            # Load ground truth\n",
    "            gt_mask = self.load_ground_truth(mask_path)\n",
    "            \n",
    "            # Calculate IoU\n",
    "            iou = self.calculate_iou(pred_data['pred_binary'], gt_mask)\n",
    "            all_ious.append(iou)\n",
    "            \n",
    "            # Save images\n",
    "            image_name = Path(image_path).stem\n",
    "            self.save_individual_images(\n",
    "                pred_data['image_array'], \n",
    "                gt_mask, \n",
    "                pred_data['pred_binary'],\n",
    "                model_output_dir / \"all_images\",\n",
    "                image_name\n",
    "            )\n",
    "            \n",
    "            # Track metrics\n",
    "            image_metrics.append({\n",
    "                'image_idx': idx,\n",
    "                'image_name': image_name,\n",
    "                'image_path': image_path,\n",
    "                'iou': iou\n",
    "            })\n",
    "        \n",
    "        # Save best and worst cases (same logic as ensemble)\n",
    "        if save_best_worst and len(image_metrics) > 10:\n",
    "            # Sort by IoU\n",
    "            sorted_metrics = sorted(image_metrics, key=lambda x: x['iou'])\n",
    "            \n",
    "            # Get worst 5 and best 5\n",
    "            worst_5 = sorted_metrics[:5]\n",
    "            best_5 = sorted_metrics[-5:]\n",
    "            \n",
    "            print(\"\\nSaving best and worst cases...\")\n",
    "            \n",
    "            # Save worst cases\n",
    "            worst_dir = model_output_dir / \"worst_cases\"\n",
    "            for rank, metric in enumerate(worst_5):\n",
    "                idx = metric['image_idx']\n",
    "                pred_data = predictions[idx]\n",
    "                gt_mask = self.load_ground_truth(test_paths['masks'][idx])\n",
    "                \n",
    "                self.save_individual_images(\n",
    "                    pred_data['image_array'],\n",
    "                    gt_mask,\n",
    "                    pred_data['pred_binary'],\n",
    "                    worst_dir,\n",
    "                    f\"worst_{rank+1}_iou{metric['iou']:.3f}_{metric['image_name']}\"\n",
    "                )\n",
    "            \n",
    "            # Save best cases\n",
    "            best_dir = model_output_dir / \"best_cases\"\n",
    "            for rank, metric in enumerate(best_5):\n",
    "                idx = metric['image_idx']\n",
    "                pred_data = predictions[idx]\n",
    "                gt_mask = self.load_ground_truth(test_paths['masks'][idx])\n",
    "                \n",
    "                self.save_individual_images(\n",
    "                    pred_data['image_array'],\n",
    "                    gt_mask,\n",
    "                    pred_data['pred_binary'],\n",
    "                    best_dir,\n",
    "                    f\"best_{rank+1}_iou{metric['iou']:.3f}_{metric['image_name']}\"\n",
    "                )\n",
    "            \n",
    "            # Save summary JSON\n",
    "            summary = {\n",
    "                'model_name': model_name,\n",
    "                'model_info': model_info,\n",
    "                'total_images': len(test_paths['images']),\n",
    "                'mean_iou': float(np.mean(all_ious)),\n",
    "                'std_iou': float(np.std(all_ious)),\n",
    "                'min_iou': float(np.min(all_ious)),\n",
    "                'max_iou': float(np.max(all_ious)),\n",
    "                'worst_5_cases': worst_5,\n",
    "                'best_5_cases': best_5\n",
    "            }\n",
    "            \n",
    "            with open(model_output_dir / 'summary.json', 'w') as f:\n",
    "                json.dump(summary, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"\\nModel: {model_name}\")\n",
    "            print(f\"Mean IoU: {summary['mean_iou']:.3f} ± {summary['std_iou']:.3f}\")\n",
    "            print(f\"IoU range: [{summary['min_iou']:.3f}, {summary['max_iou']:.3f}]\")\n",
    "            print(\"\\nWorst 5 cases:\")\n",
    "            for case in worst_5:\n",
    "                print(f\"  - {case['image_name']}: IoU = {case['iou']:.3f}\")\n",
    "            print(\"\\nBest 5 cases:\")\n",
    "            for case in best_5:\n",
    "                print(f\"  - {case['image_name']}: IoU = {case['iou']:.3f}\")\n",
    "        \n",
    "        # Save detailed metrics CSV\n",
    "        metrics_df = pd.DataFrame(image_metrics)\n",
    "        metrics_df.to_csv(model_output_dir / 'image_metrics.csv', index=False)\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'output_dir': str(model_output_dir),\n",
    "            'mean_iou': np.mean(all_ious),\n",
    "            'image_metrics': image_metrics\n",
    "        }\n",
    "    \n",
    "    def inference_on_unseen_images(self, model_infos, image_paths, output_dir, \n",
    "                                ensemble_name=\"ensemble\", device_id=0, batch_size=8):\n",
    "        \"\"\"\n",
    "        Run inference on unseen images using ensemble models.\n",
    "        \n",
    "        Parameters:\n",
    "            model_infos: List of model info dictionaries\n",
    "            image_paths: List of paths to images for inference\n",
    "            output_dir: Directory to save results\n",
    "            ensemble_name: Name for this ensemble run\n",
    "            device_id: GPU device ID\n",
    "            batch_size: Batch size for inference\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with paths to generated outputs\n",
    "        \"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        run_dir = output_dir / f\"{ensemble_name}_{timestamp}\"\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nRunning inference on {len(image_paths)} unseen images\")\n",
    "        print(f\"Output directory: {run_dir}\")\n",
    "        \n",
    "        # Get ensemble predictions\n",
    "        predictions = self.ensemble_predict_batch(\n",
    "            model_infos, \n",
    "            image_paths, \n",
    "            device_id=device_id, \n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Process each image\n",
    "        results = []\n",
    "        print(\"\\nSaving predictions and visualizations...\")\n",
    "        \n",
    "        for idx, (image_path, pred_data) in enumerate(\n",
    "            tqdm(zip(image_paths, predictions), total=len(image_paths), desc=\"Processing images\")):\n",
    "            \n",
    "            image_name = Path(image_path).stem\n",
    "            image_dir = run_dir / image_name\n",
    "            image_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Save original image\n",
    "            original_img = Image.fromarray((pred_data['image_array'] * 255).astype(np.uint8))\n",
    "            original_path = image_dir / f\"{image_name}_original.png\"\n",
    "            original_img.save(original_path)\n",
    "            \n",
    "            # Save binary prediction mask\n",
    "            binary_mask = pred_data['pred_binary']\n",
    "            mask_img = Image.fromarray((binary_mask * 255).astype(np.uint8), mode='L')\n",
    "            mask_path = image_dir / f\"{image_name}_mask.png\"\n",
    "            mask_img.save(mask_path)\n",
    "            \n",
    "            # Save prediction overlay\n",
    "            overlay = self.create_prediction_overlay(\n",
    "                pred_data['image_array'], \n",
    "                binary_mask,\n",
    "                color=[0, 255, 0]  # Green for ensemble\n",
    "            )\n",
    "            overlay_img = Image.fromarray((overlay * 255).astype(np.uint8))\n",
    "            overlay_path = image_dir / f\"{image_name}_overlay.png\"\n",
    "            overlay_img.save(overlay_path)\n",
    "            \n",
    "            # Save confidence heatmap\n",
    "            confidence = pred_data['pred_mask']\n",
    "            heatmap_path = image_dir / f\"{image_name}_confidence.png\"\n",
    "            self.save_confidence_heatmap(confidence, heatmap_path)\n",
    "            \n",
    "            # Save confidence overlay\n",
    "            confidence_overlay = self.create_confidence_overlay(\n",
    "                pred_data['image_array'],\n",
    "                confidence\n",
    "            )\n",
    "            conf_overlay_img = Image.fromarray((confidence_overlay * 255).astype(np.uint8))\n",
    "            conf_overlay_path = image_dir / f\"{image_name}_confidence_overlay.png\"\n",
    "            conf_overlay_img.save(conf_overlay_path)\n",
    "            \n",
    "            # Store result info\n",
    "            result = {\n",
    "                'image_name': image_name,\n",
    "                'original_path': str(image_path),\n",
    "                'outputs': {\n",
    "                    'original': str(original_path),\n",
    "                    'mask': str(mask_path),\n",
    "                    'overlay': str(overlay_path),\n",
    "                    'confidence': str(heatmap_path),\n",
    "                    'confidence_overlay': str(conf_overlay_path)\n",
    "                },\n",
    "                'confidence_stats': {\n",
    "                    'mean': float(confidence.mean()),\n",
    "                    'std': float(confidence.std()),\n",
    "                    'min': float(confidence.min()),\n",
    "                    'max': float(confidence.max()),\n",
    "                    'pixels_above_50': int((confidence > 0.5).sum()),\n",
    "                    'pixels_above_90': int((confidence > 0.9).sum())\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        # Create summary JSON\n",
    "        summary = {\n",
    "            'timestamp': timestamp,\n",
    "            'ensemble_name': ensemble_name,\n",
    "            'num_models': len(model_infos),\n",
    "            'num_images': len(image_paths),\n",
    "            'output_directory': str(run_dir),\n",
    "            'model_architectures': [f\"{m['architecture']}_{m['backbone']}\" for m in model_infos],\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        with open(run_dir / 'inference_summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # Create index HTML for easy viewing\n",
    "        self.create_html_index(results, run_dir)\n",
    "        \n",
    "        print(\"\\nInference complete!\")\n",
    "        print(f\"Results saved to: {run_dir}\")\n",
    "        print(f\"View results: {run_dir / 'index.html'}\")\n",
    "        \n",
    "        return summary\n",
    "\n",
    "    def create_prediction_overlay(self, image_array, binary_mask, color=[0, 255, 0], alpha=0.5):\n",
    "        \"\"\"Create overlay of prediction on original image.\"\"\"\n",
    "        overlay = image_array.copy()\n",
    "        mask_indices = binary_mask > 0\n",
    "        \n",
    "        for c, color_val in enumerate(color):\n",
    "            overlay[mask_indices, c] = (1 - alpha) * overlay[mask_indices, c] + \\\n",
    "                                    alpha * (color_val / 255.0)\n",
    "        \n",
    "        return overlay\n",
    "\n",
    "    def save_confidence_heatmap(self, confidence_map, save_path):\n",
    "        \"\"\"Save confidence/probability map as heatmap.\"\"\"\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(confidence_map, cmap='hot', vmin=0, vmax=1)\n",
    "        plt.colorbar(label='Confidence')\n",
    "        plt.title('Prediction Confidence Heatmap')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def create_confidence_overlay(self, image_array, confidence_map, alpha=0.6):\n",
    "        \"\"\"Overlay confidence heatmap on original image.\"\"\"\n",
    "        import matplotlib.cm as cm\n",
    "        \n",
    "        # Convert confidence to RGB using colormap\n",
    "        colormap = cm.get_cmap('hot')\n",
    "        confidence_rgb = colormap(confidence_map)[:, :, :3]\n",
    "        \n",
    "        # Blend with original image\n",
    "        overlay = (1 - alpha) * image_array + alpha * confidence_rgb\n",
    "        overlay = np.clip(overlay, 0, 1)\n",
    "        \n",
    "        return overlay\n",
    "\n",
    "    def create_html_index(self, results, output_dir):\n",
    "        \"\"\"Create HTML index page for viewing results.\"\"\"\n",
    "        html_content = \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Ensemble Inference Results</title>\n",
    "            <style>\n",
    "                body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }\n",
    "                .container { max-width: 1400px; margin: 0 auto; }\n",
    "                .image-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; }\n",
    "                .image-card { background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }\n",
    "                .image-name { font-weight: bold; margin-bottom: 10px; color: #333; }\n",
    "                .thumbnails { display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px; margin-bottom: 10px; }\n",
    "                .thumbnail { width: 100%; cursor: pointer; border: 2px solid transparent; transition: border-color 0.3s; }\n",
    "                .thumbnail:hover { border-color: #007bff; }\n",
    "                .stats { font-size: 12px; color: #666; background: #f8f8f8; padding: 10px; border-radius: 4px; }\n",
    "                .stat-row { margin: 2px 0; }\n",
    "                h1 { color: #333; text-align: center; }\n",
    "                .summary { background: white; padding: 20px; border-radius: 8px; margin-bottom: 30px; }\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <h1>Ensemble Inference Results</h1>\n",
    "                <div class=\"summary\">\n",
    "                    <h3>Summary</h3>\n",
    "                    <p><strong>Total Images:</strong> \"\"\" + str(len(results)) + \"\"\"</p>\n",
    "                    <p><strong>Timestamp:</strong> \"\"\" + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + \"\"\"</p>\n",
    "                </div>\n",
    "                <div class=\"image-grid\">\n",
    "        \"\"\"\n",
    "        \n",
    "        for result in results:\n",
    "            name = result['image_name']\n",
    "            stats = result['confidence_stats']\n",
    "            \n",
    "            # Create relative paths for HTML\n",
    "            rel_dir = f\"{name}/\"\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "                <div class=\"image-card\">\n",
    "                    <div class=\"image-name\">{name}</div>\n",
    "                    <div class=\"thumbnails\">\n",
    "                        <img class=\"thumbnail\" src=\"{rel_dir}{name}_original.png\" alt=\"Original\" title=\"Original\">\n",
    "                        <img class=\"thumbnail\" src=\"{rel_dir}{name}_overlay.png\" alt=\"Prediction Overlay\" title=\"Prediction Overlay\">\n",
    "                        <img class=\"thumbnail\" src=\"{rel_dir}{name}_confidence.png\" alt=\"Confidence Heatmap\" title=\"Confidence Heatmap\">\n",
    "                        <img class=\"thumbnail\" src=\"{rel_dir}{name}_confidence_overlay.png\" alt=\"Confidence Overlay\" title=\"Confidence Overlay\">\n",
    "                    </div>\n",
    "                    <div class=\"stats\">\n",
    "                        <div class=\"stat-row\"><strong>Confidence Stats:</strong></div>\n",
    "                        <div class=\"stat-row\">Mean: {stats['mean']:.3f} ± {stats['std']:.3f}</div>\n",
    "                        <div class=\"stat-row\">Range: [{stats['min']:.3f}, {stats['max']:.3f}]</div>\n",
    "                        <div class=\"stat-row\">Pixels &gt;50%: {stats['pixels_above_50']:,}</div>\n",
    "                        <div class=\"stat-row\">Pixels &gt;90%: {stats['pixels_above_90']:,}</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "                </div>\n",
    "            </div>\n",
    "            <script>\n",
    "                // Make images clickable to view full size\n",
    "                document.querySelectorAll('.thumbnail').forEach(img => {\n",
    "                    img.addEventListener('click', () => {\n",
    "                        window.open(img.src, '_blank');\n",
    "                    });\n",
    "                });\n",
    "            </script>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(output_dir / 'index.html', 'w') as f:\n",
    "            f.write(html_content)\n",
    "\n",
    "# Convenience function for running inference\n",
    "def run_inference_on_new_images(viz_system, df_model_results, unseen_image_paths, output_dir):\n",
    "    \"\"\"\n",
    "    Run inference using pre-defined k-fold ensembles on new images.\n",
    "    \n",
    "    Parameters:\n",
    "        viz_system: LaTeXVisualizationSystem instance\n",
    "        df_model_results: DataFrame with model information\n",
    "        unseen_image_paths: List of paths to new images\n",
    "        output_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the same models as in k-fold ensemble\n",
    "    models_to_use = [\n",
    "        ('unet', 'tu-mambaout_small'),\n",
    "        ('upernet', 'tu-efficientnetv2_rw_s.ra2_in1k'),\n",
    "        ('segformer', 'tu-mambaout_base'),\n",
    "        ('linknet', 'timm-efficientnet-b5'),\n",
    "        ('unetplusplus', 'tu-efficientnetv2_rw_s.ra2_in1k'),\n",
    "        ('segformer', 'tu-regnety_080.ra3_in1k'),\n",
    "    ]\n",
    "    \n",
    "    # Process each model ensemble\n",
    "    for arch, backbone in models_to_use:\n",
    "        model_name = f\"{arch}_{backbone}\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing ensemble: {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get all k-fold models for this architecture-backbone combination\n",
    "        model_infos = []\n",
    "        for fold in range(5):\n",
    "            mask = (df_model_results['architecture'] == arch) & \\\n",
    "                   (df_model_results['backbone'] == backbone) & \\\n",
    "                   (df_model_results['validation_fold'] == fold)\n",
    "            \n",
    "            fold_models = df_model_results[mask]\n",
    "            if not fold_models.empty:\n",
    "                model_infos.append(fold_models.iloc[0].to_dict())\n",
    "        \n",
    "        if len(model_infos) == 0:\n",
    "            print(f\"No models found for {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Found {len(model_infos)} fold models\")\n",
    "        \n",
    "        # Run inference\n",
    "        viz_system.inference_on_unseen_images(\n",
    "            model_infos,\n",
    "            unseen_image_paths,\n",
    "            output_dir,\n",
    "            ensemble_name=model_name,\n",
    "            device_id=0\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUER_ENSEMBLE:\n",
    "    if not MAJ_DATASET:\n",
    "        logger.info(\"Loading existing results...\")\n",
    "        df_model_results = pd.read_parquet(SAVED_MODEL_RESULTS_PARQUET_PATH)\n",
    "        df_per_image_results = pd.read_parquet(SAVED_PER_IMAGE_RESULTS_PARQUET_PATH)\n",
    "        \n",
    "        # Remove duplicates, keep highest eval_test_iou\n",
    "        df_model_results = df_model_results.sort_values('eval_test_iou', ascending=False).drop_duplicates(\n",
    "            subset=['architecture', 'backbone', 'validation_fold'], keep='first'\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # Load test paths\n",
    "        test_paths = load_test_data(TEST_DATASET_FILE, DATASET_IMAGES_DIR, DATASET_MASKS_DIR)\n",
    "\n",
    "    # Initialize visualization system\n",
    "    viz_system = LaTeXVisualizationSystem(img_size=IMG_SIZE)\n",
    "\n",
    "    # Simple function to get model indices by name and fold\n",
    "    def get_model_indices(df, architecture, backbone, fold=None):\n",
    "        \"\"\"Get model indices by architecture, backbone, and optionally fold.\"\"\"\n",
    "        mask = (df['architecture'] == architecture) & (df['backbone'] == backbone)\n",
    "        if fold is not None:\n",
    "            mask = mask & (df['validation_fold'] == fold)\n",
    "        return df[mask].index.tolist()\n",
    "\n",
    "    # Define the models to ensemble\n",
    "    models_to_ensemble = [\n",
    "        ('unet', 'tu-mambaout_small'),                          # top1 mAP@0.95\n",
    "        ('upernet', 'tu-efficientnetv2_rw_s.ra2_in1k'),        # top2 mAP@0.95\n",
    "        ('segformer', 'tu-mambaout_base'),                     # top1 mAP@0.75\n",
    "        ('linknet', 'timm-efficientnet-b5'),                   # top1 mAP@0.5\n",
    "        ('unetplusplus', 'tu-efficientnetv2_rw_s.ra2_in1k'),   # top2 mAP@0.5\n",
    "        ('segformer', 'tu-regnety_080.ra3_in1k'),              # top2 IoU\n",
    "    ]\n",
    "\n",
    "    # Dictionary to store ensemble results\n",
    "    ensemble_results = {}\n",
    "    individual_fold_results = {}\n",
    "\n",
    "    # Process each model separately\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Creating K-Fold Ensembles for Each Model\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for arch, backbone in models_to_ensemble:\n",
    "        model_name = f\"{arch}_{backbone}\"\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Processing: {model_name}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Get all folds for this model\n",
    "        fold_indices = []\n",
    "        fold_performances = []\n",
    "        \n",
    "        for fold in range(5):  # 5-fold CV\n",
    "            indices = get_model_indices(df_model_results, arch, backbone, fold)\n",
    "            if indices:\n",
    "                fold_indices.extend(indices)\n",
    "                # Get individual fold performance\n",
    "                fold_data = df_model_results.iloc[indices[0]]\n",
    "                fold_performances.append({\n",
    "                    'fold': fold,\n",
    "                    'iou': fold_data['eval_test_iou'],\n",
    "                    'f1': fold_data['eval_test_f1_score'],\n",
    "                    'map50': fold_data['eval_test_map_50']\n",
    "                })\n",
    "                print(f\"  Fold {fold}: IoU = {fold_data['eval_test_iou']:.4f}\")\n",
    "        \n",
    "        if len(fold_indices) == 0:\n",
    "            print(f\"  No models found for {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Store individual fold results\n",
    "        individual_fold_results[model_name] = fold_performances\n",
    "        \n",
    "        # Calculate statistics\n",
    "        ious = [f['iou'] for f in fold_performances]\n",
    "        mean_iou = np.mean(ious)\n",
    "        std_iou = np.std(ious)\n",
    "        \n",
    "        print(\"\\n  Individual Fold Statistics:\")\n",
    "        print(f\"     Mean IoU: {mean_iou:.4f} ± {std_iou:.4f}\")\n",
    "        print(f\"     Best fold: {max(ious):.4f}\")\n",
    "        print(f\"     Worst fold: {min(ious):.4f}\")\n",
    "        \n",
    "        # Create ensemble for this model across all folds\n",
    "        print(f\"\\n  Creating ensemble with {len(fold_indices)} folds...\")\n",
    "\n",
    "        # Get model infos for ensemble\n",
    "        model_infos = [df_model_results.iloc[idx].to_dict() for idx in fold_indices]\n",
    "\n",
    "        # Create unique folder for each model\n",
    "        model_specific_dir = VIZ_OUTPUT_DIR / \"kfold_ensembles\" / model_name\n",
    "\n",
    "        # Use process_ensemble_models with model-specific directory\n",
    "        ensemble_result = viz_system.process_ensemble_models(\n",
    "            model_infos,\n",
    "            test_paths,\n",
    "            model_specific_dir,\n",
    "            device_id=0\n",
    "        )\n",
    "        \n",
    "        # Extract results\n",
    "        if ensemble_result:\n",
    "            # Get the actual output path\n",
    "            actual_output = model_specific_dir / 'ensemble_5_models'\n",
    "            \n",
    "            # Get ensemble IoU directly from result\n",
    "            ensemble_iou = ensemble_result.get('mean_iou', None)\n",
    "            \n",
    "            if ensemble_iou:\n",
    "                improvement = ((ensemble_iou - mean_iou) / mean_iou) * 100\n",
    "                improvement_best = ((ensemble_iou - max(ious)) / max(ious)) * 100\n",
    "                \n",
    "                print(\"\\n  Ensemble Results:\")\n",
    "                print(f\"     Ensemble IoU: {ensemble_iou:.4f}\")\n",
    "                print(f\"     Improvement over mean: {improvement:.2f}%\")\n",
    "                print(f\"     Improvement over best fold: {improvement_best:.2f}%\")\n",
    "            else:\n",
    "                ensemble_iou = None\n",
    "                improvement = None\n",
    "                improvement_best = None\n",
    "        else:\n",
    "            actual_output = None\n",
    "            ensemble_iou = None\n",
    "            improvement = None\n",
    "            improvement_best = None\n",
    "        \n",
    "        # Store ensemble result\n",
    "        ensemble_results[model_name] = {\n",
    "            'output_path': str(actual_output) if actual_output else None,\n",
    "            'fold_indices': fold_indices,\n",
    "            'num_folds': len(fold_indices),\n",
    "            'individual_fold_performances': fold_performances,\n",
    "            'fold_mean_iou': mean_iou,\n",
    "            'fold_std_iou': std_iou,\n",
    "            'ensemble_result': ensemble_result,\n",
    "            'ensemble_iou': ensemble_iou,\n",
    "            'improvement_percent': improvement,\n",
    "            'improvement_best_percent': improvement_best\n",
    "        }\n",
    "\n",
    "    # Create Summary Report\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Creating K-Fold Ensemble Report\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Summary statistics\n",
    "    successful_ensembles = [r for r in ensemble_results.values() if r.get('ensemble_iou') is not None]\n",
    "    all_improvements = [r['improvement_percent'] for r in successful_ensembles if r.get('improvement_percent') is not None]\n",
    "\n",
    "    print(\"\\nSUMMARY STATISTICS:\")\n",
    "    print(f\"Models processed: {len(ensemble_results)}\")\n",
    "    print(f\"Successful ensembles: {len(successful_ensembles)}\")\n",
    "\n",
    "    if all_improvements:\n",
    "        print(\"\\nImprovement over mean fold:\")\n",
    "        print(f\"  - Average: {np.mean(all_improvements):.2f}%\")\n",
    "        print(f\"  - Best: {max(all_improvements):.2f}%\")\n",
    "        print(f\"  - Worst: {min(all_improvements):.2f}%\")\n",
    "\n",
    "    # Detailed results\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"DETAILED RESULTS BY MODEL:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for model_name, result in ensemble_results.items():\n",
    "        if result.get('ensemble_iou') is not None:\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"  Folds: {result['num_folds']}\")\n",
    "            print(f\"  Mean fold IoU: {result['fold_mean_iou']:.4f} ± {result['fold_std_iou']:.4f}\")\n",
    "            print(f\"  Best fold IoU: {max([f['iou'] for f in result['individual_fold_performances']]):.4f}\")\n",
    "            print(f\"  Ensemble IoU: {result['ensemble_iou']:.4f}\")\n",
    "            print(f\"  Improvement: +{result['improvement_percent']:.2f}% (vs mean), {result['improvement_best_percent']:+.2f}% (vs best)\")\n",
    "            print(f\"  Output: {result['output_path']}\")\n",
    "\n",
    "    # Create Detailed Report\n",
    "    report_content = f\"\"\"# K-Fold Ensemble Analysis Report\n",
    "    Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "    ## Executive Summary\n",
    "    This report analyzes the performance improvement achieved by ensembling models across their k-fold variants.\n",
    "\n",
    "    ## Dataset Information\n",
    "    - Total test images: {len(test_paths['images'])}\n",
    "    - Number of folds: 5\n",
    "    - Models evaluated: {len(models_to_ensemble)}\n",
    "\n",
    "    ## Detailed Results by Model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for model_name, result in ensemble_results.items():\n",
    "        if result.get('ensemble_iou') is not None:\n",
    "            report_content += f\"### {model_name}\\n\"\n",
    "            report_content += f\"- **Number of folds ensembled**: {result['num_folds']}\\n\"\n",
    "            report_content += f\"- **Individual fold IoUs**: {[f'{f:.4f}' for f in [fold['iou'] for fold in result['individual_fold_performances']]]}\\n\"\n",
    "            report_content += f\"- **Mean fold IoU**: {result['fold_mean_iou']:.4f} ± {result['fold_std_iou']:.4f}\\n\"\n",
    "            report_content += f\"- **Best single fold**: {max([fold['iou'] for fold in result['individual_fold_performances']]):.4f}\\n\"\n",
    "            report_content += f\"- **Ensemble IoU**: {result['ensemble_iou']:.4f}\\n\"\n",
    "            report_content += f\"- **Improvement over mean**: {result['improvement_percent']:.2f}%\\n\"\n",
    "            report_content += f\"- **Output directory**: `{result.get('output_path', 'Not generated')}`\\n\\n\"\n",
    "\n",
    "    # Summary statistics\n",
    "    if all_improvements:\n",
    "        report_content += f\"\"\"## Overall Statistics\n",
    "\n",
    "    - **Average improvement**: {np.mean(all_improvements):.2f}%\n",
    "    - **Best improvement**: {max(all_improvements):.2f}%\n",
    "    - **Worst improvement**: {min(all_improvements):.2f}%\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with open(VIZ_OUTPUT_DIR / 'KFOLD_ENSEMBLE_REPORT.md', 'w') as f:\n",
    "        f.write(report_content)\n",
    "\n",
    "    print(f\"\\nReport saved to: {VIZ_OUTPUT_DIR / 'KFOLD_ENSEMBLE_REPORT.md'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUER_ENSEMBLE:\n",
    "    # Create LaTeX Table\n",
    "    latex_table = \"\"\"\\\\begin{table}[h]\n",
    "    \\\\centering\n",
    "    \\\\caption{K-Fold Ensemble Performance Comparison}\n",
    "    \\\\label{tab:kfold_ensemble}\n",
    "    \\\\begin{tabular}{|l|c|c|c|c|c|}\n",
    "    \\\\hline\n",
    "    \\\\textbf{Model} & \\\\textbf{Folds} & \\\\textbf{Mean Fold IoU} & \\\\textbf{Best Fold} & \\\\textbf{Ensemble IoU} & \\\\textbf{Improvement} \\\\\\\\\n",
    "    \\\\hline\n",
    "    \"\"\"\n",
    "\n",
    "    for model_name, result in ensemble_results.items():\n",
    "        if result.get('ensemble_iou') is not None:\n",
    "            model_name_tex = model_name.replace('_', '\\\\_')[:35] + '...' if len(model_name) > 35 else model_name.replace('_', '\\\\_')\n",
    "            fold_ious = [fold['iou'] for fold in result['individual_fold_performances']]\n",
    "            \n",
    "            latex_table += f\"{model_name_tex} & {result['num_folds']} & \"\n",
    "            latex_table += f\"{result['fold_mean_iou']:.3f} ± {result['fold_std_iou']:.3f} & \"\n",
    "            latex_table += f\"{max(fold_ious):.3f} & \"\n",
    "            latex_table += f\"\\\\textbf{{{result['ensemble_iou']:.3f}}} & \"\n",
    "            latex_table += f\"+{result['improvement_percent']:.1f}\\\\% \\\\\\\\\\n\"\n",
    "\n",
    "    latex_table += \"\"\"\\\\hline\n",
    "    \\\\end{tabular}\n",
    "    \\\\end{table}\"\"\"\n",
    "\n",
    "    with open(VIZ_OUTPUT_DIR / 'kfold_ensemble_table.tex', 'w') as f:\n",
    "        f.write(latex_table)\n",
    "\n",
    "    print(f\"LaTeX table saved to: {VIZ_OUTPUT_DIR / 'kfold_ensemble_table.tex'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUER_ENSEMBLE:\n",
    "    # Configuration for LaTeX figure generation\n",
    "    \n",
    "    def sanitize_label(text):\n",
    "        \"\"\"\n",
    "        Convert text to valid LaTeX label format.\n",
    "        \n",
    "        Removes spaces, underscores, and special characters to ensure\n",
    "        compatibility with LaTeX reference system.\n",
    "        \n",
    "        Parameters:\n",
    "            text (str): Input text to sanitize\n",
    "            \n",
    "        Returns:\n",
    "            str: LaTeX-compatible label string\n",
    "        \"\"\"\n",
    "        # Remove common prefixes and suffixes\n",
    "        text = text.replace('timm-', '').replace('tu-', '')\n",
    "        # Keep only alphanumeric characters\n",
    "        return re.sub(r'[^a-zA-Z0-9]', '', text).lower()\n",
    "\n",
    "    def escape_latex(text):\n",
    "        \"\"\"\n",
    "        Escape special LaTeX characters in text.\n",
    "        \n",
    "        Parameters:\n",
    "            text (str): Input text containing LaTeX special characters\n",
    "            \n",
    "        Returns:\n",
    "            str: Text with escaped special characters\n",
    "        \"\"\"\n",
    "        chars = {\n",
    "            '_': r'\\_',\n",
    "            '%': r'\\%',\n",
    "            '&': r'\\&',\n",
    "            '#': r'\\#',\n",
    "            '$': r'\\$',\n",
    "            '{': r'\\{',\n",
    "            '}': r'\\}',\n",
    "        }\n",
    "        for char, escape in chars.items():\n",
    "            text = text.replace(char, escape)\n",
    "        return text\n",
    "\n",
    "    def format_model_name(model_name):\n",
    "        \"\"\"\n",
    "        Format model name for improved display in LaTeX.\n",
    "        \n",
    "        Splits architecture and backbone components, applies proper\n",
    "        capitalization and formatting conventions.\n",
    "        \n",
    "        Parameters:\n",
    "            model_name (str): Combined model name (architecture_backbone)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (formatted_architecture, formatted_backbone)\n",
    "        \"\"\"\n",
    "        # Split architecture and backbone\n",
    "        parts = model_name.split('_', 1)\n",
    "        if len(parts) == 2:\n",
    "            arch, backbone = parts\n",
    "            # Clean up common prefixes\n",
    "            backbone = backbone.replace('timm-', '').replace('tu-', '')\n",
    "            # Capitalize architecture\n",
    "            arch = arch.capitalize()\n",
    "            if arch == 'Unetplusplus':\n",
    "                arch = 'UNet++'\n",
    "            elif arch == 'Upernet':\n",
    "                arch = 'UPerNet'\n",
    "            elif arch == 'Segformer':\n",
    "                arch = 'SegFormer'\n",
    "            elif arch == 'Linknet':\n",
    "                arch = 'LinkNet'\n",
    "            return arch, backbone\n",
    "        return model_name, ''\n",
    "\n",
    "    def get_image_files(directory, case_type='best', case_number=1):\n",
    "        \"\"\"\n",
    "        Retrieve all image files for a specific test case.\n",
    "        \n",
    "        Searches for original, ground truth, prediction, and overlay\n",
    "        images based on naming convention.\n",
    "        \n",
    "        Parameters:\n",
    "            directory (Path): Directory containing case images\n",
    "            case_type (str): Type of case ('best' or 'worst')\n",
    "            case_number (int): Case index number\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (files_dict, iou_score)\n",
    "                - files_dict: Dictionary mapping image types to file paths\n",
    "                - iou_score: IoU score extracted from filename\n",
    "        \"\"\"\n",
    "        files = {}\n",
    "        patterns = {\n",
    "            'original': f'{case_type}_{case_number}_iou*_original.png',\n",
    "            'gt': f'{case_type}_{case_number}_iou*_gt.png',\n",
    "            'pred': f'{case_type}_{case_number}_iou*_pred.png',\n",
    "            'overlay_gt': f'{case_type}_{case_number}_iou*_overlay_gt.png',\n",
    "            'overlay_pred': f'{case_type}_{case_number}_iou*_overlay_pred.png'\n",
    "        }\n",
    "        \n",
    "        for key, pattern in patterns.items():\n",
    "            found = list(directory.glob(pattern))\n",
    "            if found:\n",
    "                files[key] = found[0]\n",
    "        \n",
    "        # Extract IoU from filename\n",
    "        iou = None\n",
    "        if files:\n",
    "            first_file = list(files.values())[0]\n",
    "            match = re.search(r'iou([\\d.]+)', first_file.name)\n",
    "            if match:\n",
    "                iou = float(match.group(1))\n",
    "        \n",
    "        return files, iou\n",
    "\n",
    "    def load_summary_data(model_output_path):\n",
    "        \"\"\"\n",
    "        Load summary JSON file for additional model information.\n",
    "        \n",
    "        Parameters:\n",
    "            model_output_path (Path): Path to model output directory\n",
    "            \n",
    "        Returns:\n",
    "            dict: Summary data or None if not found\n",
    "        \"\"\"\n",
    "        summary_path = Path(model_output_path) / 'summary.json'\n",
    "        if summary_path.exists():\n",
    "            with open(summary_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "\n",
    "    # Initialize LaTeX document structure\n",
    "    latex_figures = r\"\"\"% LaTeX code for K-Fold Ensemble Visualization Figures\n",
    "% Add this to your preamble:\n",
    "% \\usepackage{graphicx}\n",
    "% \\usepackage{subcaption}\n",
    "% \\usepackage{float}\n",
    "\n",
    "% IMPORTANT: Set the graphics path to your kfold_ensembles directory\n",
    "% \\graphicspath{{./path/to/kfold_ensembles/}}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Track all models for comparison figure\n",
    "    all_models_best = []\n",
    "\n",
    "    # Process each model\n",
    "    for model_name, result in ensemble_results.items():\n",
    "        if result.get('output_path') is None:\n",
    "            continue\n",
    "        \n",
    "        # Extract path from stored result\n",
    "        model_dir = Path(result['output_path'])\n",
    "        \n",
    "        # Check if pointing to ensemble_5_models subdirectory\n",
    "        if model_dir.name == 'ensemble_5_models':\n",
    "            model_dir = model_dir.parent\n",
    "        \n",
    "        # Ensemble results location\n",
    "        ensemble_dir = model_dir / 'ensemble_5_models'\n",
    "        \n",
    "        if not ensemble_dir.exists():\n",
    "            print(f\"Warning: Ensemble directory not found for {model_name}: {ensemble_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Format model name for display\n",
    "        arch_display, backbone_display = format_model_name(model_name)\n",
    "        model_name_tex = escape_latex(model_name)\n",
    "        arch_tex = escape_latex(arch_display)\n",
    "        backbone_tex = escape_latex(backbone_display)\n",
    "        model_label = sanitize_label(model_name)\n",
    "        \n",
    "        # Load summary for additional info\n",
    "        summary = load_summary_data(ensemble_dir)\n",
    "        \n",
    "        # Get ensemble IoU from result\n",
    "        ensemble_iou = result.get('ensemble_iou', 0)\n",
    "        \n",
    "        latex_figures += f\"\"\"\n",
    "% Model: {arch_display} with {backbone_display}\n",
    "% Ensemble IoU: {ensemble_iou:.4f}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Process best cases\n",
    "        best_cases_dir = ensemble_dir / \"best_cases\"\n",
    "        if best_cases_dir.exists():\n",
    "            # Get best case 1\n",
    "            best_files, best_iou = get_image_files(best_cases_dir, 'best', 1)\n",
    "            \n",
    "            if best_files and all(k in best_files for k in ['original', 'gt', 'pred', 'overlay_pred']):\n",
    "                # Store for comparison figure\n",
    "                all_models_best.append({\n",
    "                    'model_name': model_name,\n",
    "                    'arch': arch_display,\n",
    "                    'backbone': backbone_display,\n",
    "                    'ensemble_iou': ensemble_iou,\n",
    "                    'best_iou': best_iou,\n",
    "                    'overlay_file': best_files['overlay_pred'].name,\n",
    "                    'model_dir': model_dir.name\n",
    "                })\n",
    "                \n",
    "                # Create figure for best case\n",
    "                latex_figures += f\"\"\"% Best performing segmentation for {arch_tex}\n",
    "\\\\begin{{figure}}[htbp]\n",
    "\\\\centering\n",
    "\\\\begin{{subfigure}}{{0.24\\\\textwidth}}\n",
    "    \\\\includegraphics[width=\\\\textwidth]{{{model_dir.name}/ensemble_5_models/best_cases/{best_files['original'].name}}}\n",
    "    \\\\caption{{Input Image}}\n",
    "\\\\end{{subfigure}}\n",
    "\\\\hfill\n",
    "\\\\begin{{subfigure}}{{0.24\\\\textwidth}}\n",
    "    \\\\includegraphics[width=\\\\textwidth]{{{model_dir.name}/ensemble_5_models/best_cases/{best_files['gt'].name}}}\n",
    "    \\\\caption{{Ground Truth}}\n",
    "\\\\end{{subfigure}}\n",
    "\\\\hfill\n",
    "\\\\begin{{subfigure}}{{0.24\\\\textwidth}}\n",
    "    \\\\includegraphics[width=\\\\textwidth]{{{model_dir.name}/ensemble_5_models/best_cases/{best_files['pred'].name}}}\n",
    "    \\\\caption{{Prediction}}\n",
    "\\\\end{{subfigure}}\n",
    "\\\\hfill\n",
    "\\\\begin{{subfigure}}{{0.24\\\\textwidth}}\n",
    "    \\\\includegraphics[width=\\\\textwidth]{{{model_dir.name}/ensemble_5_models/best_cases/{best_files['overlay_pred'].name}}}\n",
    "    \\\\caption{{Overlay}}\n",
    "\\\\end{{subfigure}}\n",
    "\\\\caption{{Best segmentation result for {arch_tex} with {backbone_tex} backbone. K-fold ensemble of 5 models achieves IoU = {best_iou:.3f} on this image (ensemble mean IoU = {ensemble_iou:.3f}).}}\n",
    "\\\\label{{fig:{model_label}best}}\n",
    "\\\\end{{figure}}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Process worst cases\n",
    "        worst_cases_dir = ensemble_dir / \"worst_cases\"\n",
    "        if worst_cases_dir.exists():\n",
    "            # Get worst case 1\n",
    "            worst_files, worst_iou = get_image_files(worst_cases_dir, 'worst', 1)\n",
    "            \n",
    "            if worst_files and all(k in worst_files for k in ['original', 'gt', 'pred', 'overlay_pred']):\n",
    "                latex_figures += f\"\"\"% Challenging case for {arch_tex}\n",
    "\\\\begin{{figure}}[htbp]\n",
    "\\\\centering\n",
    "\\\\begin{{subfigure}}{{0.24\\\\textwidth}}\n",
    "    \\\\includegraphics[width=\\\\textwidth]{{{model_dir.name}/ensemble_5_models/worst_cases/{worst_files['original'].name}}}\n",
    "    \\\\caption{{Input Image}}\n",
    "\\\\end{{subfigure}}\n",
    "\\\\hfill\n",
    "\\\\begin{{subfigure}}{{0.24\\\\textwidth}}\n",
    "    \\\\includegraphics[width=\\\\textwidth]{{{model_dir.name}/ensemble_5_models/worst_cases/{worst_files['gt'].name}}}\n",
    "    \\\\caption{{Ground Truth}}\n",
    "\\\\end{{subfigure}}\n",
    "\\\\hfill\n",
    "\\\\begin{{subfigure}}{{0.24\\\\textwidth}}\n",
    "    \\\\includegraphics[width=\\\\textwidth]{{{model_dir.name}/ensemble_5_models/worst_cases/{worst_files['pred'].name}}}\n",
    "    \\\\caption{{Prediction}}\n",
    "\\\\end{{subfigure}}\n",
    "\\\\hfill\n",
    "\\\\begin{{subfigure}}{{0.24\\\\textwidth}}\n",
    "    \\\\includegraphics[width=\\\\textwidth]{{{model_dir.name}/ensemble_5_models/worst_cases/{worst_files['overlay_pred'].name}}}\n",
    "    \\\\caption{{Overlay}}\n",
    "\\\\end{{subfigure}}\n",
    "\\\\caption{{Most challenging case for {arch_tex} with {backbone_tex} backbone. This example shows the limitations with IoU = {worst_iou:.3f}.}}\n",
    "\\\\label{{fig:{model_label}worst}}\n",
    "\\\\end{{figure}}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Create comparison figure with all models' best cases\n",
    "    if all_models_best:\n",
    "        # Sort by ensemble IoU descending\n",
    "        all_models_best.sort(key=lambda x: x['ensemble_iou'], reverse=True)\n",
    "        \n",
    "        latex_figures += \"\"\"\n",
    "% Comparison of Best Cases Across All K-Fold Ensembles\n",
    "\n",
    "\\\\begin{figure}[htbp]\n",
    "\\\\centering\n",
    "\"\"\"\n",
    "        \n",
    "        # Use top 6 models\n",
    "        models_to_show = all_models_best[:6]\n",
    "        \n",
    "        for i, model_data in enumerate(models_to_show):\n",
    "            # Add line break after every 3 subfigures\n",
    "            if i > 0 and i % 3 == 0:\n",
    "                latex_figures += \"\\n\\\\vspace{0.5em}\\n\"\n",
    "            \n",
    "            latex_figures += f\"\"\"\\\\begin{{subfigure}}{{0.32\\\\textwidth}}\n",
    "    \\\\includegraphics[width=\\\\textwidth]{{{model_data['model_dir']}/ensemble_5_models/best_cases/{model_data['overlay_file']}}}\n",
    "    \\\\caption{{{model_data['arch']} (IoU={model_data['ensemble_iou']:.3f})}}\n",
    "\\\\end{{subfigure}}\"\"\"\n",
    "            \n",
    "            # Add horizontal space between subfigures (except last in row)\n",
    "            if (i + 1) % 3 != 0 and i < len(models_to_show) - 1:\n",
    "                latex_figures += \"\\n\\\\hfill\"\n",
    "            elif (i + 1) % 3 == 0 and i < len(models_to_show) - 1:\n",
    "                latex_figures += \"\\n\"\n",
    "        \n",
    "        latex_figures += \"\"\"\n",
    "\\\\caption{Comparison of best segmentation results across different architectures using k-fold ensembles. Green overlays show the ensemble predictions. Models are ordered by mean ensemble IoU performance.}\n",
    "\\\\label{fig:kfoldcomparison}\n",
    "\\\\end{figure}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add section for individual model comparisons\n",
    "    latex_figures += \"\"\"\n",
    "% Individual Model Performance Across Multiple Cases\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Select best performing model to show multiple cases\n",
    "    if all_models_best:\n",
    "        best_model = all_models_best[0]\n",
    "        model_dir = Path(VIZ_OUTPUT_DIR) / 'kfold_ensembles' / best_model['model_dir']\n",
    "        ensemble_dir = model_dir / 'ensemble_5_models'\n",
    "        best_cases_dir = ensemble_dir / 'best_cases'\n",
    "        \n",
    "        if best_cases_dir.exists():\n",
    "            # Get top 3 best cases for this model\n",
    "            latex_figures += f\"\"\"% Multiple successful cases for best model: {best_model['arch']}\n",
    "\\\\begin{{figure}}[htbp]\n",
    "\\\\centering\n",
    "\"\"\"\n",
    "            \n",
    "            for case_num in range(1, 4):  # Cases 1, 2, 3\n",
    "                files, iou = get_image_files(best_cases_dir, 'best', case_num)\n",
    "                if files and 'overlay_pred' in files:\n",
    "                    latex_figures += f\"\"\"\\\\begin{{subfigure}}{{0.32\\\\textwidth}}\n",
    "    \\\\includegraphics[width=\\\\textwidth]{{{best_model['model_dir']}/ensemble_5_models/best_cases/{files['overlay_pred'].name}}}\n",
    "    \\\\caption{{Case {case_num}: IoU={iou:.3f}}}\n",
    "\\\\end{{subfigure}}\"\"\"\n",
    "                    \n",
    "                    if case_num < 3:\n",
    "                        latex_figures += \"\\n\\\\hfill\"\n",
    "            \n",
    "            latex_figures += f\"\"\"\n",
    "\\\\caption{{Top 3 segmentation results for {best_model['arch']} with {best_model['backbone']} backbone, demonstrating consistent high performance across different dermoscopy images.}}\n",
    "\\\\label{{fig:bestmodelmultiple}}\n",
    "\\\\end{{figure}}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add usage instructions\n",
    "    latex_figures += \"\"\"\n",
    "% USAGE INSTRUCTIONS\n",
    "%\n",
    "% 1. Copy the kfold_ensembles directory to your LaTeX project:\n",
    "%    cp -r \"\"\" + str(VIZ_OUTPUT_DIR / 'kfold_ensembles') + \"\"\" ./figures/\n",
    "%\n",
    "% 2. In your LaTeX document preamble, add:\n",
    "%    \\\\graphicspath{{./figures/kfold_ensembles/}}\n",
    "%\n",
    "% 3. Include the figures in your document:\n",
    "%    \\\\input{kfold_ensemble_figures_improved.tex}\n",
    "%\n",
    "% 4. Or copy individual figure environments to your document\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Save the improved LaTeX file\n",
    "    output_file = VIZ_OUTPUT_DIR / 'kfold_ensemble_figures_improved.tex'\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(latex_figures)\n",
    "\n",
    "    print(f\"\\nImproved LaTeX figure snippets saved to: {output_file}\")\n",
    "\n",
    "    # Create summary of generated figures\n",
    "    summary_text = f\"\"\"LaTeX Figure Generation Summary\n",
    "==============================\n",
    "\n",
    "Total models processed: {len(all_models_best)}\n",
    "Figures generated per model: 2 (best case + worst case)\n",
    "Comparison figures: 2\n",
    "\n",
    "Models included (ordered by performance):\n",
    "\"\"\"\n",
    "\n",
    "    for i, model in enumerate(all_models_best, 1):\n",
    "        summary_text += f\"{i}. {model['arch']} with {model['backbone']} - Ensemble IoU: {model['ensemble_iou']:.4f}\\n\"\n",
    "\n",
    "    summary_text += f\"\"\"\n",
    "Output files:\n",
    "- LaTeX figures: {output_file}\n",
    "- This summary: {VIZ_OUTPUT_DIR / 'latex_generation_summary.txt'}\n",
    "\n",
    "To use in your LaTeX document:\n",
    "1. Copy the kfold_ensembles folder to your LaTeX project\n",
    "2. Add \\\\graphicspath{{./figures/kfold_ensembles/}} to your preamble\n",
    "3. Include the generated .tex file or copy individual figures\n",
    "\"\"\"\n",
    "\n",
    "    with open(VIZ_OUTPUT_DIR / 'latex_generation_summary.txt', 'w') as f:\n",
    "        f.write(summary_text)\n",
    "\n",
    "    print(\"\\nSummary of generated figures:\")\n",
    "    print(summary_text)\n",
    "\n",
    "    # Save results to JSON\n",
    "    results_summary = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'models_evaluated': len(models_to_ensemble),\n",
    "        'successful_ensembles': len(successful_ensembles),\n",
    "        'average_improvement': float(np.mean(all_improvements)) if all_improvements else None,\n",
    "        'ensemble_results': ensemble_results\n",
    "    }\n",
    "\n",
    "    with open(VIZ_OUTPUT_DIR / 'kfold_ensemble_results.json', 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "    print(\"\\nK-Fold ensemble analysis complete\")\n",
    "    print(f\"Results saved to: {VIZ_OUTPUT_DIR / 'kfold_ensembles/'}\")\n",
    "    print(f\"Summary saved to: {VIZ_OUTPUT_DIR / 'kfold_ensemble_results.json'}\")\n",
    "    print(\"LaTeX outputs:\")\n",
    "    print(f\"   - Table: {VIZ_OUTPUT_DIR / 'kfold_ensemble_table.tex'}\")\n",
    "    print(f\"   - Figures: {VIZ_OUTPUT_DIR / 'kfold_ensemble_figures.tex'}\")\n",
    "    print(f\"   - Report: {VIZ_OUTPUT_DIR / 'KFOLD_ENSEMBLE_REPORT.md'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood Analysis\n",
    "### Image Pre-processing and Building Mask Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data configuration\n",
    "images_to_process = [\n",
    "    \"data/notebook_04/geotiff/tile_1024_split/24991118_tile_10_7_18eac4.tif\",\n",
    "    \"data/notebook_04/geotiff/tile_1024_split/24991118_tile_10_8_2931b8.tif\",\n",
    "    \"data/notebook_04/geotiff/tile_1024_split/24991118_tile_11_7_4db66f.tif\",\n",
    "    \"data/notebook_04/geotiff/tile_1024_split/24991118_tile_11_8_6b34a7.tif\",\n",
    "    \"data/notebook_04/geotiff/tile_1024_split/24991118_tile_12_7_7ce219.tif\",\n",
    "    \"data/notebook_04/geotiff/tile_1024_split/24991118_tile_12_8_83485c.tif\",\n",
    "]\n",
    "\n",
    "# Output directory\n",
    "output_dir = QUARTIER_OUTPUT_DIR / \"masked_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load building polygons\n",
    "gdf_toitures = gpd.read_file(\"data/SITG/CAD_BATIMENT_HORSOL_TOIT_2024-11-03.gpkg\")\n",
    "print(f\"Loaded {len(gdf_toitures)} building polygons\")\n",
    "\n",
    "# Process each image\n",
    "for image_path in images_to_process:\n",
    "    print(f\"\\nProcessing: {image_path}\")\n",
    "    \n",
    "    with rasterio.open(image_path) as src:\n",
    "        print(f\"  Original - Bands: {src.count}, Dtype: {src.dtypes[0]}\")\n",
    "        print(f\"  Original - Size: {src.width} x {src.height}\")\n",
    "        print(f\"  Original - Data range: {src.read(1).min()} to {src.read(1).max()}\")\n",
    "        \n",
    "        # Check original color interpretation\n",
    "        original_colorinterp = src.colorinterp\n",
    "        print(f\"  Original color interpretation: {original_colorinterp}\")\n",
    "        \n",
    "        # Get profile for output\n",
    "        profile = src.profile.copy()\n",
    "        \n",
    "        # Reproject polygons if needed\n",
    "        if gdf_toitures.crs != src.crs:\n",
    "            gdf_reproj = gdf_toitures.to_crs(src.crs)\n",
    "        else:\n",
    "            gdf_reproj = gdf_toitures\n",
    "        \n",
    "        # Find intersecting buildings\n",
    "        image_bbox = box(*src.bounds)\n",
    "        intersecting = gdf_reproj[gdf_reproj.intersects(image_bbox)]\n",
    "        \n",
    "        if len(intersecting) == 0:\n",
    "            print(f\"  Warning: No buildings found in {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Found {len(intersecting)} buildings\")\n",
    "        \n",
    "        # Apply mask to keep buildings\n",
    "        geometries = intersecting.geometry.values\n",
    "        masked_data, masked_transform = mask(\n",
    "            src, \n",
    "            geometries, \n",
    "            crop=False,\n",
    "            invert=False,    # Keep inside buildings\n",
    "            nodata=None,     # Don't set nodata to 0\n",
    "            filled=True\n",
    "        )\n",
    "        \n",
    "        print(f\"  Masked data shape: {masked_data.shape}\")\n",
    "        print(f\"  Masked data range: {masked_data.min()} to {masked_data.max()}\")\n",
    "        \n",
    "        # Calculate coverage\n",
    "        non_zero_pixels = np.count_nonzero(masked_data)\n",
    "        total_pixels = masked_data.shape[1] * masked_data.shape[2]\n",
    "        coverage_pct = (non_zero_pixels / total_pixels) * 100\n",
    "        print(f\"  Coverage: {coverage_pct:.2f}%\")\n",
    "        \n",
    "        # Update profile maintaining original properties\n",
    "        profile.update({\n",
    "            'driver': 'GTiff',\n",
    "            'compress': 'lzw',\n",
    "            'tiled': True,\n",
    "            'count': src.count,           # Keep original band count\n",
    "            'dtype': src.dtypes[0],       # Keep original data type\n",
    "            'width': masked_data.shape[2], # Update dimensions\n",
    "            'height': masked_data.shape[1],\n",
    "            'transform': masked_transform,  # Use masked transform\n",
    "        })\n",
    "        \n",
    "        # Only set nodata if original had it\n",
    "        if src.nodata is not None:\n",
    "            profile['nodata'] = src.nodata\n",
    "        \n",
    "        # Save masked image\n",
    "        base_name = Path(image_path).stem\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}_masked.tif\")\n",
    "        \n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            # Write all bands\n",
    "            dst.write(masked_data)\n",
    "            \n",
    "            # Set proper color interpretation for RGB images\n",
    "            if src.count >= 3:\n",
    "                # Set color interpretation to RGB\n",
    "                dst.colorinterp = [ColorInterp.red, ColorInterp.green, ColorInterp.blue]\n",
    "                if src.count == 4:\n",
    "                    dst.colorinterp = [ColorInterp.red, ColorInterp.green, ColorInterp.blue, ColorInterp.alpha]\n",
    "            else:\n",
    "                # Copy original color interpretation\n",
    "                dst.colorinterp = original_colorinterp\n",
    "            \n",
    "            # Copy over any additional metadata tags\n",
    "            dst.update_tags(**src.tags())\n",
    "            \n",
    "            # Add custom metadata about masking\n",
    "            dst.update_tags(\n",
    "                PROCESSING='Building mask applied',\n",
    "                MASK_COVERAGE=f'{coverage_pct:.2f}%',\n",
    "                BUILDINGS_COUNT=str(len(intersecting))\n",
    "            )\n",
    "        \n",
    "        print(f\"  Saved: {output_path}\")\n",
    "        \n",
    "        # Verify saved file\n",
    "        with rasterio.open(output_path) as verify:\n",
    "            print(f\"  Verification - Bands: {verify.count}, Dtype: {verify.dtypes[0]}\")\n",
    "            print(f\"  Verification - Color interp: {verify.colorinterp}\")\n",
    "            print(f\"  Verification - Data range: {verify.read(1).min()} to {verify.read(1).max()}\")\n",
    "        \n",
    "        # Create color preview for RGB images\n",
    "        if src.count >= 3:\n",
    "            print(f\"  Creating color preview...\")\n",
    "            \n",
    "            # Create RGB composite for display using percentile stretch\n",
    "            rgb_display = np.zeros((masked_data.shape[1], masked_data.shape[2], 3), dtype=np.uint8)\n",
    "            \n",
    "            for i in range(3):\n",
    "                band_data = masked_data[i].astype(np.float32)\n",
    "                \n",
    "                # Only process non-zero pixels (building areas)\n",
    "                non_zero_mask = band_data > 0\n",
    "                \n",
    "                if np.any(non_zero_mask):\n",
    "                    # Calculate percentiles only on non-zero values\n",
    "                    non_zero_values = band_data[non_zero_mask]\n",
    "                    p2, p98 = np.percentile(non_zero_values, [2, 98])\n",
    "                    \n",
    "                    # Stretch to 0-255 range\n",
    "                    stretched = np.clip((band_data - p2) / (p98 - p2) * 255, 0, 255)\n",
    "                    rgb_display[:, :, i] = stretched.astype(np.uint8)\n",
    "            \n",
    "            # Display comparison\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "            \n",
    "            # Original RGB using same percentile stretch\n",
    "            original_rgb = np.zeros((src.height, src.width, 3), dtype=np.uint8)\n",
    "            original_data = src.read()\n",
    "            \n",
    "            for i in range(3):\n",
    "                band_data = original_data[i].astype(np.float32)\n",
    "                p2, p98 = np.percentile(band_data, [2, 98])\n",
    "                stretched = np.clip((band_data - p2) / (p98 - p2) * 255, 0, 255)\n",
    "                original_rgb[:, :, i] = stretched.astype(np.uint8)\n",
    "            \n",
    "            axes[0].imshow(original_rgb)\n",
    "            axes[0].set_title('Original Color Image')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(rgb_display)\n",
    "            axes[1].set_title('Masked Color Image (Buildings Only)')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "print(f\"\\nProcessing complete. Color masked images saved to: {output_dir}\")\n",
    "\n",
    "# Final verification of all output files\n",
    "print(\"\\nFinal verification of all output files:\")\n",
    "for image_path in images_to_process:\n",
    "    base_name = Path(image_path).stem\n",
    "    output_path = os.path.join(output_dir, f\"{base_name}_masked.tif\")\n",
    "    if os.path.exists(output_path):\n",
    "        with rasterio.open(output_path) as check:\n",
    "            print(f\"  {base_name}_masked.tif:\")\n",
    "            print(f\"     - Bands: {check.count} ({check.dtypes[0]})\")\n",
    "            print(f\"     - Color interp: {check.colorinterp}\")\n",
    "            print(f\"     - Size: {check.width} x {check.height}\")\n",
    "            print(f\"     - CRS: {check.crs}\")\n",
    "            print(f\"     - Non-zero pixels: {np.count_nonzero(check.read()):,}\")\n",
    "    else:\n",
    "        print(f\"  {base_name}_masked.tif: NOT FOUND\")\n",
    "\n",
    "print(\"\\nNote: These files can be opened in QGIS/ArcGIS for full color display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference on Masked Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def run_inference_on_new_images(viz_system, df_model_results, unseen_image_paths, output_dir):\n",
    "    \"\"\"\n",
    "    Run inference using pre-defined k-fold ensembles on new images.\n",
    "    \n",
    "    Applies the same ensemble models used in validation to new unseen images,\n",
    "    generating predictions and visualizations for each model combination.\n",
    "    \n",
    "    Parameters:\n",
    "        viz_system: LaTeXVisualizationSystem instance\n",
    "        df_model_results: DataFrame with model information\n",
    "        unseen_image_paths: List of paths to new images\n",
    "        output_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define models matching k-fold ensemble configuration\n",
    "    models_to_use = [\n",
    "        ('unet', 'tu-mambaout_small'),\n",
    "        ('upernet', 'tu-efficientnetv2_rw_s.ra2_in1k'),\n",
    "        ('segformer', 'tu-mambaout_base'),\n",
    "        ('linknet', 'timm-efficientnet-b5'),\n",
    "        ('unetplusplus', 'tu-efficientnetv2_rw_s.ra2_in1k'),\n",
    "        ('segformer', 'tu-regnety_080.ra3_in1k'),\n",
    "    ]\n",
    "    \n",
    "    # Process each model ensemble\n",
    "    for arch, backbone in models_to_use:\n",
    "        model_name = f\"{arch}_{backbone}\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing ensemble: {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get all k-fold models for this architecture-backbone combination\n",
    "        model_infos = []\n",
    "        for fold in range(5):\n",
    "            mask = (df_model_results['architecture'] == arch) & \\\n",
    "                   (df_model_results['backbone'] == backbone) & \\\n",
    "                   (df_model_results['validation_fold'] == fold)\n",
    "            \n",
    "            fold_models = df_model_results[mask]\n",
    "            if not fold_models.empty:\n",
    "                model_infos.append(fold_models.iloc[0].to_dict())\n",
    "        \n",
    "        if len(model_infos) == 0:\n",
    "            print(f\"Warning: No models found for {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Found {len(model_infos)} fold models\")\n",
    "        \n",
    "        # Run inference\n",
    "        viz_system.inference_on_unseen_images(\n",
    "            model_infos,\n",
    "            unseen_image_paths,\n",
    "            output_dir,\n",
    "            ensemble_name=model_name,\n",
    "            device_id=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of new image paths\n",
    "unseen_images = [\n",
    "    QUARTIER_OUTPUT_DIR / \"masked_images/24991118_tile_10_7_18eac4_masked.tif\",\n",
    "    QUARTIER_OUTPUT_DIR / \"masked_images/24991118_tile_10_8_2931b8_masked.tif\",\n",
    "    QUARTIER_OUTPUT_DIR / \"masked_images/24991118_tile_11_7_4db66f_masked.tif\",\n",
    "    QUARTIER_OUTPUT_DIR / \"masked_images/24991118_tile_11_8_6b34a7_masked.tif\",\n",
    "    QUARTIER_OUTPUT_DIR / \"masked_images/24991118_tile_12_7_7ce219_masked.tif\",\n",
    "    QUARTIER_OUTPUT_DIR / \"masked_images/24991118_tile_12_8_83485c_masked.tif\",\n",
    "]\n",
    "\n",
    "# Load existing results\n",
    "logger.info(\"Loading existing results...\")\n",
    "df_model_results = pd.read_parquet(SAVED_MODEL_RESULTS_PARQUET_PATH)\n",
    "df_per_image_results = pd.read_parquet(SAVED_PER_IMAGE_RESULTS_PARQUET_PATH)\n",
    "\n",
    "# Remove duplicates, keeping highest performing models\n",
    "df_model_results = df_model_results.sort_values('eval_test_iou', ascending=False).drop_duplicates(\n",
    "    subset=['architecture', 'backbone', 'validation_fold'], keep='first'\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Load test paths\n",
    "test_paths = load_test_data(TEST_DATASET_FILE, DATASET_IMAGES_DIR, DATASET_MASKS_DIR)\n",
    "\n",
    "# Initialize visualization system\n",
    "viz_system = LaTeXVisualizationSystem(img_size=IMG_SIZE)\n",
    "\n",
    "# Run inference on new images\n",
    "run_inference_on_new_images(viz_system, df_model_results, unseen_images, QUARTIER_OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
